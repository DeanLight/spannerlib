[
  {
    "objectID": "session_tests/006_test_introduction_tutorial.html",
    "href": "session_tests/006_test_introduction_tutorial.html",
    "title": "Spannerlib",
    "section": "",
    "text": "import numpy as np\nimport tempfile\nfrom pandas import DataFrame\nfrom pathlib import Path\nimport pandas as pd\n\nfrom spannerlib.utils import checkLogs\nfrom spannerlib.span import Span\nfrom spannerlib.session import Session,test_session\n\n\ntest_session(\n    \"\"\"\n    new uncle(str, str)\n    uncle(\"bob\", \"greg\")\n    ?uncle(X,Y)\n    \"\"\",\n    pd.DataFrame(data={\"X\": [\"bob\"], \"Y\": [\"greg\"]})\n)\n\n'?uncle(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\nbob\ngreg\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n_= test_session(\n    [\n    \"\"\"\n    new lecturer(str, str)\n    lecturer(\"walter\", \"chemistry\")\n    lecturer(\"linus\", \"operation systems\")\n    lecturer(\"rick\", \"physics\")\n\n    new enrolled(str, str)\n    enrolled(\"abigail\", \"chemistry\")\n    enrolled(\"abigail\", \"operation systems\")\n    enrolled(\"jordan\", \"chemistry\")\n    enrolled(\"gale\", \"operation systems\")\n    enrolled(\"howard\", \"chemistry\")\n    enrolled(\"howard\", \"physics\")\n\n    enrolled_in_chemistry(X) &lt;- enrolled(X, \"chemistry\").\n    ?enrolled_in_chemistry(\"jordan\")\n    \"\"\",\n    \"\"\"\n    ?enrolled_in_chemistry(\"gale\")\n    \"\"\",\n    \"\"\"\n    ?enrolled_in_chemistry(X)\n    \"\"\",\n    \"\"\"\n    enrolled_in_physics_and_chemistry(X) &lt;- enrolled(X, \"chemistry\"), enrolled(X, \"physics\").\n    ?enrolled_in_physics_and_chemistry(X)\n    \"\"\",\n    \"\"\"\n    lecturer_of(X, Z) &lt;- lecturer(X, Y), enrolled(Z, Y).\n    ?lecturer_of(X, \"abigail\")\n    \"\"\",\n    \"\"\"\n    gpa_doc = \"abigail 100 jordan 80 gale 79 howard 60\"\n    student_gpas(Student, Grade) &lt;- rgx(\"(\\w+).*?(\\d+)\",$gpa_doc)-&gt;(StudentSpan, GradeSpan),\n        as_str(StudentSpan)-&gt;(Student), as_str(GradeSpan)-&gt;(Grade).\n    gpa_of_chemistry_students(X, Grade) &lt;- student_gpas(X, Grade), enrolled(X, \"chemistry\").\n    ?gpa_of_chemistry_students(X, \"100\")\n    \"\"\"],\n    [\n        True,\n        False,\n        pd.DataFrame(data={\"X\": [\"howard\", \"jordan\", \"abigail\"]}),\n        pd.DataFrame(data={\"X\": [\"howard\"]}),\n        pd.DataFrame(data={\"X\": [\"linus\", \"walter\"]}),\n        pd.DataFrame(data={\"X\": [\"abigail\"]}),\n    ]\n)\n\n'?enrolled_in_chemistry(\"jordan\")'\n\n\nTrue\n\n\n'?enrolled_in_chemistry(\"gale\")'\n\n\nFalse\n\n\n'?enrolled_in_chemistry(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\nabigail\n\n\nhoward\n\n\njordan\n\n\n\n\n\n\n\n\n\n'?enrolled_in_physics_and_chemistry(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\nhoward\n\n\n\n\n\n\n\n\n\n'?lecturer_of(X,\"abigail\")'\n\n\n\n\n\n\n\nX\n\n\n\n\nlinus\n\n\nwalter\n\n\n\n\n\n\n\n\n\n'?gpa_of_chemistry_students(X,\"100\")'\n\n\n\n\n\n\n\nX\n\n\n\n\nabigail\n\n\n\n\n\n\n\n\n\n\n_=test_session(\n    [\"\"\"\n    jsonpath_simple_1 = \"foo[*].baz\"\n    json_ds_simple_1  = \"{'foo': [{'baz': 1}, {'baz': 2}]}\"\n    simple_1(X) &lt;- json_path($json_ds_simple_1, $jsonpath_simple_1) -&gt; (Path,X).\n    ?simple_1(X)\n    \"\"\",\"\"\"\n    jsonpath_simple_2 = \"a.*.b.`parent`.c\"\n    json_ds_simple_2 = \"{'a': {'x': {'b': 1, 'c': 'number one'}, 'y': {'b': 2, 'c': 'number two'}}}\"\n    simple_2(X) &lt;- json_path($json_ds_simple_2, $jsonpath_simple_2) -&gt; (Path,X).\n    ?simple_2(X)\n    \"\"\",\"\"\"\n    json_ds_advanced  = \"{'foo': [{'baz': 1}, {'baz': {'foo': [{'baz': 1}, {'baz': 2}]}}]}\"\n    advanced(X) &lt;- json_path($json_ds_advanced, $jsonpath_simple_1) -&gt; (Path,X).\n    ?advanced(X)\n    \"\"\"],\n    [\n        pd.DataFrame(data={\"X\": ['2', '1']}),\n        pd.DataFrame(data={\"X\": [\"number two\", \"number one\"]}),\n        pd.DataFrame(data={\"X\": ['1','''{'foo': [{'baz': 1}, {'baz': 2}]}''']})\n    ]\n)\n\n'?simple_1(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\n\n'?simple_2(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\nnumber one\n\n\nnumber two\n\n\n\n\n\n\n\n\n\n'?advanced(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n{'foo': [{'baz': 1}, {'baz': 2}]}"
  },
  {
    "objectID": "session_tests/004_test_recursive.html",
    "href": "session_tests/004_test_recursive.html",
    "title": "Spannerlib",
    "section": "",
    "text": "import numpy as np\nimport tempfile\nfrom pandas import DataFrame\nfrom pathlib import Path\nimport pandas as pd\n\nfrom spannerlib.utils import checkLogs\nfrom spannerlib.span import Span\nfrom spannerlib.session import Session,test_session\n\n\ntest_session(\n    [\"\"\"\n    new S(int,int)\n    new S2(int,int)\n    R(X,Y)&lt;-S(X,Y).\n    R(X,Y)&lt;-S(X,Z),R(Z,Y).\n    R2(X,Y,Z)&lt;-S2(X,Y),R(Y,Z).\n    R2(X,Y,Z)&lt;-S2(X,Y),R(X,Y),R2(X,Y,Y),S(X,Z).\n    \"\"\",\n    \"\"\"?R2(X,Y,Z)\"\"\"\n    ],\n    [\n        None,\n        pd.DataFrame(columns=['X','Y','Z'])\n    ],\n    debug=True\n)\n\n\n\n\n'?R2(X,Y,Z)'\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test_session(\n#     [\n#         \"\"\"\n#         test_range(X) &lt;- yield_range(5) -&gt; (X).\n#         ?test_range(X)\n#         \"\"\",\n#         \"\"\"\n#         test_range_span(X) &lt;- yield_range_span(5) -&gt; (X).\n#         ?test_range_span(X)\n#         \"\"\",\n#         \"\"\"\n#         test_range_str(X) &lt;- yield_range_str(5) -&gt; (X).\n#         ?test_range_str(X)\n#         \"\"\",\n#     ],\n#     [\n#         pd.DataFrame({'X':range(5)}),\n#         pd.DataFrame({'X':[Span(0,0),Span(0,1),Span(0,2),Span(0,3),Span(0,4)]}),\n#         pd.DataFrame({'X':[\"string0\",\"string1\",\"string2\",\"string3\",\"string4\"]}),\n#     ],\n\n# )\n\n\n# with checkLogs(name='spannerlib.engine'):\ntest_session(\n    [\n        \"\"\"\n            new parent(str, str)\n            parent(\"Liam\", \"Noah\")\n            parent(\"Noah\", \"Oliver\")\n            parent(\"James\", \"Lucas\")\n            parent(\"Noah\", \"Benjamin\")\n            parent(\"Benjamin\", \"Mason\")\n            ancestor(X,Y) &lt;- parent(X,Y).\n            ancestor(X,Y) &lt;- parent(X,Z), ancestor(Z,Y).\n        \"\"\",\n        \"\"\"\n            ?ancestor(\"Liam\", X)\n        \"\"\"\n\n    ],\n    [   None,\n        pd.DataFrame({'X':['Mason','Oliver','Benjamin','Noah']}),\n    ],\n    # debug=True\n)\n\n'?ancestor(\"Liam\",X)'\n\n\n\n\n\n\n\nX\n\n\n\n\nBenjamin\n\n\nMason\n\n\nNoah\n\n\nOliver\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ntest_session(\n    [\n        \"\"\"\n            new parent(str, str)\n            parent(\"Liam\", \"Noah\")\n            parent(\"Noah\", \"Oliver\")\n            parent(\"James\", \"Lucas\")\n            parent(\"Noah\", \"Benjamin\")\n            parent(\"Benjamin\", \"Mason\")\n            ancestor(X,Y) &lt;- parent(X,Y).\n            ancestor(X,Y) &lt;- parent(X,Z), ancestor(Z,Y).\n\n            ?ancestor(\"Liam\", X)\n        \"\"\",\n        \"\"\"\n            ?ancestor(X, \"Mason\")\n\n        \"\"\",\n        \"\"\"\n            ?ancestor(\"Mason\", X)\n\n        \"\"\",\n    ],\n    [\n        pd.DataFrame({'X':['Mason','Oliver','Benjamin','Noah']}),\n        pd.DataFrame({'X':['Noah','Liam','Benjamin']}),\n        pd.DataFrame({'X':[]}),\n    ],\n\n)\n\n'?ancestor(\"Liam\",X)'\n\n\n\n\n\n\n\nX\n\n\n\n\nBenjamin\n\n\nMason\n\n\nNoah\n\n\nOliver\n\n\n\n\n\n\n\n\n\n'?ancestor(X,\"Mason\")'\n\n\n\n\n\n\n\nX\n\n\n\n\nBenjamin\n\n\nLiam\n\n\nNoah\n\n\n\n\n\n\n\n\n\n'?ancestor(\"Mason\",X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ntest_session(\n    \"\"\"\n            new C(int)\n            C(1)\n            C(2)\n            C(3)\n\n            B(X) &lt;- C(X).\n            A(X) &lt;- B(X).\n            B(X) &lt;- A(X).\n\n            ?A(X)\n            \"\"\",\n    pd.DataFrame({'X':[1,2,3]}),\n)\n\n'?A(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;"
  },
  {
    "objectID": "session_tests/005_test_complex.html",
    "href": "session_tests/005_test_complex.html",
    "title": "Spannerlib",
    "section": "",
    "text": "import numpy as np\nimport tempfile\nfrom pandas import DataFrame\nfrom pathlib import Path\nimport pandas as pd\n\nfrom spannerlib.utils import checkLogs\nfrom spannerlib.span import Span\nfrom spannerlib.session import Session,test_session\n\n\ndef which_century(year):\n    yield int(year / 100) + 1\n\ndef which_era(cet):\n    if 1 &lt;= cet &lt; 4:\n        era = \"Targerian Regime\"\n    elif 4 &lt;= cet &lt; 8:\n        era =  \"Lanister Regime\"\n    elif 8 &lt;= cet &lt; 12:\n        era =  \"Stark Regime\"\n    elif 12 &lt;= cet &lt; 16:\n        era =  \"Barathion Regime\"\n    elif cet &gt;= 16:\n        era = \"Long Winter\"\n    yield era\n\ntest_session(\n    ie_funcs=[\n        ['which_century', which_century, [int], [int]],\n        ['which_era', which_era, [int], [str]]\n    ],\n    code_strings=[\n    \"\"\"\n    new event(str, int)\n    event(\"First Dragon\", 250)\n    event(\"Mad king\", 390)\n    event(\"Winter came\", 1750)\n    event(\"Hodor\", 999)\n    event(\"Joffery died\", 799)\n    \n    new important_year(int)\n    important_year(999)\n    important_year(1750)\n    important_year(250)\n    \n    \n    important_events(EVE, Y) &lt;- event(EVE, Y), important_year(Y).\n    \n    important_events_per_cet(EVE, CET) &lt;- important_events(EVE, Y), which_century(Y) -&gt; (CET).\n    ?important_events_per_cet(EVE, CET)\n    \"\"\",\n    \"\"\"\n    important_events_per_era(EVE, ERA) &lt;- important_events_per_cet(EVE, CET), which_era(CET) -&gt; (ERA).\n    ?important_events_per_era(EVE, ERA)\n    \"\"\"\n    ],\n    expected_outputs = [\n        DataFrame({\n            'EVE': ['Hodor', 'Winter came', 'First Dragon'],\n            'CET': [10, 18, 3]\n        }),\n        DataFrame({\n            'EVE': ['Hodor', 'Winter came', 'First Dragon'],\n            'ERA': ['Stark Regime', 'Long Winter', 'Targerian Regime']\n        })\n        \n    ]\n)\n\n'?important_events_per_cet(EVE,CET)'\n\n\n\n\n\n\n\nEVE\nCET\n\n\n\n\nFirst Dragon\n3\n\n\nHodor\n10\n\n\nWinter came\n18\n\n\n\n\n\n\n\n\n\n'?important_events_per_era(EVE,ERA)'\n\n\n\n\n\n\n\nEVE\nERA\n\n\n\n\nFirst Dragon\nTargerian Regime\n\n\nHodor\nStark Regime\n\n\nWinter came\nLong Winter\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ndef get_population_growth_rate(year):\n    if year &lt;= 1700:\n        yield 1\n    elif 1700 &lt; year &lt;= 1900:\n        yield 3\n    elif 1900 &lt; year &lt;= 2000:\n        yield 5\n    else:  \n        yield 1\n\ndef historical_event(year):\n    if year == 1600:\n        yield \"Discovery of New Land\"\n    elif year == 1750:\n        yield \"Industrial Revolution\"\n    elif year == 1920:\n        yield \"Roaring Twenties\"\n    elif year == 2000:\n        yield \"Y2K Bug Panic\"\n    else:\n        yield \"No significant event\"\n\ntest_session(\n    ie_funcs=[\n        ['get_population_growth_rate', get_population_growth_rate, [int], [int]],\n        ['historical_event', historical_event, [int], [str]]\n    ],\n    code_strings=[\n    \"\"\"\n    new year_event(int, str)\n    year_event(1600, \"Discovery of New Land\")\n    year_event(1750, \"Industrial Revolution\")\n    year_event(1920, \"Roaring Twenties\")\n    year_event(2000, \"Y2K Bug Panic\")\n    \n    new population_year(int)\n    population_year(1600)\n    population_year(1700)\n    population_year(1800)\n    population_year(1900)\n    population_year(2000)\n    population_year(1750)\n    population_year(1920)\n    \n    population_growth_rate(Y,GROWTH) &lt;- population_year(Y) ,get_population_growth_rate(Y) -&gt; (GROWTH).\n    \n    historical_event(Y, EVENT) &lt;- year_event(Y, EVENT).\n    \n    important_years(Y, EVENT, GROWTH) &lt;- population_growth_rate(Y, GROWTH), historical_event(Y, EVENT).\n    \n    ?important_years(Y, EVENT, GROWTH)\n    \"\"\"],\n    expected_outputs = [\n     pd.DataFrame([\n        [1600, \"Discovery of New Land\", 1],\n        [1750, \"Industrial Revolution\", 3],\n        [1920, \"Roaring Twenties\", 5],\n        [2000, \"Y2K Bug Panic\", 5]\n     ], columns=['Y', 'EVENT', 'GROWTH'])   \n    ]\n)\n\n'?important_years(Y,EVENT,GROWTH)'\n\n\n\n\n\n\n\nY\nEVENT\nGROWTH\n\n\n\n\n1600\nDiscovery of New Land\n1\n\n\n1750\nIndustrial Revolution\n3\n\n\n1920\nRoaring Twenties\n5\n\n\n2000\nY2K Bug Panic\n5\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ndef technological_advancement(year):\n        if year &lt;= 1700:\n            yield \"Low\"\n        elif 1700 &lt; year &lt;= 1900:\n            yield \"Moderate\"\n        elif 1900 &lt; year &lt;= 2000:\n            yield \"High\"\n        else:\n            yield \"Low\"\n\ntest_session(\n    \"\"\"\n    new year_event(int, str)\n    year_event(1600, \"Discovery of New Land\")\n    year_event(1750, \"Industrial Revolution\")\n    year_event(1920, \"Roaring Twenties\")\n    year_event(2000, \"Y2K Bug Panic\")\n    \n    new population_year(int)\n    population_year(1600)\n    population_year(1700)\n    population_year(1800)\n    population_year(1900)\n    population_year(2000)\n    population_year(1750)\n    population_year(1920)\n    \n    population_growth_rate(Y, GROWTH) &lt;- population_year(Y), get_population_growth_rate(Y) -&gt; (GROWTH).\n    \n    technological_advancement(Y, TECH) &lt;- year_event(Y, Z), technological_advancement(Y) -&gt; (TECH).\n    \n    important_years(Y, EVENT, GROWTH, TECH) &lt;- population_growth_rate(Y, GROWTH), year_event(Y, EVENT), technological_advancement(Y, TECH).\n    \n    ?important_years(Y, EVENT, GROWTH, TECH)\n    \"\"\",\n\n    pd.DataFrame([\n        [1600, \"Discovery of New Land\", 1, \"Low\"],\n        [1750, \"Industrial Revolution\", 3, \"Moderate\"],\n        [1920, \"Roaring Twenties\", 5, \"High\"],\n        [2000, \"Y2K Bug Panic\", 5, \"High\"]\n    ], columns=['Y', 'EVENT', 'GROWTH', 'TECH']),\n    ie_funcs=[\n        ['get_population_growth_rate', get_population_growth_rate, [int], [int]],\n        ['technological_advancement', technological_advancement, [int], [str]]\n    ]\n)\n\n'?important_years(Y,EVENT,GROWTH,TECH)'\n\n\n\n\n\n\n\nY\nEVENT\nGROWTH\nTECH\n\n\n\n\n1600\nDiscovery of New Land\n1\nLow\n\n\n1750\nIndustrial Revolution\n3\nModerate\n\n\n1920\nRoaring Twenties\n5\nHigh\n\n\n2000\nY2K Bug Panic\n5\nHigh\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ndef get_match_result(home_team, away_team, home_goals, away_goals):\n    if home_goals &gt; away_goals:\n        yield \"Home Win\"\n    elif home_goals &lt; away_goals:\n        yield \"Away Win\"\n    else:\n        yield \"Draw\"\n\ndef get_player_position(player_name):\n    if player_name in [\"Lionel Messi\", \"Cristiano Ronaldo\"]:\n        yield \"Forward\"\n    elif player_name in [\"Virgil van Dijk\", \"Sergio Ramos\"]:\n        yield \"Defender\"\n    else:\n        yield \"Unknown\"\n\ntest_session(\n    [\n    \"\"\"\n    new team(str)\nteam(\"Barcelona\")\n    team(\"Real Madrid\")\n    team(\"Liverpool\")\n    team(\"Juventus\")\n    \n    new player(str, str)\n    player(\"Lionel Messi\", \"Barcelona\")\n    player(\"Cristiano Ronaldo\", \"Juventus\")\n    player(\"Virgil van Dijk\", \"Liverpool\")\n    player(\"Sergio Ramos\", \"Real Madrid\")\n    \n    new match(str, str, int, int)\n    match(\"Barcelona\", \"Real Madrid\", 3, 2)\n    match(\"Real Madrid\", \"Barcelona\", 1, 1)\n    match(\"Liverpool\", \"Juventus\", 2, 0)\n    match(\"Juventus\", \"Liverpool\", 0, 1)\n    \n    match_result(HT, AT, HG, AG, RESULT) &lt;- match(HT, AT, HG, AG), get_match_result(HT, AT, HG, AG) -&gt; (RESULT).\n    \n    player_position(P, POS) &lt;- player(P, X), get_player_position(P) -&gt; (POS).\n    \n    ?match_result(HT, AT, HG, AG, RESULT)\n    \n    \"\"\",\n    \"\"\"?player_position(P, POS) \"\"\"    \n    ],\n    [\n        pd.DataFrame([\n            ['Barcelona', 'Real Madrid', 3, 2, 'Home Win'],\n            ['Real Madrid', 'Barcelona', 1, 1, 'Draw'],\n            ['Liverpool', 'Juventus', 2, 0, 'Home Win'],\n            ['Juventus', 'Liverpool', 0, 1, 'Away Win']\n        ], columns=['HT', 'AT', 'HG', 'AG', 'RESULT']),\n        pd.DataFrame([\n            ['Lionel Messi', 'Forward'],\n            ['Cristiano Ronaldo', 'Forward'],\n            ['Virgil van Dijk', 'Defender'],\n            ['Sergio Ramos', 'Defender']\n        ], columns=['P', 'POS'])\n    ],\n    ie_funcs=[\n        ['get_match_result', get_match_result, [str, str, int, int], [str]],\n        ['get_player_position', get_player_position, [str], [str]]\n    ]\n\n)\n\n'?match_result(HT,AT,HG,AG,RESULT)'\n\n\n\n\n\n\n\nHT\nAT\nHG\nAG\nRESULT\n\n\n\n\nBarcelona\nReal Madrid\n3\n2\nHome Win\n\n\nJuventus\nLiverpool\n0\n1\nAway Win\n\n\nLiverpool\nJuventus\n2\n0\nHome Win\n\n\nReal Madrid\nBarcelona\n1\n1\nDraw\n\n\n\n\n\n\n\n\n\n'?player_position(P,POS)'\n\n\n\n\n\n\n\nP\nPOS\n\n\n\n\nCristiano Ronaldo\nForward\n\n\nLionel Messi\nForward\n\n\nSergio Ramos\nDefender\n\n\nVirgil van Dijk\nDefender\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ndef calc_economic_index(interest, inflation):\n    yield interest + inflation\n\ndef classify_economic_index(index):\n    if index &gt; 15:\n        yield \"Bad\"\n    else:\n        yield \"Good\"\n\ntest_session(\n    \"\"\"\n    new economy_data(int, int, int)  # year, interest, inflation\n    economy_data(2000, 4, 3)\n    economy_data(2008, 8, 10)\n    economy_data(2019, 3, 2)\n\n    complex_economic_status(YEAR, STATUS) &lt;- \n        economy_data(YEAR, INTEREST, INFLATION),\n        calc_economic_index(INTEREST, INFLATION) -&gt; (INDEX),\n        classify_economic_index(INDEX) -&gt; (STATUS).\n\n    ?complex_economic_status(YEAR, STATUS)\n    \"\"\",\n    pd.DataFrame([\n        [2000, 'Good'],\n        [2008, 'Bad'],\n        [2019, 'Good']\n    ], columns=['YEAR', 'STATUS']),\n    ie_funcs=[\n        ['calc_economic_index', calc_economic_index, [int, int], [int]],\n        ['classify_economic_index', classify_economic_index, [int], [str]]\n    ]\n)\n\n'?complex_economic_status(YEAR,STATUS)'\n\n\n\n\n\n\n\nYEAR\nSTATUS\n\n\n\n\n2000\nGood\n\n\n2008\nBad\n\n\n2019\nGood\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ndef calc_travel_time(distance):\n    yield int(distance / 1000)\n\ndef analyze_elements(elements):\n    score = 0\n    if 'Carbon' in elements:\n        score += 5\n    if 'Water' in elements:\n        score += 5\n    yield score\n\n\ndef calculate_life_probability(age, element_score, travel_time):\n    yield int((element_score * 2) - (age / 1000) - travel_time)\n\n\ntest_session(\n    \"\"\"\n    new exoplanet(str, int, int, str)  # name, distance, age, elements\n    exoplanet(\"PlanetA\", 4000, 10000, \"Carbon,Water\")\n    exoplanet(\"PlanetB\", 9000, 5000, \"Hydrogen,Silicon\")\n\n    travel_time(NAME, TIME) &lt;- exoplanet(NAME, DIST, AGE, ELEM), calc_travel_time(DIST) -&gt; (TIME).\n    element_score(NAME, SCORE) &lt;- exoplanet(NAME, DIST, AGE, ELEM), analyze_elements(ELEM) -&gt; (SCORE).\n    life_probability(NAME, PROB) &lt;- exoplanet(NAME, DIST, AGE, ELEM), travel_time(NAME, TIME), element_score(NAME, SCORE), calculate_life_probability(AGE, SCORE, TIME) -&gt; (PROB).\n    ?life_probability(NAME, PROB)\n    \"\"\",\n    pd.DataFrame([\n        ['PlanetA', 6],\n        ['PlanetB', -14]\n    ], columns=['NAME', 'PROB']),\n    ie_funcs=[\n        ['calc_travel_time', calc_travel_time, [int], [int]],\n        ['analyze_elements', analyze_elements, [str], [int]],\n        ['calculate_life_probability', calculate_life_probability, [int, int, int], [int]]\n    ]\n)\n\n'?life_probability(NAME,PROB)'\n\n\n\n\n\n\n\nNAME\nPROB\n\n\n\n\nPlanetA\n6\n\n\nPlanetB\n-14\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ndef ie_func1(age):\n    yield age + 1\n\ndef ie_func2(course, increased_age):\n    if increased_age &gt; 26:\n        yield f\"Senior-{course}\"\n    else:\n        yield f\"Junior-{course}\"\n\ntest_session(\n    \"\"\"\n    new student(str, int)\n    student(\"Alice\", 25)\n    student(\"Bob\", 22)\n    student(\"Charlie\", 28)\n    \n    new course(str, str)\n    course(\"CS101\", \"Computer Science\")\n    course(\"EN101\", \"English\")\n    course(\"MA101\", \"Mathematics\")\n    \n    new enrolled(str, str)\n    enrolled(\"Alice\", \"CS101\")\n    enrolled(\"Alice\", \"EN101\")\n    enrolled(\"Bob\", \"MA101\")\n    enrolled(\"Charlie\", \"EN101\")\n    \n    sample_rule3(S, A, G) &lt;- student(S, A), enrolled(S, C), course(C, T), ie_func1(A) -&gt; (IA), ie_func2(T, IA) -&gt; (G).\n                            \n    ?sample_rule3(S, A, G)\n    \"\"\",\n    pd.DataFrame({\n        'S': ['Alice', 'Alice', 'Bob', 'Charlie'],\n        'A': [25, 25, 22, 28],\n        'G': ['Junior-Computer Science', 'Junior-English', 'Junior-Mathematics', 'Senior-English']\n    }),\n    ie_funcs=[['ie_func1', ie_func1, [int], [int]], ['ie_func2', ie_func2, [str, int], [str]]]\n)\n\n'?sample_rule3(S,A,G)'\n\n\n\n\n\n\n\nS\nA\nG\n\n\n\n\nAlice\n25\nJunior-Computer Science\n\n\nAlice\n25\nJunior-English\n\n\nBob\n22\nJunior-Mathematics\n\n\nCharlie\n28\nSenior-English\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ndef ie_func1(x):\n    yield x * 2\n\ndef ie_func2(x, y):\n    yield x + y\n\ndef ie_func3(x):\n    yield x - 1\n\ndef ie_func4(x, y):\n    yield x * y\n\ntest_session(\n    \"\"\"\n    new rel1(int)\n    new rel2(int)\n    new rel3(int)\n    new rel4(int)\n    rel1(1)\n    rel1(2)\n    rel2(1)\n    rel2(2)\n    rel3(2)\n    rel4(3)\n\n    sample_rule1(X, Y1, Y2, Z1, Z2, W) &lt;- rel1(Y1), rel2(Z1), ie_func1(Z1) -&gt; (Y2), rel3(Y2),\n        ie_func2(Y1, Y2) -&gt; (Z2),rel4(Z2),ie_func3(Z2) -&gt; (W),ie_func4(Y1, W) -&gt; (X).\n\n    ?sample_rule1(X, Y1, Y2, Z1, Z2, W)\n    \"\"\",\n    pd.DataFrame({\n        'X': [2],  \n        'Y1': [1],\n        'Y2': [2],\n        'Z1': [1],\n        'Z2': [3],\n        'W': [2]\n    }),\n    ie_funcs=[\n        ['ie_func1', ie_func1, [int], [int]],\n        ['ie_func2', ie_func2, [int, int], [int]],\n        ['ie_func3', ie_func3, [int], [int]],\n        ['ie_func4', ie_func4, [int, int], [int]]\n    ]\n)\n\n'?sample_rule1(X,Y1,Y2,Z1,Z2,W)'\n\n\n\n\n\n\n\nX\nY1\nY2\nZ1\nZ2\nW\n\n\n\n\n2\n1\n2\n1\n3\n2\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;"
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "Reference",
    "section": "",
    "text": "Documentation for the internal implementation of spannerlib is still under construction.\nFeel free to browse the source code if you are interested in implementation details.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "magic_system.html",
    "href": "magic_system.html",
    "title": "spannerlog Magic",
    "section": "",
    "text": "source\n\nset_magic_session\n\n set_magic_session (session:spannerlib.session.Session)\n\n*Changes the session used by the magic system to the one provided\nArgs: session (Session): the session to use in the magic system*\n\nsource\n\n\nget_magic_session\n\n get_magic_session ()\n\n*Returns the session used by the magic system\nReturns: Session*\n\nsess_1 = Session()\nsess_1\n\n&lt;spannerlib.session.Session&gt;\n\n\n\nassert get_magic_session()==get_magic_session()\nassert get_magic_session()!=sess_1\nset_magic_session(sess_1)\nassert get_magic_session()==sess_1\n\n#TODO from here add to parser, if we have curly braces try to get var name from global\n\n\nsource\n\n\nparse_options\n\n parse_options (line)\n\n\nsource\n\n\ncurly_brace_to_glob_var\n\n curly_brace_to_glob_var (line:str)\n\nIf the line is a variable name enclosed in curly braces, return the variable from the global scope\n\ntest = \"heyyyy.txt\"\nr = parse_options(\"-o test, -a {test}\")\nassert r == {'output': 'test,', 'append': 'heyyyy.txt'}\n\nr = parse_options(\"-o test\")\nassert r == {'output': 'test'}\n\n\nsource\n\n\nload_ipython_extension\n\n load_ipython_extension (ipython)\n\n\nsource\n\n\nspannerlogMagic\n\n spannerlogMagic (shell=None, **kwargs)\n\n*Base class for implementing magic functions.\nShell functions which can be reached as %function_name. All magic functions should accept a string, which they can parse for their own needs. This can make some functions easier to type, eg %cd ../ vs. %cd(\"../\")\nClasses providing magic functions need to subclass this class, and they MUST:\n\nUse the method decorators @line_magic and @cell_magic to decorate individual methods as magic functions, AND\nUse the class decorator @magics_class to ensure that the magic methods are properly registered at the instance level upon instance initialization.\n\nSee :mod:magic_functions for examples of actual implementation classes.*\n\nsource\n\n\nclean_query_lines\n\n clean_query_lines (code)\n\nremove query lines from code by removing lines that start with ‘?’\n\ntry:\n    load_ipython_extension(get_ipython())\n    logger.warning(\"spannerlog magic loaded\")\nexcept (AttributeError, ImportError):\n    pass\n\nspannerlog magic loaded\n\n\n\nnew A(int)\nA(1)\n?A(X)\n\n'?A(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\nsession = get_magic_session()\nassert_df_equals(session.export(\"?A(X)\"),pd.DataFrame({'X':[1]}))\n\n\n\n\n\n\n\n\nX\n\n\n\n\n0\n1\n\n\n\n\n\n\n\n\nnew A(int)\n?A(X)\n\n'?A(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\nnew B(int)\n\n\nnew B(int)\n\n\nassert Path('text.slog').read_text() == 'new A(int)\\n\\nnew B(int)\\n\\n'\nPath('text.slog').unlink()\n\n\nwrite_to_file = 'text2.slog'\n\n\nnew A(int)\n\n\nnew B(int)\n\n\nassert Path(write_to_file).read_text() == 'new A(int)\\n\\nnew B(int)\\n\\n'\nPath(write_to_file).unlink()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Spannerlib",
    "section": "",
    "text": "Welcome to the spannerlib project.\nThe spannerlib is a framework for building programming languages that are a combination of imperative and declarative languages. This combination is based off of derivations of the document spanner model.\nCurrently, we implement a language called spannerlog over python. spannerlog is an extension of statically types datalog which allows users to define their own ie functions which can be used to derive new structured information from relations.\nThe spannerlog repl, shown bellow is served using the jupyter magic commands\nBellow, we will show you how to install and use spannerlog through Spannerlib.\nFor more comprehensive walkthroughs, see our tutorials section.",
    "crumbs": [
      "Welcome to Spannerlib"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Welcome to Spannerlib",
    "section": "Installation",
    "text": "Installation\n\nUnix\nTo download and install RGXLog run the following commands in your terminal:\ngit clone https://github.com/DeanLight/spannerlib\ncd spannerlib\n\npip install -e .\ndownload corenlp to spannerlib/rgxlog/\nfrom this link\n# verify everything worked\n# first time might take a couple of minutes since run time assets are being configured\npython nbdev_test.py\n\n\ndocker\ngit clone https://github.com/DeanLight/spannerlib\ncd spannerlib\ndownload corenlp to spannerlib/rgxlog/\nfrom this link\ndocker build . -t spannerlib_image\n\n# on windows, change `pwd to current working directory`\n# to get a bash terminal to the container\ndocker run --name swc --rm -it \\\n  -v `pwd`:/spannerlib:Z \\\n  spannerlib_image bash\n\n# to run an interactive notebook on host port 8891\ndocker run --name swc --rm -it \\\n  -v `pwd`:/spannerlib:Z \\\n  -p8891:8888 \\\n  spannerlib_image jupyter notebook --no-browser --allow-root\n\n#Verify tests inside the container\npython /spannerlib/nbdev_test.py",
    "crumbs": [
      "Welcome to Spannerlib"
    ]
  },
  {
    "objectID": "index.html#getting-started---tldr",
    "href": "index.html#getting-started---tldr",
    "title": "Welcome to Spannerlib",
    "section": "Getting started - TLDR",
    "text": "Getting started - TLDR\nHere is a TLDR intro, for a more comprehensive tutorial, please see the introduction section of the tutorials.\n\nimport spannerlib\nimport pandas as pd\n# get dynamic access to the session running through the jupyter magic system\nfrom spannerlib import get_magic_session\nsession = get_magic_session()\n\nGet a dataframe\n\nlecturer_df = pd.DataFrame(\n    [[\"walter\",\"chemistry\"],\n     [\"linus\", \"operating_systems\"],\n     ['rick', 'physics']\n    ],columns=[\"name\",\"course\"])\nlecturer_df\n\n\n\n\n\n\n\n\nname\ncourse\n\n\n\n\n0\nwalter\nchemistry\n\n\n1\nlinus\noperating_systems\n\n\n2\nrick\nphysics\n\n\n\n\n\n\n\nOr a CSV\n\npd.read_csv('sample_data/example_students.csv',names=[\"name\",\"course\"])\n\n\n\n\n\n\n\n\nname\ncourse\n\n\n\n\n0\nabigail\nchemistry\n\n\n1\nabigail\noperation systems\n\n\n2\njordan\nchemistry\n\n\n3\ngale\noperation systems\n\n\n4\nhoward\nchemistry\n\n\n5\nhoward\nphysics\n\n\n\n\n\n\n\nImport them to the session\n\nsession.import_rel(\"lecturer\",lecturer_df)\nsession.import_rel(\"enrolled\",\"sample_data/enrolled.csv\",delim=\",\")\n\nThey can even be documents\n\ndocuments = pd.DataFrame([\n    [\"abigail is happy, but walter did not approve\"],\n    [\"howard is happy, gale is happy, but jordan is sad\"]\n])\nsession.import_rel(\"documents\",documents)\n\n\n?documents(X)\n\n'?documents(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\nabigail is happy, but walter did not approve\n\n\nhoward is happy, gale is happy, but jordan is sad\n\n\n\n\n\n\n\n\n\nDefine your own IE functions to extract information from relations\n\n# the function itself, writing it as a python generator makes your data processing lazy\ndef get_happy(text):\n    \"\"\"\n    get the names of people who are happy in `text`\n    \"\"\"\n    import re\n\n    compiled_rgx = re.compile(\"(\\w+) is happy\")\n    num_groups = compiled_rgx.groups\n    for match in re.finditer(compiled_rgx, text):\n        if num_groups == 0:\n            matched_strings = [match.group()]\n        else:\n            matched_strings = [group for group in match.groups()]\n        yield matched_strings\n\n# register the ie function with the session\nsession.register(\n    \"get_happy\", # name of the function\n    get_happy, # the function itself\n    [str], # input types\n    [str] # output types\n)\n\nrgxlog supports relations over the following primitive types * strings * spans * integers\nWrite a rgxlog program (like datalog but you can use your own ie functions)\n\nsession.remove_all_rules()\n\n\n# you can also define data inline via a statically typed variant of datalog syntax\nnew sad_lecturers(str)\nsad_lecturers(\"walter\")\nsad_lecturers(\"linus\")\n\n# and include primitive variable\ngpa_doc = \"abigail 100 jordan 80 gale 79 howard 60\"\n\n# define datalog rules\nenrolled_in_chemistry(X) &lt;- enrolled(X, \"chemistry\").\nenrolled_in_physics_and_chemistry(X) &lt;- enrolled_in_chemistry(X), enrolled(X, \"physics\").\n\n# and query them inline (to print to screen)\n# ?enrolled_in_chemistry(\"jordan\") # returns empty tuple ()\n# ?enrolled_in_chemistry(\"gale\") # returns nothing\n# ?enrolled_in_chemistry(X) # returns \"abigail\", \"jordan\" and \"howard\"\n# ?enrolled_in_physics_and_chemistry(X) # returns \"howard\"\n\nlecturer_of(X,Z) &lt;- lecturer(X,Y), enrolled(Z,Y).\n\n# use ie functions in body clauses to extract structured data from unstructured data\n\n# standard ie functions like regex are already registered\nstudent_gpas(Student, Grade) &lt;- \n    rgx(\"(\\w+).*?(\\d+)\",$gpa_doc)-&gt;(StudentSpan, GradeSpan),\n    as_str(StudentSpan)-&gt;(Student), as_str(GradeSpan)-&gt;(Grade).\n\n# and you can use your defined functions as well\nhappy_students_with_sad_lecturers_and_their_gpas(Student, Grade, Lecturer) &lt;-\n    documents(Doc),\n    get_happy(Doc)-&gt;(Student),\n    sad_lecturers(Lecturer),\n    lecturer_of(Lecturer,Student),\n    student_gpas(Student, Grade).\n\nAnd query it\n\n?happy_students_with_sad_lecturers_and_their_gpas(Stu,Gpa,Lec)\n\n'?happy_students_with_sad_lecturers_and_their_gpas(Stu,Gpa,Lec)'\n\n\n\n\n\n\n\nStu\nGpa\nLec\n\n\n\n\nabigail\n100\nlinus\n\n\ngale\n79\nlinus\n\n\nhoward\n60\nwalter\n\n\n\n\n\n\n\n\n\nYou can also get query results as Dataframes for downstream processing\n\ndf = session.export(\n    \"?happy_students_with_sad_lecturers_and_their_gpas(Stu,Gpa,Lec)\")\ndf\n\n\n\n\n\n\n\n\nStu\nGpa\nLec\n\n\n\n\n0\nabigail\n100\nlinus\n\n\n1\ngale\n79\nlinus\n\n\n2\nhoward\n60\nwalter",
    "crumbs": [
      "Welcome to Spannerlib"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Welcome to Spannerlib",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nRelevant papers\n\nspannerlog\nRecursive RGXLog",
    "crumbs": [
      "Welcome to Spannerlib"
    ]
  },
  {
    "objectID": "engine.html",
    "href": "engine.html",
    "title": "Engine",
    "section": "",
    "text": "df = pd.DataFrame([\n    [1,'2fs'],[3,4]\n])\nassert list(_pd_drop_row(df,[3,4]).itertuples(index=False,name=None))==[(1,'2fs')]\n\n\nsource"
  },
  {
    "objectID": "engine.html#helper-functions",
    "href": "engine.html#helper-functions",
    "title": "Engine",
    "section": "",
    "text": "df = pd.DataFrame([\n    [1,'2fs'],[3,4]\n])\nassert list(_pd_drop_row(df,[3,4]).itertuples(index=False,name=None))==[(1,'2fs')]\n\n\nsource"
  },
  {
    "objectID": "engine.html#engine-class",
    "href": "engine.html#engine-class",
    "title": "Engine",
    "section": "Engine Class",
    "text": "Engine Class\n\nsource\n\nEngine\n\n Engine (rewrites=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nTest\n\ns = pd.DataFrame([\n    [1,1],\n    [2,2],\n    [3,3],\n    [4,5]\n])\n\ns2 = pd.DataFrame([\n    [1,2,3],\n    [2,3,4],\n    [3,4,5],\n    [4,5,6]\n])\n\n\nr1 = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='S', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n        Relation(name='S2', terms=[FreeVar(name='X'), FreeVar(name='A'),3]),\n    ])\n\nr2 = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='S', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n        IERelation(name='T', in_terms=[FreeVar(name='X'),FreeVar(name='Y')], out_terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\nr3 = Rule(\n    head=Relation(name='R2', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='S3', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n        Relation(name='S2', terms=[FreeVar(name='X'), FreeVar(name='A'),1]),\n    ])\n\n\nrec_r1 = Rule(\n    head=Relation(name='A', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='B', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\nrec_r2 = Rule(\n    head=Relation(name='A', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='C', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\nrec_r3 = Rule(\n    head=Relation(name='B', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='D', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\nrec_r4 = Rule(\n    head=Relation(name='B', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='A', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\n\ne = Engine()\ne.set_relation(RelationDefinition(name='S', scheme=[int,int]))\ne.set_relation(RelationDefinition(name='S2', scheme=[int,int,int]))\ne.set_relation(RelationDefinition(name='S3', scheme=[int,int]))\n\ne.add_rule(r1,RelationDefinition(name='R', scheme=[int,int]))\ne.add_rule(r2,RelationDefinition(name='R', scheme=[int,int]))\ne.add_rule(r3,RelationDefinition(name='R2', scheme=[int,int]))\n\n\ne.add_facts('S',s)\ne.add_facts('S2',s2)\n\n\ne.db['S']\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n4\n5\n\n\n1\n1\n1\n\n\n2\n3\n3\n\n\n3\n2\n2\n\n\n\n\n\n\n\n\ne.db['S2']\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n3\n4\n5\n\n\n1\n1\n2\n3\n\n\n2\n2\n3\n4\n\n\n3\n4\n5\n6\n\n\n\n\n\n\n\n\ndraw(e.term_graph)\n\n\n\n\n\ne.rules_to_ids\n\n{'R(X,Y) &lt;- S(X,Y),S2(X,A,3).': (0, 'R'),\n 'R(X,Y) &lt;- S(X,Y),T(X,Y) -&gt; (X,Y).': (1, 'R'),\n 'R2(X,Y) &lt;- S3(X,Y),S2(X,A,1).': (2, 'R2')}\n\n\n\ne.del_rule(pretty(r3))\n\n\ndraw(e.term_graph)\n\n\n\n\n\n# assert serialize_graph(e.term_graph) ==([('S', {'rel': 'S', 'rule_id': {0, 1}}),\n#   ('S2', {'rel': 'S2', 'rule_id': {0}}),\n#   ('R', {'rel': 'R', 'op': 'union', 'rule_id': {0, 1}}),\n#   (0, {'op': 'rename', 'names': [(0, 'X'), (1, 'Y')], 'rule_id': {0}}),\n#   (1, {'op': 'rename', 'names': [(0, 'X'), (1, 'A')], 'rule_id': {0}}),\n#   (2, {'op': 'select', 'theta': [(2, 3)], 'rule_id': {0}}),\n#   (3, {'op': 'join', 'rule_id': {0}}),\n#   (4, {'op': 'project', 'on': ['X', 'Y'], 'rel': '_R_0', 'rule_id': {0}}),\n#   (8, {'op': 'rename', 'names': [(0, 'X'), (1, 'Y')], 'rule_id': {1}}),\n#   (9, {'op': 'project', 'on': ['X', 'Y'], 'rule_id': {1}}),\n#   (10, {'op': 'calc', 'func': 'T', 'rule_id': {1}}),\n#   (11, {'op': 'rename', 'names': [(0, 'X'), (1, 'Y')], 'rule_id': {1}}),\n#   (6, {'op': 'join', 'rule_id': {1}}),\n#   (7, {'op': 'project', 'on': ['X', 'Y'], 'rel': '_R_1', 'rule_id': {1}})],\n#  [('R', 4, {}),\n#   ('R', 7, {}),\n#   (0, 'S', {}),\n#   (1, 'S2', {}),\n#   (2, 1, {}),\n#   (3, 0, {}),\n#   (3, 2, {}),\n#   (4, 3, {}),\n#   (8, 'S', {}),\n#   (9, 8, {}),\n#   (10, 9, {}),\n#   (11, 10, {}),\n#   (6, 8, {}),\n#   (6, 11, {}),\n#   (7, 6, {})])\n\n\ne.add_rule(rec_r1,RelationDefinition(name='A', scheme=[int,int]))\ne.add_rule(rec_r2,RelationDefinition(name='A', scheme=[int,int]))\ne.add_rule(rec_r3,RelationDefinition(name='B', scheme=[int,int]))\ne.add_rule(rec_r4,RelationDefinition(name='B', scheme=[int,int]))\n\n\ndraw(e.term_graph)\n\n\n\n\n\ne.del_head('R')\n\n\nassert e.rules_to_ids == {'A(X,Y) &lt;- B(X,Y).': (3, 'A'),\n 'A(X,Y) &lt;- C(X,Y).': (4, 'A'),\n 'B(X,Y) &lt;- D(X,Y).': (5, 'B'),\n 'B(X,Y) &lt;- A(X,Y).': (6, 'B')}\n\n\ndraw(e.term_graph)"
  },
  {
    "objectID": "engine.html#naive-execution",
    "href": "engine.html#naive-execution",
    "title": "Engine",
    "section": "Naive execution",
    "text": "Naive execution\nA recursive least fixed point logic algorithm mimicing the bottom up evalutation.\n\nsource\n\nget_rel\n\n get_rel (rel, db, **kwargs)\n\n\nsource\n\n\ncompute_node\n\n compute_node (G, root, ret_inter=False)\n\n\nsource\n\n\ncompute_recursive_node\n\n compute_recursive_node (G, u, results, stack=None)\n\n\nsource\n\n\ncompute_acyclic_node\n\n compute_acyclic_node (G, u, results, stack=None)\n\n\nTest - path query\n\ngraph  = nx.DiGraph()\ngraph.add_nodes_from([\n    0,1,2,3,\n])\ngraph.add_edges_from(\n    [(0,1),(0,2),(1,3),(2,3),(3,4)]\n)\ndraw(graph)\nedges_df = pd.DataFrame(list(graph.edges),columns=['S','T'])\nedges_df\ndb = DB({\n    'edges':edges_df\n})\n\n\n\n\n\nexpected_paths = pd.DataFrame(\n    [\n        [0,1],\n        [0,2],\n        [1,3],\n        [2,3],\n        [3,4],\n        [0,3],\n        [0,4],\n        [1,4],\n        [2,4]\n    ],\n    columns=['S','T']\n)\n\n\ng = nx.DiGraph()\ng.add_nodes_from([\n    ('edges',{'rel':'edges','op':'get_rel','db':db}),\n    (1,{'op':'rename','schema':['S','T']}),\n    (2,{'op':'rename','schema':['S','X']}),\n    (3,{'op':'rename','schema':['X','T']}),\n    (4,{'op':'join','schema':['S','X','T']}),\n    (5,{'op':'project','schema':['S','T']}),\n    ('reachable',{'op':'union','schema':[0,1]}),\n    (6,{'op':'rename','schema':['S','T']})]\n)\ng.add_edges_from([\n    (1,'edges'),\n    (2,'edges'),\n    (4,2),\n    (4,3),\n    (5,4),\n    ('reachable',5),\n    ('reachable',1),\n    (3,'reachable'),\n    (6,'reachable')\n])\ndraw(g)\n\n\n\n\n\nroot = 6\n# with checkLogs():\nres,inter = compute_node(g,root,True)\nassert_df_equals(res,expected_paths)\n\n\n\n\n\n\n\n\nS\nT\n\n\n\n\n0\n0\n1\n\n\n1\n2\n4\n\n\n2\n0\n4\n\n\n3\n3\n4\n\n\n4\n0\n3\n\n\n5\n1\n4\n\n\n6\n2\n3\n\n\n7\n0\n2\n\n\n8\n1\n3"
  },
  {
    "objectID": "engine.html#e2e-tests",
    "href": "engine.html#e2e-tests",
    "title": "Engine",
    "section": "e2e tests",
    "text": "e2e tests\n\nCase 0 - path queries\n\ne=Engine()\ne.set_relation(RelationDefinition(name='edges',scheme=[int,int]))\ne.add_facts('edges',edges_df)\n\nbase_rule = Rule(\n    head=Relation(name='reachable',terms=[FreeVar(name='S'),FreeVar(name='T')]),\n    body=[\n        Relation(name='edges',terms=[FreeVar(name='S'),FreeVar(name='T')]),\n    ])\n\nrec_rule = Rule(\n    head=Relation(name='reachable',terms=[FreeVar(name='S'),FreeVar(name='T')]),\n    body=[\n        Relation(name='edges',terms=[FreeVar(name='S'),FreeVar(name='X')]),\n        Relation(name='reachable',terms=[FreeVar(name='X'),FreeVar(name='T')]),\n    ])\n\ne.add_rule(base_rule,RelationDefinition(name='reachable',scheme=[int,int]))\ne.add_rule(rec_rule,RelationDefinition(name='reachable',scheme=[int,int]))\n\nprint(e.rules_to_ids)\n\n{'reachable(S,T) &lt;- edges(S,T).': (0, 'reachable'), 'reachable(S,T) &lt;- edges(S,X),reachable(X,T).': (1, 'reachable')}\n\n\n\nq,r = e.plan_query(Relation(name='reachable',terms=[FreeVar(name='S'),FreeVar(name='T')]))\ndraw(q)\n# with checkLogs():\nres,inter = e.run_query(Relation(name='reachable',terms=[FreeVar(name='S'),FreeVar(name='T')]),return_intermediate=True)\n\nassert_df_equals(res,expected_paths)\n\n\n\n\n\n\n\n\n\n\n\nS\nT\n\n\n\n\n0\n0\n1\n\n\n1\n2\n4\n\n\n2\n0\n4\n\n\n3\n3\n4\n\n\n4\n0\n3\n\n\n5\n1\n4\n\n\n6\n2\n3\n\n\n7\n0\n2\n\n\n8\n1\n3\n\n\n\n\n\n\n\n\n# make sure we actually got the intermediate results\nassert len(inter)!=0\n\n\n\nCase1\n\ne = Engine()\ne.set_relation(RelationDefinition(name='S', scheme=[int,int]))\ne.set_relation(RelationDefinition(name='S2', scheme=[int,int,int]))\n\ne.add_rule(r1,RelationDefinition(name='R', scheme=[int,int]))\ne.add_rule(r2,RelationDefinition(name='R', scheme=[int,int]))\n\ne.add_facts('S',s)\ne.add_facts('S2',s2)\n\ndef func(x,y):\n    return [(y,x)]\n\nie_def = IEFunction(name='T',func=func,in_schema=[int,int],out_schema=[int,int])\n\ne.set_ie_function(ie_def)\ng = e._inline_db_and_ies_in_graph(e.term_graph)\nprint(e.rules_to_ids)\ndisplay(s)\ndisplay(s2)\n# draw(g)\n\n{'R(X,Y) &lt;- S(X,Y),S2(X,A,3).': (0, 'R'), 'R(X,Y) &lt;- S(X,Y),T(X,Y) -&gt; (X,Y).': (1, 'R')}\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n1\n\n\n1\n2\n2\n\n\n2\n3\n3\n\n\n3\n4\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5\n\n\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nres = e.run_query(Relation(name='R',terms=[FreeVar(name='X'),FreeVar(name='Y')]))\nassert_df_equals(res,pd.DataFrame([\n    [1,1],\n    [2,2],\n    [3,3]\n],columns=['X','Y']))\nres\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\n1\n1\n\n\n1\n3\n3\n\n\n2\n2\n2\n\n\n\n\n\n\n\n\nres = e.run_query(Relation(name='R',terms=[FreeVar(name='S'),3]))\nassert_df_equals(res,pd.DataFrame([\n    [3]\n],columns=['S']))\nres\n\n\n\n\n\n\n\n\nS\n\n\n\n\n1\n3\n\n\n\n\n\n\n\n\n\ncase 2\n\ne2 = Engine()\ne2.set_relation(RelationDefinition(name='C', scheme=[int,int]))\ne2.set_relation(RelationDefinition(name='D', scheme=[int,int]))\n\nfor rule in [rec_r1,rec_r2,rec_r3,rec_r4]:\n    e2.add_rule(rule,RelationDefinition(name=rule.head.name, scheme=[int,int]))\n\ne2.add_fact(Relation(name='C',terms=[1,2]))\ne2.add_fact(Relation(name='D',terms=[3,4]))\n\ng2 = e2._inline_db_and_ies_in_graph(e2.term_graph)\ne2.rules_to_ids\n\n{'A(X,Y) &lt;- B(X,Y).': (0, 'A'),\n 'A(X,Y) &lt;- C(X,Y).': (1, 'A'),\n 'B(X,Y) &lt;- D(X,Y).': (2, 'B'),\n 'B(X,Y) &lt;- A(X,Y).': (3, 'B')}\n\n\n\n# with checkLogs():\nres = e2.run_query(Relation(name='A',terms=[FreeVar(name='X'),FreeVar(name='Y')]))\nassert_df_equals(res,pd.DataFrame([\n    [3,4],\n    [1,2]\n],columns=['X','Y']))\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\n1\n2\n\n\n1\n3\n4\n\n\n\n\n\n\n\n\nres = e2.run_query(Relation(name='B',terms=[FreeVar(name='X'),FreeVar(name='Y')]))\nassert_df_equals(res,pd.DataFrame([\n    [3,4],\n    [1,2]\n],columns=['X','Y']))\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\n1\n2\n\n\n1\n3\n4\n\n\n\n\n\n\n\n\n\nCase 3\n\ne = Engine()\ne.set_relation(RelationDefinition(name='string', scheme=[str]))\ne.add_fact(Relation(name='string',terms=['a']))\ne.add_fact(Relation(name='string',terms=['aa']))\ndef func(str):\n    yield (len(str),)\n\ne.set_ie_function(IEFunction(name='Length',func=func,in_schema=[str],out_schema=[int]))\n\nr = Rule(\n    head=Relation(name='string_length', terms=[FreeVar(name='Str'), FreeVar(name='Len')]),\n    body=[\n        Relation(name='string', terms=[FreeVar(name='Str')]),\n        IERelation(name='Length', in_terms=[FreeVar(name='Str')], out_terms=[FreeVar(name='Len')]),\n    ])\n\ne.add_rule(r,RelationDefinition(name='string_length', scheme=[str,int]))\n# check that adding the same rule twice does nothing.\ne.add_rule(r,RelationDefinition(name='string_length', scheme=[str,int]))\n\n\n\ng = e._inline_db_and_ies_in_graph(e.term_graph)\nprint(e.rules_to_ids)\ndraw(g)\n\n{'string_length(Str,Len) &lt;- string(Str),Length(Str) -&gt; (Len).': (0, 'string_length')}\n\n\n\n\n\n\n# with checkLogs():\nres = e.run_query(Relation(name='string_length', terms=[FreeVar(name='Str'), FreeVar(name='Len')]))\nassert_df_equals(res,pd.DataFrame([\n    ['a',1],\n    ['aa',2]\n],columns=['Str','Len']))\n\n\n\n\n\n\n\n\nStr\nLen\n\n\n\n\n0\na\n1\n\n\n1\naa\n2\n\n\n\n\n\n\n\n\nAggregation\n\ns3 = pd.DataFrame([\n    [1,2,\"str\"],\n    [1,3,\"str\"],\n    [3,4,\"str\"],\n    [3,5,\"str\"]\n])\n\n\ne = Engine()\ne.set_relation(RelationDefinition(name='S3', scheme=[int,int,str]))\ne.add_facts('S3',s3)\n\ne.set_agg_function(AGGFunction(name='max',func='max',in_schema=[int],out_schema=[int]))\ne.set_agg_function(AGGFunction(name='count',func='count',in_schema=[str],out_schema=[int]))\n\nagg_rule = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y'), FreeVar(name='Z')],\n    agg=[None,'max','count']\n    ),\n    body=[\n        Relation(name='S3', terms=[FreeVar(name='X'),FreeVar(name='Y'),FreeVar(name='Z')]),\n    ])\n\ne.add_rule(agg_rule,RelationDefinition(name='R', scheme=[int,int,int]))\n\ng = e._inline_db_and_ies_in_graph(e.term_graph)\nprint(e.rules_to_ids)\ndraw(g)\n\n{'R(X,max(Y),count(Z)) &lt;- S3(X,Y,Z).': (0, 'R')}\n\n\n\n\n\n\nres = e.run_query(Relation(name='R',terms=[FreeVar(name='X'),FreeVar(name='Y'),FreeVar(name='Z')]))\nassert_df_equals(res,pd.DataFrame([\n    [1,3,2],\n    [3,5,2]\n],columns=['X','Y','Z']))\nres\n\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\n0\n3\n5\n2\n\n\n1\n1\n3\n2"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "#TODOs\n\n# TODO make sure tutorials look good in doc preivew\n# TODO fix documentation for all modules\n# TODO go over all remaining todos\n# TODO remove all corenlp mentions\n# TODO move rust regex module to a different repo"
  },
  {
    "objectID": "utils.html#default-registires",
    "href": "utils.html#default-registires",
    "title": "Utils",
    "section": "Default registires",
    "text": "Default registires\n\nsource\n\nDefaultAGGs\n\n DefaultAGGs ()\n\n\nsource\n\n\nDefaultIEs\n\n DefaultIEs ()\n\n\nsource\n\n\nvisualize_callback_df\n\n visualize_callback_df ()\n\nreturns a dataframe summerising the registered callbacks"
  },
  {
    "objectID": "utils.html#typing-utils",
    "href": "utils.html#typing-utils",
    "title": "Utils",
    "section": "typing utils",
    "text": "typing utils\n\nsource\n\nschema_merge\n\n schema_merge (schema1, schema2)\n\nmerges two schemas, taking the stricter type between the two for each index\n\nsource\n\n\ntype_merge\n\n type_merge (type1, type2)\n\n\nsource\n\n\nis_of_schema\n\n is_of_schema (relation, schema, ignore_types=None)\n\nchecks if a relation is of a given schema\n\nsource\n\n\nschema_match\n\n schema_match (schema, expected, ignore_types=None)\n\nchecks if\n\nassert schema_match([Span,Span,Span],[Span,Span,(str,Span)])\nassert not schema_match([Span,Span,str],[Span,Span,Span])\nassert is_of_schema([Span('aa',0,1),Span('aa',1,2),'a'],[Span,Span,str])\n\nassert is_of_schema([1.4,2,None],[Real,Real,object])\n\nassert schema_merge([Span,Span,str],[Span,Span,str]) == [Span,Span,str]\nassert schema_merge([Span,Span,str],[Span,Span,object]) == [Span,Span,str]\n\nassert is_of_schema([1.4,2,\"he\"],[Real,Real,Real],ignore_types=[str])\n\n\ntype(\"he\")\n\nstr"
  },
  {
    "objectID": "utils.html#test-utils",
    "href": "utils.html#test-utils",
    "title": "Utils",
    "section": "Test utils",
    "text": "Test utils\n\nsource\n\ndf_to_list\n\n df_to_list (df)\n\n\nsource\n\n\nserialize_graph\n\n serialize_graph (g)\n\n\nsource\n\n\nserialize_tree\n\n serialize_tree (g)\n\n\nsource\n\n\nassert_df_equals\n\n assert_df_equals (df1, df2)\n\n\nsource\n\n\nspan_to_str\n\n span_to_str (span)\n\n\nsource\n\n\nassert_df\n\n assert_df (df, values, columns=None)\n\n\nsource\n\n\nserialize_df_values\n\n serialize_df_values (df)\n\n\nassert serialize_df_values(pd.DataFrame([[1,2],[3,4]],columns=['a','b'])) == {(1, 2), (3, 4)}\n\nassert_df(pd.DataFrame([(1,2),(3,4)],columns=['a','b']),{(1, 2), (3, 4)},['a','b'])\n\n\n# Span dataframes\ntext = \"John Doe: 35 years old, Jane Smith: 28 years old\"\nspan_df = pd.DataFrame([\n    [Span(text,0,8),Span(text,10,12)],\n    [Span(text,24,34),Span(text,36,38)]],\n    columns=['col_0','col_1']\n)\nspan_df\n\n\n\n\n\n\n\n\ncol_0\ncol_1\n\n\n\n\n0\n(J, o, h, n, , D, o, e)\n(3, 5)\n\n\n1\n(J, a, n, e, , S, m, i, t, h)\n(2, 8)\n\n\n\n\n\n\n\n\nconverted_df = span_df.map(lambda x: x.as_str())\nconverted_df\n\n\n\n\n\n\n\n\ncol_0\ncol_1\n\n\n\n\n0\nJohn Doe\n35\n\n\n1\nJane Smith\n28\n\n\n\n\n\n\n\n\nstr_df = pd.DataFrame([\n    ['John Doe','35'],\n    ['Jane Smith','28']],\n    columns=['col_0','col_1']\n    )\nstr_df\n\n\n\n\n\n\n\n\ncol_0\ncol_1\n\n\n\n\n0\nJohn Doe\n35\n\n\n1\nJane Smith\n28\n\n\n\n\n\n\n\n\nassert_df_equals(converted_df,str_df)\n\n\n\n\n\n\n\n\ncol_0\ncol_1\n\n\n\n\n0\nJohn Doe\n35\n\n\n1\nJane Smith\n28\n\n\n\n\n\n\n\n\nassert_df_equals(span_df,str_df)\n\n\n\n\n\n\n\n\ncol_0\ncol_1\n\n\n\n\n0\nJohn Doe\n35\n\n\n1\nJane Smith\n28\n\n\n\n\n\n\n\n\nsource\n\n\ncheckLogs\n\n checkLogs (level:int=10, name:str='__main__', toFile=None)\n\n*context manager for temporarily changing logging levels. used for debugging purposes\nArgs: level (logging.Level: optional): logging level to change the logger to. Defaults to logging.DEBUG. name (str: optional): module name to raise logging level for. Defaults to root logger toFile (Path: optional): File to output logs to. Defaults to None\nYields: [logging.Logger]: the logger object that we raised the level of*\n\nsource\n\n\npatch_method\n\n patch_method (func:Callable, *args, **kwargs)\n\nApplies fastcore’s patch decorator and removes func from cls.__abstractsmethods__ in case  func is an abstractmethods"
  },
  {
    "objectID": "utils.html#cli-utils",
    "href": "utils.html#cli-utils",
    "title": "Utils",
    "section": "CLI utils",
    "text": "CLI utils\n\nsource\n\nkill_process_and_children\n\n kill_process_and_children (process:subprocess.Popen)\n\n\nsource\n\n\nrun_cli_command\n\n run_cli_command (command:str, stderr:bool=False, shell:bool=False,\n                  timeout:float=-1)\n\nThis utility can be used to run any cli command, and iterate over the output.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncommand\nstr\n\na single command string\n\n\nstderr\nbool\nFalse\nif true, suppress stderr output. default: False\n\n\nshell\nbool\nFalse\nif true, spawn shell process (e.g. /bin/sh), which allows using system variables (e.g. $HOME),but is considered a security risk (see: https://docs.python.org/3/library/subprocess.html#security-considerations)\n\n\ntimeout\nfloat\n-1\nif positive, kill the process after timeout seconds. default: -1\n\n\nReturns\nIterable\n\nstring iterator"
  },
  {
    "objectID": "utils.html#graph-utils",
    "href": "utils.html#graph-utils",
    "title": "Utils",
    "section": "Graph utils",
    "text": "Graph utils\n\nsource\n\nget_new_node_name\n\n get_new_node_name (g, prefix=None, avoid_names_from=None)\n\n\nsource\n\n\nis_node_in_graphs\n\n is_node_in_graphs (name, gs)\n\n\ng = nx.Graph()\ng.add_node(1)\ng.add_node(2)\ng.add_node('hello')\n\ng2 = nx.Graph()\ng2.add_node(1)\ng2.add_node(2)\ng2.add_node(3)\ng2.add_node('hello_1')\n\nassert _biggest_int_node_name(g) == 2\n\n\nassert is_node_in_graphs(3,[g,g2])\nassert not is_node_in_graphs(4,[g,g2])\n\n\nassert get_new_node_name(g) == 3\nassert get_new_node_name(g,'hello') == 'hello_0'\ng.add_node('hello_0')\nassert get_new_node_name(g,'hello') == 'hello_1'\nassert get_new_node_name(g,'hello',avoid_names_from=[g2]) == 'hello_2'\nassert get_new_node_name(g,'world') == 'world'"
  },
  {
    "objectID": "utils.html#file-system-utils",
    "href": "utils.html#file-system-utils",
    "title": "Utils",
    "section": "file system utils",
    "text": "file system utils\n\nsource\n\nget_git_root\n\n get_git_root (path='.')\n\n\nassert get_git_root() == Path.cwd().parent\n\n\nsource\n\n\nload_env\n\n load_env (path=None)\n\n\nload_env()\n\nLoaded env from .env.dev\n\n\n\nsource\n\n\nget_lib_name\n\n get_lib_name ()\n\n\nsource\n\n\nget_base_file_path\n\n get_base_file_path ()"
  },
  {
    "objectID": "callbacks/rust_spanner_regex.html",
    "href": "callbacks/rust_spanner_regex.html",
    "title": "Rust spanner regex",
    "section": "",
    "text": "# download_and_install_rust_regex()\n\n\nsource\n\nrgx\n\n rgx (regex_pattern:str, out_type:str, text:Optional[str]=None,\n      text_file:Optional[str]=None)\n\nAn IE function which runs regex using rust’s enum-spanner-rs and yields tuples of strings/spans (not both).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nregex_pattern\nstr\n\nthe pattern to run\n\n\nout_type\nstr\n\nstring/span - decides which one will be returned\n\n\ntext\nOptional\nNone\nthe string on which regex is run\n\n\ntext_file\nOptional\nNone\nuse text from this file instead of text. default: None\n\n\nReturns\nIterable\n\na tuple of strings/spans\n\n\n\n\nsource\n\n\nrgx_span\n\n rgx_span (text:str, regex_pattern:str)\n\n*computes the spans of all the matches of the regex pattern in the text\nbased on the enum-spanner-rs rust package*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nThe input text for the regex operation\n\n\nregex_pattern\nstr\nThe pattern of the regex operation\n\n\nReturns\nIterable\ntuples of spans that represents the results\n\n\n\n\nsource\n\n\nrgx_string\n\n rgx_string (text:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nThe input text for the regex operation\n\n\nregex_pattern\nstr\nhe pattern of the regex operation\n\n\nReturns\nIterable\ntuples of strings that represents the results\n\n\n\n\nsource\n\n\nrgx_span_from_file\n\n rgx_span_from_file (text_file:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext_file\nstr\nThe input file for the regex operation\n\n\nregex_pattern\nstr\nThe pattern of the regex operation\n\n\nReturns\nIterable\ntuples of spans that represents the results\n\n\n\n\nsource\n\n\nrgx_string_from_file\n\n rgx_string_from_file (text_file:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext_file\nstr\nThe input file for the regex operation\n\n\nregex_pattern\nstr\nThe pattern of the regex operation\n\n\nReturns\nIterable\ntuples of strings that represents the results"
  },
  {
    "objectID": "callbacks/json_path.html",
    "href": "callbacks/json_path.html",
    "title": "Json Path",
    "section": "",
    "text": "This module contains ie functions for querying json like strings using jsonPath syntax.\n\nsource\n\njson_path\n\n json_path (json_document:str, path_expression:str)\n\nYields the matching sub-documents and the path from the document roots to the match as a tuple\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\njson_document\nstr\nThe document on which we will run the path expression.\n\n\npath_expression\nstr\nThe query to execute.\n\n\n\n\nlist(json_path('{\"foo\": [{\"baz\": 1}, {\"baz\": 2}]}', 'foo[*].baz'))\n\n[('foo.[0].baz', '1'), ('foo.[1].baz', '2')]\n\n\n\n\n\n\n\n\n\nTable 1: Registered Callbacks\n\n\n\n\n\nname\nfunction\ninput_schema\noutput_schema\ntype\n\n\n\n\njson_path\njson_path\n['str', 'str']\n['str', 'str']\nIE Function",
    "crumbs": [
      "Callbacks",
      "JsonPath"
    ]
  },
  {
    "objectID": "tutorials/llm_code_documentation_example.html",
    "href": "tutorials/llm_code_documentation_example.html",
    "title": "Copilot example",
    "section": "",
    "text": "Exported source\n# importing dependencies\nimport re\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pathlib import Path\nfrom spannerlib.utils import load_env\nfrom spannerlib import get_magic_session,Session,Span\n# load openAI api key from env file\nload_env()",
    "crumbs": [
      "Tutorials",
      "Copilot Agent"
    ]
  },
  {
    "objectID": "tutorials/llm_code_documentation_example.html#problem-definition",
    "href": "tutorials/llm_code_documentation_example.html#problem-definition",
    "title": "Copilot example",
    "section": "Problem definition",
    "text": "Problem definition\nGiven:\n\nA collection of python files.\nA cursor position in a python file.\n\nReturn:\n\nA doc string of the python function that wraps the position of our cursor.\n\nWe will reuse our llm and format ie function from the basic tutorial. and introduce some new ie functions, namely:\n\nast_xpath(file,xpath_query)-&gt;(ast_node) that allows us to select ast nodes using xpath queries.\nast_to_span(file,ast_node)-&gt;(ast_span) that allows us to get the span of code described by an ast node of a given file. We will also use an ie functions from the standard libraries.\nexpr_eval(expression_template,val_1,...,val_n)-&gt;(expression_result) allows us to given a template string for a python expression in a printf like format.\n\nused to generate small ie functions easily\nthe usage will become clear when writting our agent bellow.\n\nSpan comparison ie functions such as span_contained(span1,span2)-&gt;(bool) which return true if span1 is a subspan of span2.\n\nWe will also add a text-specific aggregation function:\n\nlex_concat(strings)-&gt;(string) takes a set of strings and concatenate them when they are sorted by lexicographic order.\n\nWe must sort them lexicogrphically since aggregation functions in Datalog must operate under the set semantics\nThis is a limitation of the declerative lanuage we chose for this demo, but it can be overcome by using the spannerlib framework on a callback extension of a richer declerative language list SQL.",
    "crumbs": [
      "Tutorials",
      "Copilot Agent"
    ]
  },
  {
    "objectID": "tutorials/llm_code_documentation_example.html#tldr",
    "href": "tutorials/llm_code_documentation_example.html#tldr",
    "title": "Copilot example",
    "section": "TLDR",
    "text": "TLDR\n%%spannerlog\n# find all function definitions in the code base\nFuncDefSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(tet, \"//FunctionDef\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    expr_eval(\"{0}.name\",node)-&gt;(name).\n\n# find all function calls in the code base\nFuncCallSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(text, \"//Call/func/Name\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    as_str(span)-&gt;(name).\n\n# find the function whose span contains the cursor\nCursorWrappingFunc(cursor,name)&lt;-\n    Cursors(cursor),\n    FuncDefSpan(span,name),\n    span_contained(cursor,span)-&gt;(True).\n\n# get all functions who had a call to called_name inside them\nMentions(lex_concat(caller_span),called_name)&lt;-\n    FuncCallSpan(called_span,called_name),\n    FuncDefSpan(caller_span,caller_name),\n    span_contained(called_span,caller_span)-&gt;(True).\n\nmodel = 'gpt-3.5-turbo'\n# compose current function and all mentions into a prompt and\n# ask the model to generate the documentation for the function\nDocumentFunction(cursor,answer)&lt;-\n    CursorWrappingFunc(cursor,name),\n    Mentions(mentions,name),\n    FuncDefSpan(def_span,name),\n    as_str(def_span)-&gt;(def_string),\n    format($func_document_template,mentions,def_string)-&gt;(prompt),\n    llm($model,prompt)-&gt;(answer).",
    "crumbs": [
      "Tutorials",
      "Copilot Agent"
    ]
  },
  {
    "objectID": "tutorials/llm_code_documentation_example.html#a-full-walkthrough-of-our-implementation.",
    "href": "tutorials/llm_code_documentation_example.html#a-full-walkthrough-of-our-implementation.",
    "title": "Copilot example",
    "section": "A full walkthrough of our implementation.",
    "text": "A full walkthrough of our implementation.\n\nImporting code from previous tutorial\n\nsess = get_magic_session()\nsess.register('llm',llm,[str,str],[str])\nsess.register('format', format_ie, string_schema,[str])\n\n\n\nImplementing novel Callback functions\nIf the implementation details are not of interest, feel free to move to the next section.\nIn order to analyze the structure of the code, we will be using python’s ast module. We will write a very generic ie function that gets a piece of code, and an xpath query string returns the spans of all matches of the query over the ast of the given code.\nTo do so we will use the pyastgrep library that allows us to look for xpath matches in python ast’s. We will write a modified version of it’s main function that returns Spans of the ast nodes.\n\nsource\n\n\nast_xpath\n\n ast_xpath (py_str, xpath_query)\n\n\n\nExported source\nimport ast\nfrom functools import cache\nfrom pyastgrep.search import search_python_files,Match\nfrom pyastgrep.asts import ast_to_xml\nfrom lxml import etree\n\n\n\n\nExported source\n@cache\ndef _py_to_xml(py:str)-&gt;str:\n    ast_tree = ast.parse(py)\n    node_mappings = {}\n    xml_tree = ast_to_xml(ast_tree, node_mappings)\n    return xml_tree,ast_tree,node_mappings\n\ndef _xml_to_string(xml_tree):\n    return etree.tostring(xml_tree, pretty_print=True).decode('utf-8')\n\ndef _print_file_xml(file_path):\n    text = Path(file_path).read_text()\n    xml_tree,_,_ = _py_to_xml(text)\n    print(_xml_to_string(xml_tree))\n\n\ndef _ast_to_string(ast_tree):\n    if isinstance(ast_tree,ast.AST):\n        return ast.unparse(ast_tree)\n    else:\n        return ast_tree\n\ndef ast_xpath(py_str,xpath_query):\n    if isinstance(py_str,Path):\n        py_str = py_str.read_text()\n    if isinstance(py_str,Span):\n        py_str = str(py_str)\n    xml_tree,ast_tree,node_mappings = _py_to_xml(py_str)\n    xml_matches = xml_tree.xpath(xpath_query)\n    ast_matches = [node_mappings[match] if match in node_mappings else match for match in xml_matches]\n    return ast_matches\n\n\n\ncode_file = Path('copilot_data/example_code.py')\ncode_text = code_file.read_text()\nprint(code_text)\n\ndef f(x,y):\n    x+y \n\ndef g(x,y):\n    return f(x,y)**2\n\nclass A:\n    def __init__(self, x):\n        self.x = x\n    def method(self, y):\n        return f(self.x, y)\n\nprint(f(2,3))\n\n\n\nx_t,a_t,n_m = _py_to_xml(code_text)\n_print_file_xml(code_file)\n# we do not present the output xml here as it is too long\n\n\nprint(_ast_to_string(a_t))\n\ndef f(x, y):\n    x + y\n\ndef g(x, y):\n    return f(x, y) ** 2\n\nclass A:\n\n    def __init__(self, x):\n        self.x = x\n\n    def method(self, y):\n        return f(self.x, y)\nprint(f(2, 3))\n\n\n\nfor match in ast_xpath(code_file,'//FunctionDef'):\n    print(_ast_to_string(match))\n\nprint(\"=\"*80)\nfor match in ast_xpath(code_text,'//FunctionDef/@name'):\n    print(_ast_to_string(match))\n\ndef f(x, y):\n    x + y\ndef g(x, y):\n    return f(x, y) ** 2\ndef __init__(self, x):\n    self.x = x\ndef method(self, y):\n    return f(self.x, y)\n================================================================================\nf\ng\n__init__\nmethod\n\n\n\nsource\n\n\nast_to_span\n\n ast_to_span (string, node)\n\ngiven a node  of an ast from file , returns the location of the node in the file as a Span object\n\n\nExported source\n@cache\ndef _get_lines(path):\n    if isinstance(path,Path):\n        tuple(path.read_text().split('\\n'))\n    else:\n        return tuple(path.split('\\n'))\n\ndef _get_character_position(path, line_number, column_offset):\n    \"\"\"gets a character position from a line number and column offset\"\"\"\n    lines = _get_lines(path)\n    if line_number &lt; 1 or line_number &gt; len(lines):\n        raise ValueError(\"Invalid line number\")\n    line = lines[line_number - 1]\n    if column_offset &lt; 0 or column_offset &gt; len(line):\n        raise ValueError(\"Invalid column offset\")\n    return sum(len(lines[i]) + 1 for i in range(line_number - 1)) + column_offset\n\ndef ast_to_span(string,node):\n    \"\"\"given a node &lt;node&gt; of an ast from file &lt;path&gt;,\n    returns the location of the node in the file as a Span object\"\"\"\n    if isinstance(string,Path):\n        text = string.read_text()\n        name = string.name\n    else:\n        text = string\n        name = None\n    start = _get_character_position(str(text),node.lineno,node.col_offset)\n    if hasattr(node,'end_lineno') and hasattr(node,'end_col_offset'):\n        end = _get_character_position(str(text),node.end_lineno,node.end_col_offset)\n    else:\n        end = start + len(ast.unparse(node))\n    return [Span(text,start,end,name=name)]\n\n\n\nmatches = ast_xpath(code_text,'//FunctionDef')\nm = matches[0]\n\n\nspan = ast_to_span(code_file,m)[0]\nspan,str(span)\n\n([@example_code.py,0,19) \"def f(x,y)...\", 'def f(x,y):\\n    x+y')\n\n\n\nsource\n\n\nlex_concat\n\n lex_concat (strings)\n\n\n\nExported source\ndef lex_concat(strings):\n    return '\\n'.join(sorted([str(s) for s in strings]))\n\n\n\n# note that we use the ast node class directly in spannerlib since we want to access node attributes as well\nsess.register('ast_xpath',ast_xpath,[(str,Path,Span),str],[ast.AST])\nsess.register('ast_to_span',ast_to_span,[(str,Span,Path),ast.AST],[Span])\nsess.register_agg('lex_concat',lex_concat,[(str,Span)],[str])\n\n\n\nUsing our ie functions\n\n# non primitve variables cannot be initiallzed from within spannerlob, so we define\n# a path variable from the outside.\nsess.import_var('code_file',code_file)\n\n\nExampleAST(span,string)&lt;-\n    ast_xpath($code_file,'//FunctionDef')-&gt;(node),\n    ast_to_span($code_file,node)-&gt;(span),\n    as_str(span)-&gt;(string).\n\n?ExampleAST(span,string)\n\n'?ExampleAST(span,string)'\n\n\n\n\n\n\n\nspan\nstring\n\n\n\n\n[@example_code.py,0,19) \"def f(x,y)...\"\ndef f(x,y): x+y\n\n\n[@example_code.py,22,54) \"def g(x,y)...\"\ndef g(x,y): return f(x,y)**2\n\n\n[@example_code.py,69,110) \"def __init...\"\ndef __init__(self, x): self.x = x\n\n\n[@example_code.py,115,163) \"def method...\"\ndef method(self, y): return f(self.x, y)\n\n\n\n\n\n\n\n\n\n\nBringing in data\nTo keep the outputs readable, we will only load a single python file\n\nexample_files = pd.DataFrame(\n    [(Span(code_file),)]\n)\nexample_files.map(repr)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n[@example_code.py,0,178) \"def f(x,y)...\"\n\n\n\n\n\n\n\n\nprint(code_file.read_text())\n\ndef f(x,y):\n    x+y \n\ndef g(x,y):\n    return f(x,y)**2\n\nclass A:\n    def __init__(self, x):\n        self.x = x\n    def method(self, y):\n        return f(self.x, y)\n\nprint(f(2,3))\n\n\nWe will simulate a cursor position inside the function f\n\ncursors =pd.DataFrame([(Span(code_file,16,17),)])\ncursors.map(repr)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n[@example_code.py,16,17) \"x\"\n\n\n\n\n\n\n\nWe import this data to our session.\n\nsess.import_rel('Files',example_files)\nsess.import_rel('Cursors',cursors)\n\nAnd now we can incrementally build our rules from the bottom up\n\n# get all spans of function definitions and their name\n# note we use expr_eval to get the name attribute of the ast Node we assigned to the free variable 'node'.\nFuncDefSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(text, \"//FunctionDef\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    expr_eval(\"{0}.name\",node)-&gt;(name).\n\n?FuncDefSpan(span,name)\n\n'?FuncDefSpan(span,name)'\n\n\n\n\n\n\n\nspan\nname\n\n\n\n\n[@example_code.py,0,19) \"def f(x,y)...\"\nf\n\n\n[@example_code.py,22,54) \"def g(x,y)...\"\ng\n\n\n[@example_code.py,69,110) \"def __init...\"\n__init__\n\n\n[@example_code.py,115,163) \"def method...\"\nmethod\n\n\n\n\n\n\n\n\n\n# get all spans of function calls and their names\nFuncCallSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(text, \"//Call/func/Name\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    as_str(span)-&gt;(name).\n\n?FuncCallSpan(span,name)\n\n'?FuncCallSpan(span,name)'\n\n\n\n\n\n\n\nspan\nname\n\n\n\n\n[@example_code.py,45,46) \"f\"\nf\n\n\n[@example_code.py,151,152) \"f\"\nf\n\n\n[@example_code.py,165,170) \"print\"\nprint\n\n\n[@example_code.py,171,172) \"f\"\nf\n\n\n\n\n\n\n\n\n\n# we compute the func wrapping a cursor by checking with function def span contains our cursor\nCursorWrappingFunc(cursor,name)&lt;-\n    Cursors(cursor),\n    FuncDefSpan(span,name),\n    span_contained(cursor,span)-&gt;(True).\n\n?CursorWrappingFunc(cursor,name)\n\n'?CursorWrappingFunc(cursor,name)'\n\n\n\n\n\n\n\ncursor\nname\n\n\n\n\n[@example_code.py,16,17) \"x\"\nf\n\n\n\n\n\n\n\n\n\n# we get all mentions of a function by looking for function calls of it that are sub spans of func definitions\n# we aggregate our mentions using lex_concat to get a single mention context per function.\nMentions(lex_concat(caller_span),called_name)&lt;-\n    FuncCallSpan(called_span,called_name),\n    FuncDefSpan(caller_span,caller_name),\n    span_contained(called_span,caller_span)-&gt;(True).\n\n?Mentions(mentions,func)\n\n'?Mentions(mentions,func)'\n\n\n\n\n\n\n\nmentions\nfunc\n\n\n\n\ndef g(x,y): return f(x,y)**2 def method(self, y): return f(self.x, y)\nf\n\n\n\n\n\n\n\n\nNow to piece this together to an LLM prompt we can call, lets define our prompt template:\n\nfunc_document_template = \"\"\"\nsystem: based on the following context:\n{}\nExplain the following function:\n{}\nIn the format of a doc string.\n\"\"\"\nsess.import_var('func_document_template',func_document_template)\n\nAnd just like in our basic agent tutorial, we get our strings from our lowlevel rules and compose them using the format ie function.\n\nmodel = 'gpt-3.5-turbo'\nDocumentFunctionPrompt(cursor,prompt)&lt;-\n    CursorWrappingFunc(cursor,name),\n    Mentions(mentions,name),\n    FuncDefSpan(def_span,name),\n    as_str(def_span)-&gt;(def_string),\n    format($func_document_template,mentions,def_string)-&gt;(prompt).\n\n?DocumentFunctionPrompt(cursor,prompt)\n\nDocumentFunction(cursor,answer)&lt;-\n    DocumentFunctionPrompt(cursor,prompt),\n    llm($model,prompt)-&gt;(answer).\n\n?DocumentFunction(cursor,answer)\n\n'?DocumentFunctionPrompt(cursor,prompt)'\n\n\n\n\n\n\n\ncursor\nprompt\n\n\n\n\n[@example_code.py,16,17) \"x\"\nsystem: based on the following context: def g(x,y): return f(x,y)**2 def method(self, y): return f(self.x, y) Explain the following function: def f(x,y): x+y In the format of a doc string.\n\n\n\n\n\n\n\n\n'?DocumentFunction(cursor,answer)'\n\n\n\n\n\n\n\ncursor\nanswer\n\n\n\n\n[@example_code.py,16,17) \"x\"\n\"\"\" This function calculates the sum of two inputs x and y. \"\"\"\n\n\n\n\n\n\n\n\nPutting all of our spannerlog together, we get:\n\nFuncDefSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(text, \"//FunctionDef\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    expr_eval(\"{0}.name\",node)-&gt;(name).\n\nFuncCallSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(text, \"//Call/func/Name\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    as_str(span)-&gt;(name).\n\nCursorWrappingFunc(cursor,name)&lt;-\n    Cursors(cursor),\n    FuncDefSpan(span,name),\n    span_contained(cursor,span)-&gt;(True).\n\nMentions(lex_concat(caller_span),called_name)&lt;-\n    FuncCallSpan(called_span,called_name),\n    FuncDefSpan(caller_span,caller_name),\n    span_contained(called_span,caller_span)-&gt;(True).\n\nmodel = 'gpt-3.5-turbo'\nDocumentFunctionPrompt(cursor,prompt)&lt;-\n    CursorWrappingFunc(cursor,name),\n    Mentions(mentions,name),\n    FuncDefSpan(def_span,name),\n    as_str(def_span)-&gt;(def_string),\n    format($func_document_template,mentions,def_string)-&gt;(prompt).\n\nDocumentFunction(cursor,answer)&lt;-\n    DocumentFunctionPrompt(cursor,prompt),\n    llm($model,prompt)-&gt;(answer).\n\nNote how short and elegant, a complex pipeline can be when we decompose our code into: * powerful and generic callbacks * declerative compositional logic\nAnd the ease with which we can combine formal IE extractions and LLMs to get the best of both: * structured analysis * NLP via LLMs",
    "crumbs": [
      "Tutorials",
      "Copilot Agent"
    ]
  },
  {
    "objectID": "tutorials/rewriting_a_real_codebase.html",
    "href": "tutorials/rewriting_a_real_codebase.html",
    "title": "Rewriting a real code base",
    "section": "",
    "text": "In this tutorial, we will guide you through the process of refactoring an existing data pipeline into the spannerlib framework, showing the utility of this approach from a software engineering perspective. We will:\n\nGive an overview of the original implementation\nDiscuss the use case we chose this use case\nShow how to analyze which parts of a python codebase should turn into one of the following modalities:\n\nRelational Data\nDeclarative Code\nIE functions (and Aggregation functions)\nRegular python code along side spannerlib\n\nDemonstrate a rewriting of the use-case into spannerlib\nCompare between the original and the spannerlib implementations along the following metrics\n\nlines of code per modality\nDecomposition\nSeparation of concerns\nBug surface area\nReadability\nand Debug-ability\nbarriers of entry\n\n\nWe’ve chosen to adapt a medical text classification NLP pipeline, specifically dealing with classifying COVID-19 status from medical transcripts,from the paper “A Natural Language Processing System for National COVID-19 Surveillance in the US Department of Veterans Affairs” which was published in the Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. Code for the original implementation is available here.",
    "crumbs": [
      "Tutorials",
      "Rewriting a real codebase"
    ]
  },
  {
    "objectID": "tutorials/rewriting_a_real_codebase.html#tutorial-introduction",
    "href": "tutorials/rewriting_a_real_codebase.html#tutorial-introduction",
    "title": "Rewriting a real code base",
    "section": "",
    "text": "In this tutorial, we will guide you through the process of refactoring an existing data pipeline into the spannerlib framework, showing the utility of this approach from a software engineering perspective. We will:\n\nGive an overview of the original implementation\nDiscuss the use case we chose this use case\nShow how to analyze which parts of a python codebase should turn into one of the following modalities:\n\nRelational Data\nDeclarative Code\nIE functions (and Aggregation functions)\nRegular python code along side spannerlib\n\nDemonstrate a rewriting of the use-case into spannerlib\nCompare between the original and the spannerlib implementations along the following metrics\n\nlines of code per modality\nDecomposition\nSeparation of concerns\nBug surface area\nReadability\nand Debug-ability\nbarriers of entry\n\n\nWe’ve chosen to adapt a medical text classification NLP pipeline, specifically dealing with classifying COVID-19 status from medical transcripts,from the paper “A Natural Language Processing System for National COVID-19 Surveillance in the US Department of Veterans Affairs” which was published in the Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. Code for the original implementation is available here.",
    "crumbs": [
      "Tutorials",
      "Rewriting a real codebase"
    ]
  },
  {
    "objectID": "tutorials/rewriting_a_real_codebase.html#tldr",
    "href": "tutorials/rewriting_a_real_codebase.html#tldr",
    "title": "Rewriting a real code base",
    "section": "TLDR",
    "text": "TLDR\nHere is a summary of the lines of code of the original and the spannerlib implementations:\n\n\n\n\n\n\n\n\n\n\nCode Type\nComplexity\nOriginal\nSpannerlib\nline code saved\n\n\n\n\nData as Code\nMedium-Low\n3903\n0\n~90%\n\n\nData\nLow\n\n378\n\n\n\nVanilla python\nHigh\n639\n118\n~53%\n\n\nCallbacks\nMedium\n\n76\n\n\n\nSpannerlog\nMedium\n\n79\n\n\n\nTotal\n\n4542\n651\n~85%\n\n\n\nBut why does this work?\n\nThe reason our approach works is that not all code, and not all python code is born the same. In a python code base, nearby code chunks can do very different things, ie: * configuration * data management * pipeline composition * etc\nThe reason that spannerlib can help improve code bases is that it forces the user to explicitly decompose the different code types, and for many of them, it provides a programming modality that is more suited for the code base.\nThe most noteable examples are: * that we can express our compositional logic via Spannerlog * We can seperate core computations from state management * we can delegate State and Data Management to a relational DB\nThis paradigm forces as to enforce best practices on our code, and turbo charges different modalities with programming languages/paradigms that are well suited to them.",
    "crumbs": [
      "Tutorials",
      "Rewriting a real codebase"
    ]
  },
  {
    "objectID": "tutorials/rewriting_a_real_codebase.html#walkthrough",
    "href": "tutorials/rewriting_a_real_codebase.html#walkthrough",
    "title": "Rewriting a real code base",
    "section": "Walkthrough",
    "text": "Walkthrough\n\nProblem definition\nGiven:\n\nA collection of medical documents\n\nClassify each document to one of three classes:\n\nPOS/NEG/UNK\nAccording to whether the document describes the patient as having/not having/neither covid-19.\n\n\n\nStructure of the original implemenation\nThe pipeline was implemented using the [spacy] and [medspacy] frameworks, which are libraries build for rule base NLP pipelines. The original pipeline is split into 6 main components:\n\nconcept tagger:\n\nAssigns a semantic tags to each Token based on textual patterns that involve regex like patterns, over regular text, POS tags and lemmas.\n\ntarget matcher:\n\nAssigns higher level tags to entities and covid 19 mentions based on patterns over the semantic tags from the previous versions\n\nsectionizer:\n\nSegments the text into different sections found commonly in medical report, and differentiate between relevant and non relevant sections\n\ncontext:\n\nmodifies semantic attributes of entities and covid 19 such as positive status, negation, reference to entities which are not the patient etc\nthese modification depends on\n\npatterns over text and semantic tags found in the same sentence as the entitiy\nthe sections that the entity is in\n\n\npostprocessor:\n\nAnother phase of entity tag modification which was added later on\nModifications depend on either\n\npatterns found in the next sentence after the entity\npatterns found on entities which already have specific tags\n\n\ndocument classifier:\n\nassigning each covid mention a classification based on it’s associated tags\nassigning each document a label of “POS”, “UNK”, or “NEG” based on the tags of the covid-19 mentions within it.\n\n\nWe will explain about each stage in more details, including code snippets later on.\n\n\nWhy we chose this use-case\nWe are interested in giving a fair comparison between a real world NLP pipeline and our approach. This paper represents a real world NLP usecase in the pre-LLM era, which demonstrates how to combine NLP models and business logic via rules. The Code is ~4000 lines of code which is big enough to not be considered a toy example, but is small enough to allow us to present it’s decomposition in a timely fashion.\nMoreover, most of the orchestration logic in this pipeline is done using spacy’s compositional SDK. This means that we do not compare ourselves to control flow written in vanilla python, which might be an easy goal to beat, but rather a compositional SDK that is well tuned to this task, making our proof of burden harder. While the usage of a compositional SDK, as opposed to vanilla python in the original implementation, makes the original implementation shorter than it would otherwise have been, most pipeline libraries have a relatively high barrier of entry, and require the user to learn multiple interfaces and classes that are unique to that library.\nThis tradeoff elucidates the power and elegance of spannerlib’s use of the relational model explicitly for data modelling. If indeed our implementation will be shorter and simpler, while avoiding the need to learn abstractions found in current compositional libraries, this will mean that our approach sits closer to the pareto frontier on the tradeoff between conciseness and entry barriers.\nFinally, not all operations in this pipeline can be modelled as IE function and declarative code. Rather than choosing an example that neatly fits our paradigm, we showcase how regular python code can interplay with spannerlib code when only part of a codebase is suitable to refactoring.",
    "crumbs": [
      "Tutorials",
      "Rewriting a real codebase"
    ]
  },
  {
    "objectID": "tutorials/rewriting_a_real_codebase.html#deepdive-on-the-original-implementation",
    "href": "tutorials/rewriting_a_real_codebase.html#deepdive-on-the-original-implementation",
    "title": "Rewriting a real code base",
    "section": "Deepdive on the original implementation:",
    "text": "Deepdive on the original implementation:\n\nConcept Tagger\nThe concept tagger module defines a collection of rules that can be used to tag certain tokens in the text. A basic rule looks like\n# a basic pattern rule\nTargetRule( literal=\"coronavirus\", category=\"COVID-19\", pattern=[{\"LOWER\": {\"REGEX\": \"coronavirus|hcov|ncov$\"}}], )\nWhere TargetRule is a medspacy class which maps a given pattern to a tag (ie COVID-19)\nThe Target pattern belongs to a spacy specific pattern language, which may include lemma tags, POS tags, group membership and more, and has a non neglible learning cost.\nHere are some examples of rules that use Lemmas and POS in them:\n# using lemma and group membership\nTargetRule(\n        \"results positive\",\n        \"positive\",\n        pattern=[\n            {\"LOWER\": \"results\"},\n            {\"LEMMA\": \"be\", \"OP\": \"?\"},\n            {\"LOWER\": {\"IN\": [\"pos\", \"positive\"]}},\n        ],\n    )\n\n# using POS and group membership\nTargetRule(\n        \"other experiencer\",\n        category=\"other_experiencer\",\n        pattern=[\n            {\n                \"POS\": {\"IN\": [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]},\n                \"LOWER\": {\n                    \"IN\": [\n                        \"someone\",\n                        \"somebody\",\n                        \"person\",\n                        \"anyone\",\n                        \"anybody\",\n                    ]\n                },\n            }\n        ],\n    ),\nThi module contains ~327 lines of code which comprise of only dict of the form {tag:[TargetRule]}.\n\n\nTarget Matcher\nThe Targer Matcher module defines a collection of target rules that also use the tags defined in the previous module. These 2 modules are seperated since they must run sequentially, unlike rules in each module, which can run in parallel. They use the same TargetRule. For example:\n# uses the concept tags 'positive' and 'COVID-19' from the concept tagger\nTargetRule(\n            \"&lt;POSITIVE&gt; &lt;COVID-19&gt; unit\",\n            \"COVID-19\",\n            pattern=[\n                {\"_\": {\"concept_tag\": \"positive\"}, \"OP\": \"+\"},\n                {\"_\": {\"concept_tag\": \"COVID-19\"}, \"OP\": \"+\"},\n                {\"LOWER\": {\"IN\": [\"unit\", \"floor\"]}},\n            ],\n        )\nThis module contains ~726 lines of code with the same form as the concept matcher {tag:[TargetRule]}.\n\n\nSectionizer\nThe Sectionizer module contains a list of SectionRule classes which outline which text literals should be taken as starting tokens for a new section. They are used by spacy’s pipeline SDK to seperate the documents into different sections and subsequently work on the sections separately. For example:\n# section rules that describe possible starting tokens for the imaging sections\n    SectionRule(category=\"imaging\", literal=\"IMAGING:\"),\n    SectionRule(category=\"imaging\", literal=\"INTERPRETATION:\"),\n    SectionRule(category=\"imaging\", literal=\"Imaging:\"),\n    SectionRule(category=\"imaging\", literal=\"MRI:\"),\n    SectionRule(category=\"imaging\", literal=\"Radiology:\"),\nThis module contains ~116 lines of code which are just a list of SectionRules.\n\n\nContext\nThe context module is in charge of adding tags to entities, depending on what other patterns are found in the same sentence as the covid match. This id done by defining a list of ConTextRule classes which outline which pattern to look for, around which existing tags. For example, bellow is a rule that looks for covid mentions that are preceded in the same sentence by “not detected”. And in such cases adds the tag NEGATED_EXISTENCE to the mention.\nConTextRule(\n    literal=\"Not Detected\",\n    category=\"NEGATED_EXISTENCE\",\n    # direction defines whether to try to match the pattern before or after the entity in question\n    direction=\"BACKWARD\",\n    pattern=[\n        {\"LOWER\": {\"IN\": [\"not\", \"non\"]}},\n        {\"IS_SPACE\": True, \"OP\": \"*\"},\n        {\"TEXT\": \"-\", \"OP\": \"?\"},\n        {\"LOWER\": {\"REGEX\": \"detecte?d\"}},\n    ],\n    # allowed types defines which entities to apply the context search for, based on which tags they have.\n    # in this case we only try to match these patterns around COVID-19 mentions.\n    allowed_types={\"COVID-19\"},\n),\nConTextRules also allow to define callbacks that will run on matches and remove the match if necessary. In this example, a callback function defined by the authors of the original imlementation is run on the matches, and removes them if they dont fit more nuanced criteria.\nConTextRule(\n    \"active for\",\n    \"DEFINITE_POSITIVE_EXISTENCE\",\n    direction=\"FORWARD\",\n    pattern=[{\"LOWER\": \"active\"}, {\"LOWER\": \"for\", \"OP\": \"?\"}],\n    allowed_types={\"COVID-19\"},\n    max_scope=2,\n    on_match=callbacks.disambiguate_active,\n)\nThis module contains ~2370 lines of code comprise of a list of ConTextRules and ~208 lines of callback functions that are used in some of the rules.\n\n\nPostProccessor\nThis module does changes the tags of some entities depending on custom logic, that cannot be addressed by the spacy ecosystem. spacy does allow to add this custom logic using a nested strucutre of PostprocessingRule and Postprocessing pattern. For example, the following remove a coronavirus entity if ‘denies’ and ‘contact’ are in the same sentence as the entity.\nPostprocessingRule(\n        patterns=[\n            PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n            PostprocessingPattern(\n                postprocessing_functions.sentence_contains,\n                condition_args=({\"deny\", \"denies\", \"denied\"},),\n            ),\n            PostprocessingPattern(\n                postprocessing_functions.sentence_contains,\n                condition_args=({\"contact\", \"contacts\", \"confirmed\"},),\n            ),\n        \\],\n        action=postprocessing_functions.remove_ent,\n    )\nBased on our analysis of the code, the postprocessing rules, together with their custom logic, come in 3 flavors: * rules that are based on sentence context alone * these are similar to the context rules, like the example shown above. * rules based on the context of the sentence after the entity * rules based on the context of the sentece and of other tags given to the entity in previous matches.\nHere are examples of the latter two flavors:\n# if the covid mention was previously tagged with DEFINITE_POSITIVE_EXISTENCE but the sentence contains words like \"should\"\n# then we tag the mention as uncertain.\nPostprocessingRule(\n    patterns=[\n        PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n        PostprocessingPattern(\n            postprocessing_functions.is_modified_by_category,\n            condition_args=(\"DEFINITE_POSITIVE_EXISTENCE\",),\n        ),\n        PostprocessingPattern(\n            postprocessing_functions.sentence_contains,\n            condition_args=(\n                {\n                    \"should\",\n                    \"unless\",\n                    \"either\",\n                    \"if comes back\",\n                    \"if returns\",\n                    \"if s?he tests positive\",\n                },\n                True,\n            ),\n        ),\n    ],\n    action=set_is_uncertain,\n    action_args=(True,),\n)\n# If a test does not have any results within the same sentence, check the next sentence.\nPostprocessingRule(\n    patterns=[\n        PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n        PostprocessingPattern(\n            postprocessing_functions.is_modified_by_category,\n            condition_args=(\"test\",),\n        ),\n        PostprocessingPattern(has_positive, success_value=False),\n        (\n            PostprocessingPattern(\n                next_sentence_contains,\n                condition_args=(\"results? (are|is) positive\",),\n            ),\n            PostprocessingPattern(\n                next_sentence_contains, condition_args=(\"results pos[^s]\",)\n            ),\n        ),\n    ],\n    action=set_is_positive,\n    action_args=(True,),\n)\nThis module has ~364 lines of PostProcessingRule definitions and ~84 lines of custom logic python functions.\n\n\nDocument classifier\nThis module classifies the document based on the tags given to covid mentions. It is comprised out of ~77 lines of vanilla python code.\n\n\nAdditional code\nThere are ~270 more lines of code that are comprised of the main pipline logic, which loads all the modules and run the pipeline, and some utility functions.\n\n\nLines of Code overview\nWe purposefully vanilla python code from the different Rule classes which act as “Data as Code”. We will come back to this point in the analysis bellow.\n\n\n\nSection\nCodeType\n~#lines of code\n\n\n\n\nConcept Tagger\nTargetRule\n327\n\n\nTarget Matcher\nTargetRule\n726\n\n\nSectionizer\nSectionRule\n116\n\n\nContext\nConTextRule\n2370\n\n\n\nVanilla Python\n208\n\n\nPost Processing\nPostProcessingRule\n364\n\n\n\nVanilla Python\n84\n\n\nDocument Classifier\nVanilla Python\n77\n\n\nOther\nVanilla Python\n270\n\n\nTotal\n\n4542",
    "crumbs": [
      "Tutorials",
      "Rewriting a real codebase"
    ]
  },
  {
    "objectID": "tutorials/rewriting_a_real_codebase.html#rules-for-our-implemenation-and-notes-on-line-of-code-comparisons",
    "href": "tutorials/rewriting_a_real_codebase.html#rules-for-our-implemenation-and-notes-on-line-of-code-comparisons",
    "title": "Rewriting a real code base",
    "section": "Rules for our implemenation and notes on line of code comparisons",
    "text": "Rules for our implemenation and notes on line of code comparisons\nTo make our comparison fair, we have some key guidelines for our implementations, and a note on the line of code measures. First, we must note that the lines of code measured above include whitespace or lines with parenthesis that is used to make the code more readable.\n\nWe do not format the original code using formatting tools that try to squeeze more logic into more lines since that hurt readability and essentially hacks the measure of lines of code.\nIn our implementation we do not skim on whitespace or comments when it helps readability and count our lines of code including it as well.\n\nSome additional guidelines:\n\nWe do not use any other libraries other than the libraries used by the original project (such as spacy) and python standard libraries.\n\nThis is done to ensure that we do not “beat” the original implementation due to more sophisticated tool use, beyond the spannerlib framework of course.\n\n\nPlease also note that while the line of code comparison is the best quantitative analysis we could perform, the true strength of the spannerlib approach doesn’t come simply from line of code reduction, but from other software engineering concerns which we will go into bellow.\nIn the step by step implementation, we include some debugging statements and tests to help explain the code. However most of that is not part of the pipeline, in the end to end implementation section we will leave only the actual implementation of the pipeline.",
    "crumbs": [
      "Tutorials",
      "Rewriting a real codebase"
    ]
  },
  {
    "objectID": "tutorials/rewriting_a_real_codebase.html#our-implementation-step-by-step-implementation",
    "href": "tutorials/rewriting_a_real_codebase.html#our-implementation-step-by-step-implementation",
    "title": "Rewriting a real code base",
    "section": "Our implementation, step by step implementation",
    "text": "Our implementation, step by step implementation\n\n\nExported source\n# importing dependencies\nimport re\nimport csv\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pathlib import Path\nfrom spannerlib import get_magic_session,Session,Span\nsess = get_magic_session()\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n\n\n\nExported source\n# configurations\nslog_file = Path('covid_data/covid_logic.pl')\ninput_dir = Path('covid_data/sample_inputs')\ndata_dir = Path('covid_data/rules_data')\n\n\n\nDeciding Scope of spannerlib code\nWhen trying to design a conceptual pipeline, or refactor an existing codebase using spannerlib, best practices dictate that we should first start off with several questions to help us understand what parts of our code should turn into spannerlib code:\n\nWhat basic computational building blocks do we need?\n\n\nThis does not include things like:\n\ncompositional control flow\ndataclasses and other OOP hierarchies that are used for data modelling\nConstructs that help manage and inspect program state\netc …\n\nThis does include\n\nlow level numerical/textual analysis\ndata ingestion from external sources\n\n\n\nWhat in our code is not strictly data processing? for example:\n\n\nstatistics and visualizations\nlogging or publishing of results\nGetting user input in an interactive system.\n\n\nWhat data processing pipeline cannot, or is not easy to express via our declerative language?\n\n\nsuch as operations that need to extract entire relations at once from other relations, without being able to be mapped to tuple level extractions.\nOr operations that do not fit set semantics and require ordering.\nOperations that do not fit the relational paradigm well, for example graph analytics.\n\nNote that these points are not a limitation of the spannerlib paradigm but of the very limited declarative language we chose to extend (Datalog).\nThe spannerlib approach can be extended to any declarative language, including non relational ones.\n\n\nLike any programming process, you might not get the final answer on the first attempt but these questions help narrow down the design space.\nIn our case, the basic computational building blocks are:\n\nbasic textual analysis tools like pattern matching and splitting of text\n\n\nAvailable through the spannerlib’s std library\nSome primitive NLP tasks such as:\n\nsentence boundary detection\nPOS tagging\nLemmatization\n\n\n\nThis is strictly a text processing pipeline, so there are no statistics etc involved\nThere is no obvious operation that do not fit, but as we will see once we do our data modelling, there are operations that do not fit Spannerlog.\n\n\n\nDefining our ie functions\nBased on this analysis, we can already start building our IE functions:\n\nWe will use regex based ie functions form the standard library\n\nrgx for pattern matching\nrgx_split for splitting text based on delimeter patterns\n\nWe will implement using spacy\n\nPOS extraction\nLEMMA extraction\nSentence boundary detection\n\n\n\n\nExported source\ndef split_sentence(text):\n    \"\"\"\n    Splits a text into individual sentences. using spacy's sentence detection.\n    \n    Returns:\n        str: Individual sentences extracted from the input text.\n    \"\"\"\n\n    doc = nlp(str(text))\n    start = 0\n    for sentence in doc.sents:\n        end = start+len(sentence.text)\n        # note that we yield a Span object, so we can keep track of the locations of the sentences\n        yield Span(text,start,end)\n        start = end + 1\n\n\n\ntext = (input_dir/'sample1.txt').read_text()\n\n\nassert list(split_sentence(text)) == ['Patient presents to be tested for COVID-19.',\n 'His wife recently tested positive for novel coronavirus.',\n 'SARS-COV-2 results came back positive.']\nlist(split_sentence(text))\n\n[[@a6c01c,0,43) \"Patient pr...\",\n [@a6c01c,44,100) \"His wife r...\",\n [@a6c01c,101,139) \"SARS-COV-2...\"]\n\n\nNote that for both Lemmas and POS, the original pipeline is only interested in a very small subsets of lemmas and POS. We could take two approaches here: 1. Generate all POS and Lemmas and filter them declaratively 2. Configure our extractors to only extract the information we know we may want.\nSince our rgx functions output all relevant matches as Spans, we will demonstrate the second approach here.\n\n\nExported source\nclass LemmaFromList():\n    def __init__(self,lemma_list):\n        self.lemma_list = lemma_list\n\n    def __call__(self,text):\n        doc = nlp(str(text))\n        for word in doc:\n            start = word.idx\n            end = start + len(word.text)\n            if word.lemma_ in self.lemma_list:\n                yield (Span(text,start,end),word.lemma_)\n            elif word.like_num:\n                yield (Span(text,start,end),'like_num')\n            else:\n                pass\n\nlemma_list = (data_dir/'lemma_words.txt').read_text().split()\nlemmatizer = LemmaFromList(lemma_list)\n\n\n\nassert list(lemmatizer('the boy was sick')) == [(\"was\",\"be\")]\n\n\n\nExported source\nclass PosFromList():\n    def __init__(self,pos_list):\n        self.pos_list = pos_list\n    def __call__(self,text):\n        doc = nlp(str(text))\n        for word in doc:\n            start = word.idx\n            end = start + len(word.text)\n            if word.pos_ in self.pos_list:\n                yield (Span(text,start,end),word.pos_)\n\npos_annotator = PosFromList([\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"])\n\n\n\nassert list(pos_annotator('sick boy')) == [('sick','ADJ'),('boy','NOUN')]\nlist(pos_annotator('sick boy'))\n\n[([@01e12d,0,4) \"sick\", 'ADJ'), ([@01e12d,5,8) \"boy\", 'NOUN')]\n\n\n\n\nExported source\nsess.register('split_sentence',split_sentence,[(str,Span)],[Span])\nsess.register('pos',pos_annotator,[(Span,str)],[Span,str])\nsess.register('lemma',lemmatizer,[(Span,str)],[Span,str])\n\n\n\n\nData Modelling\nNow we have an idea of the building blocks we would have at our disposal. Once we have that, the next thing we need to think about it how we will model our data. Specifically, we can ask four guiding questions:\n\nHow do we get our input data, is it relational or close to it?\nWhat does our output data look like, is it relational or close to it?\nWhat parts of our code can be turned into data?\nAre there any computations that require, or are currently based on data models that are not relational?\n\nIn our case, the answer to the first 3 question are pretty simple:\n\nWe get a directory of text files, we can model that as a (path,text) relation.\nWe want a classification of files, we can model that as a (path,label) relation.\nAll the different Rule Data Classes stand a good chance of being convertible to relational data.\n\nie SectionRules are labelled text delimeters\nContextRules seem like they can be turned into regexes or at most serialized and stored as data.\n\n\nHowever, question number 4 is more tricky. To understand why, please note that while the pattern attributes of rules look very similar to regular expressions which work on a character level, spacy’s data model works on Word level, and the patterns look not only at the raw text but at token tags, either Lemma, POS or tags defined by the user.\nThis poses a little bit of a challenge, and likely stems from the fact that this solution was built using spacy in an ad-hoc manner. Spannerlib, can work on relations over all pythonic types (though relational modelling using primitives and Spans is recommended). This means that we have 2 approaches we can choose to take:\n\nWe can build based on spacy, a word level, tag aware regex pattern matcher\n\nthis could be cool but would basically be rebuilding parts of spacy, which would overfit to the current implementatio and is a classical example of the XY fallacy in code redesign.\n\nSee if we can remodel this problem using classical textual information extraction ideas.\n\nUnsurprisingly, we will choose the second approach, even though it will cause us to have some computations that are not supported by Spannerlog.\n\n\nA detour into text rewrtting\nTo understand our approach, we must first note that the sequential nature of this pipeline, like many other NLP pipelines, means that we extract tags of entities using patterns, and then reduce our natural language problem to a problem over the tags and not the original text. For example, after normalizing sars COV 2 and novel corona virus to COVID-19, we do not care what the original form of the tag was, we can simply continue analyzing the text as if COVID-19 was there all along.\nThis aspect of our pipeline leads us to a well known pattern of “text rewriting”. In this pattern, we have several phases of span/tag extractions, which are then used to rewrite the original text to a simplified form, followed by possible other rewriting iterations, before the final text form in generated for mining.\nHowever, we must note that text rewriting does not fit the limitation of Spannerlog for several reasons:\n\ntext rewriting at its most basic takes an original text, and a table of (from,to) pairs and generating a new text.\n\nThis means that rewriting requires the context of an entire table to perform, which does not fit the tuple-&gt;relation paradigm of datalog\n\nrewriting the text as a string requires sorting of the from spans\n\nDoes this, mean that our attempted refactoring a failure? Of course not :)\nAs per the spannerlib framework, the bi directional interaction between Spannerlog and regular python code does not need to be done in a single iteration. What we need is to simply stratify our pipeline and our documents into different versions, and interleave rewritting of new versions with information extraction of tags from the previous version. To do this we need to implement some rewriting logic.\n\n\nExported source\ndef rewrite(text,span_label_pairs):\n    \"\"\"rewrites a string given a dataframe with spans and the string to rewrite them to\n    assumes that the spans belong to the text\n\n    Args:\n        text (str like): string to rewrite\n        span_label_pairs (pd.Dataframe) dataframe with two columns, first is spans in the doc to rewrite\n            second is what to rewrite to\n    Returns:\n        The rewritten string\n    \"\"\"    \n    if isinstance(text,Span):\n        text = text.as_str()\n    span_label_pairs = sorted(list(span_label_pairs.itertuples(index=False,name=None)), key=lambda x: x[0].start)\n\n    rewritten_text = ''\n    current_pos = 0\n    for span,label in span_label_pairs:\n        rewritten_text += text[current_pos:span.start] + label \n        current_pos = span.end\n\n    rewritten_text += text[current_pos:]\n\n    return rewritten_text\n\n\n\ntext = 'the boy was sick'\nreplace_span_with = pd.DataFrame(lemmatizer(text))\ndisplay(replace_span_with.map(repr))\nres = rewrite(text,replace_span_with) \nassert res == 'the boy be sick'\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n[@6a488f,8,11) \"was\"\n'be'\n\n\n\n\n\n\n\n\ndoc = Span('hello darkness my old friend, I come ...',name='doc')\nspans_to_replace = pd.DataFrame([\n    [doc.slice(18,21),'young'],\n    [doc.slice(22,28),'nemesis'],\n])\nspans_to_replace.map(repr)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n[@doc,18,21) \"old\"\n'young'\n\n\n1\n[@doc,22,28) \"friend\"\n'nemesis'\n\n\n\n\n\n\n\n\nrewritten_doc=rewrite(doc,spans_to_replace)\nassert rewritten_doc == 'hello darkness my young nemesis, I come ...'\nrewritten_doc\n\n'hello darkness my young nemesis, I come ...'\n\n\n\n\nResuming data modelling\nSo after resolving the matter of patten matching on tags, we can model our documents and rules as follows:\n\nDocuments will be modelled by Docs(Path,Text,Version)\nConcept Tagger Rules will be modelled by ConceptTagRules(pattern,tag,textVersion)\n\nwhere the version is either lemma or POS for lemma and pos based pattern respectively (there are no patterns that require both)\n\nTarget Matcher Rules will be modelled by TargetTagRules(pattern,tag)\nSection delimiters and section relevancy will be modelled by SectionTags(Pattern,Tag) and PositiveSectionTags(Tag).\nContext Rules will be modelled by SentenceContextRules(pattern,tag,DisambiguationPattern)\n\nWhere we would like to tag a pattern of the sentence doesnt have any matches of DisambiguationPattern\n\n\nAs for PostprocessRules, the 3 different flavors can be modelled differently\n\nrules based on sentence context alone are modelled by PostprocessPatternRules(pattern,tag)\nrules based on sentence context and existing tags are modelled by PostprocessRulesWithAttributes(pattern,old_tag,new_tag)\n\nwhere instead of removing a mention or deleting a tag, which is not something you can or want to do declaratively, we will simply add a new tag whose semantic is to disregard the mention or the old tag.\n\nrules based on the next sentence are modelled by NextSentencePostprocessPatternRules(pattern,tag)\n\nAll patterns mentioned above are regex patterns using python’s regex flavour.\n\nA note on data modelling and schema design\nThe same operations we know and love from relational schema design, such as schema normalization and schema merging etc can be applied to data modelling in the spannerlib framework. For example, we could have, to limit the number of Postprocessing Rule relations, merged their schema by adding an ANY tag that matches any pattern and changing building a single relation of the form ProsProcessRule(pattern,old_tag,new_tag,is_next_sentence). This would have lead to less relations and less Spannerlog rules, but the remaining rules would have been slightly more complex. Such tradeoffs are analogous to tradeoffs, in regular code design, between amount, length and complexity of functions.\nLets see this in action:\n\n\nLooking at the rule files\n\nfor query in [\n    \"?ConceptTagRules(Pattern,Tag,TextType)\",\n    \"?TargetTagRules(Pattern,Tag)\",\n    \"?SectionTags(Pattern,Tag)\",\n    \"?PositiveSectionTags(Tag)\",\n    \"?SentenceContextRules(Pattern,Tag,DisambiguationPattern)\",\n    \"?PostprocessPatternRules(Pattern,Tag)\",\n    \"?PostprocessRulesWithAttributes(Pattern,Old_Tag,New_Tag)\",\n    \"?NextSentencePostprocessPatternRules(Pattern,Tag)\",\n        ]:\n    res = sess.export(query)\n    display(query)\n    display(len(res))\n    display(res[:3])\n\n'?ConceptTagRules(Pattern,Tag,TextType)'\n\n\n19\n\n\n\n\n\n\n\n\n\nPattern\nTag\nTextType\n\n\n\n\n0\n(?i)(?:(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|4...\nOTHER_CORONAVIRUS\nlemma\n\n\n1\n(?i)(?:(?:antibody|antibodies|ab) test)\nantibody test\nlemma\n\n\n2\n(?i)(?:(?:coronavirus|hcovs?|ncovs?|covs?)(?:\\...\nOTHER_CORONAVIRUS\nlemma\n\n\n\n\n\n\n\n'?TargetTagRules(Pattern,Tag)'\n\n\n20\n\n\n\n\n\n\n\n\n\nPattern\nTag\n\n\n\n\n0\n(?i)((?:person|patient) with confirm COVID-19)\n1 2 3 4\n\n\n1\n(?i)(?:(?:(?:contact|exposure) (?:with|to)? )?...\nOTHER_PERSON\n\n\n2\n(?i)(?:(?:patient|person) (?:who|that) test (?...\nOTHER_PERSON\n\n\n\n\n\n\n\n'?SectionTags(Pattern,Tag)'\n\n\n110\n\n\n\n\n\n\n\n\n\nPattern\nTag\n\n\n\n\n0\nA/P:\nobservation_and_plan\n\n\n1\nACTIVE MEDICATIONS LIST:\nmedications\n\n\n2\nACTIVE MEDICATIONS:\nmedications\n\n\n\n\n\n\n\n'?PositiveSectionTags(Tag)'\n\n\n4\n\n\n\n\n\n\n\n\n\nTag\n\n\n\n\n0\ndiagnoses\n\n\n1\nobservation_and_plan\n\n\n2\npast_medical_history\n\n\n\n\n\n\n\n'?SentenceContextRules(Pattern,Tag,DisambiguationPattern)'\n\n\n176\n\n\n\n\n\n\n\n\n\nPattern\nTag\nDisambiguationPattern\n\n\n\n\n0\n(&gt;i)contact\npositive\ndroplet|precaution|isolat\n\n\n1\n(?i)(?:(?:(?:(?:-)?hx|history|) of)(?: (?!&lt;IGN...\nnegated\n(?=a)b\n\n\n2\n(?i)(?:(?:(?:area|county|community|city) (?:wi...\nnegated\n(?=a)b\n\n\n\n\n\n\n\n'?PostprocessPatternRules(Pattern,Tag)'\n\n\n6\n\n\n\n\n\n\n\n\n\nPattern\nTag\n\n\n\n\n0\n(?=.*\\b(?:deny|denies|denied)\\b)(?=.*\\b(?:cont...\nIGNORE\n\n\n1\n(?=.*\\b(?:setting of|s/o)\\b)(?!.*\\b(?:COVID-19...\nno_positive\n\n\n2\n(?i)(.*benign.*)\nuncertain\n\n\n\n\n\n\n\n'?PostprocessRulesWithAttributes(Pattern,Old_Tag,New_Tag)'\n\n\n5\n\n\n\n\n\n\n\n\n\nPattern\nOld_Tag\nNew_Tag\n\n\n\n\n0\n.*(?:re[ -]?test|second test|repeat).*\nnegated\nno_negated\n\n\n1\n.*(?:should|unless|either|if comes back|if ret...\npositive\nuncertain\n\n\n2\n.*(?:sign|symptom|s/s).*\npositive\nuncertain\n\n\n\n\n\n\n\n'?NextSentencePostprocessPatternRules(Pattern,Tag)'\n\n\n1\n\n\n\n\n\n\n\n\n\nPattern\nTag\n\n\n\n\n0\n(?i)(?:^(?:positive|detected)|results?(?: be)?...\npositive\n\n\n\n\n\n\n\n\n\n\nPreparing the input\n\n\nRewriting texts based on tags\nIn this section we need to rewrite our texts multiple times base on: * Lemmas * Lemma concept matches * POS tags * and target matcher tags\nThis section replaces the Concept Matcher and Target Tagger modules in the original implementation\n\nLemmas(P,D,Word,Lem)&lt;-Docs(P,D,\"raw_text\"),lemma(D)-&gt;(Word,Lem).\n?Lemmas(P,D,Word,Lem)\n\n'?Lemmas(P,D,Word,Lem)'\n\n\n\n\n\n\n\nP\nD\nWord\nLem\n\n\n\n\nsample1.txt\nPatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n[@a6c01c,0,7) \"Patient\"\npatient\n\n\nsample1.txt\nPatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n[@a6c01c,20,22) \"be\"\nbe\n\n\nsample10.txt\npatient was screened for cov-19. results came back positive.\n[@9f417c,0,7) \"patient\"\npatient\n\n\nsample10.txt\npatient was screened for cov-19. results came back positive.\n[@9f417c,8,11) \"was\"\nbe\n\n\nsample2.txt\nThe patient was tested for Coronavirus 2019. Results are positive. Patient underwent no Coronavirus 2019 education.\n[@591f89,4,11) \"patient\"\npatient\n\n\nsample2.txt\nThe patient was tested for Coronavirus 2019. Results are positive. Patient underwent no Coronavirus 2019 education.\n[@591f89,12,15) \"was\"\nbe\n\n\nsample2.txt\nThe patient was tested for Coronavirus 2019. Results are positive. Patient underwent no Coronavirus 2019 education.\n[@591f89,39,43) \"2019\"\nlike_num\n\n\nsample2.txt\nThe patient was tested for Coronavirus 2019. Results are positive. Patient underwent no Coronavirus 2019 education.\n[@591f89,53,56) \"are\"\nbe\n\n\nsample2.txt\nThe patient was tested for Coronavirus 2019. Results are positive. Patient underwent no Coronavirus 2019 education.\n[@591f89,67,74) \"Patient\"\npatient\n\n\nsample2.txt\nThe patient was tested for Coronavirus 2019. Results are positive. Patient underwent no Coronavirus 2019 education.\n[@591f89,100,104) \"2019\"\nlike_num\n\n\nsample3.txt\nProblem List: 1. Pneumonia 2. Novel Coronavirus 2019\n[@45bf63,14,15) \"1\"\nlike_num\n\n\nsample3.txt\nProblem List: 1. Pneumonia 2. Novel Coronavirus 2019\n[@45bf63,27,28) \"2\"\nlike_num\n\n\nsample3.txt\nProblem List: 1. Pneumonia 2. Novel Coronavirus 2019\n[@45bf63,48,52) \"2019\"\nlike_num\n\n\nsample6.txt\nThe patient have reported novel coronavirus.\n[@2473a3,4,11) \"patient\"\npatient\n\n\nsample6.txt\nThe patient have reported novel coronavirus.\n[@2473a3,12,16) \"have\"\nhave\n\n\nsample8.txt\nPatient was sent for a covid test. Someone was tested positive.\n[@aad8ff,8,11) \"was\"\nbe\n\n\nsample8.txt\nPatient was sent for a covid test. Someone was tested positive.\n[@aad8ff,43,46) \"was\"\nbe\n\n\nsample9.txt\nPatient had contact patient with coronavirus. screening positive coronavirus.\n[@0e1178,8,11) \"had\"\nhave\n\n\nsample9.txt\nPatient had contact patient with coronavirus. screening positive coronavirus.\n[@0e1178,12,19) \"contact\"\ncontact\n\n\nsample9.txt\nPatient had contact patient with coronavirus. screening positive coronavirus.\n[@0e1178,20,27) \"patient\"\npatient\n\n\n\n\n\n\n\n\n\n?Docs(P,D,V)\n\n'?Docs(P,D,V)'\n\n\n\n\n\n\n\nP\nD\nV\n\n\n\n\nsample1.txt\nPatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\nraw_text\n\n\nsample1.txt\npatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\nlemma\n\n\nsample10.txt\npatient be screened for cov-19. results came back positive.\nlemma\n\n\nsample10.txt\npatient was screened for cov-19. results came back positive.\nraw_text\n\n\nsample2.txt\nThe patient be tested for Coronavirus like_num. Results be positive. patient underwent no Coronavirus like_num education.\nlemma\n\n\nsample2.txt\nThe patient was tested for Coronavirus 2019. Results are positive. Patient underwent no Coronavirus 2019 education.\nraw_text\n\n\nsample3.txt\nProblem List: 1. Pneumonia 2. Novel Coronavirus 2019\nraw_text\n\n\nsample3.txt\nProblem List: like_num. Pneumonia like_num. Novel Coronavirus like_num\nlemma\n\n\nsample4.txt\nneg covid education.\nlemma\n\n\nsample4.txt\nneg covid education.\nraw_text\n\n\nsample5.txt\npositive covid precaution.\nlemma\n\n\nsample5.txt\npositive covid precaution.\nraw_text\n\n\nsample6.txt\nThe patient have reported novel coronavirus.\nlemma\n\n\nsample6.txt\nThe patient have reported novel coronavirus.\nraw_text\n\n\nsample7.txt\nElevated cholesterol levels require further assessment and lifestyle adjustments .\nlemma\n\n\nsample7.txt\nElevated cholesterol levels require further assessment and lifestyle adjustments .\nraw_text\n\n\nsample8.txt\nPatient be sent for a covid test. Someone be tested positive.\nlemma\n\n\nsample8.txt\nPatient was sent for a covid test. Someone was tested positive.\nraw_text\n\n\nsample9.txt\nPatient had contact patient with coronavirus. screening positive coronavirus.\nraw_text\n\n\nsample9.txt\nPatient have contact patient with coronavirus. screening positive coronavirus.\nlemma\n\n\n\n\n\n\n\n\n\nLemmaConceptMatches(Path,Doc,Span,Label) &lt;- \n    Docs(Path,Doc,\"lemma\"),\n    ConceptTagRules(Pattern, Label, \"lemma\"),\n    rgx(Pattern,Doc) -&gt; (Span).\n?LemmaConceptMatches(Path,Doc,Span,Label)\n\n'?LemmaConceptMatches(Path,Doc,Span,Label)'\n\n\n\n\n\n\n\nPath\nDoc\nSpan\nLabel\n\n\n\n\nsample1.txt\npatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n[@4d073b,0,7) \"patient\"\npatient\n\n\nsample1.txt\npatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n[@4d073b,34,42) \"COVID-19\"\nCOVID-19\n\n\nsample1.txt\npatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n[@4d073b,69,77) \"positive\"\npositive\n\n\nsample1.txt\npatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n[@4d073b,82,99) \"novel coro...\"\nCOVID-19\n\n\nsample1.txt\npatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n[@4d073b,101,111) \"SARS-COV-2\"\nCOVID-19\n\n\nsample1.txt\npatient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n[@4d073b,130,138) \"positive\"\npositive\n\n\nsample10.txt\npatient be screened for cov-19. results came back positive.\n[@f3a9fd,0,7) \"patient\"\npatient\n\n\nsample10.txt\npatient be screened for cov-19. results came back positive.\n[@f3a9fd,50,58) \"positive\"\npositive\n\n\nsample2.txt\nThe patient be tested for Coronavirus like_num. Results be positive. patient underwent no Coronavirus like_num education.\n[@a5d37d,4,11) \"patient\"\npatient\n\n\nsample2.txt\nThe patient be tested for Coronavirus like_num. Results be positive. patient underwent no Coronavirus like_num education.\n[@a5d37d,26,37) \"Coronaviru...\"\nCOVID-19\n\n\nsample2.txt\nThe patient be tested for Coronavirus like_num. Results be positive. patient underwent no Coronavirus like_num education.\n[@a5d37d,59,67) \"positive\"\npositive\n\n\nsample2.txt\nThe patient be tested for Coronavirus like_num. Results be positive. patient underwent no Coronavirus like_num education.\n[@a5d37d,69,76) \"patient\"\npatient\n\n\nsample2.txt\nThe patient be tested for Coronavirus like_num. Results be positive. patient underwent no Coronavirus like_num education.\n[@a5d37d,90,101) \"Coronaviru...\"\nCOVID-19\n\n\nsample3.txt\nProblem List: like_num. Pneumonia like_num. Novel Coronavirus like_num\n[@389fbb,44,61) \"Novel Coro...\"\nCOVID-19\n\n\nsample4.txt\nneg covid education.\n[@3ac307,4,9) \"covid\"\nCOVID-19\n\n\nsample5.txt\npositive covid precaution.\n[@2e40a3,0,8) \"positive\"\npositive\n\n\nsample5.txt\npositive covid precaution.\n[@2e40a3,9,14) \"covid\"\nCOVID-19\n\n\nsample6.txt\nThe patient have reported novel coronavirus.\n[@2473a3,4,11) \"patient\"\npatient\n\n\nsample6.txt\nThe patient have reported novel coronavirus.\n[@2473a3,26,43) \"novel coro...\"\nCOVID-19\n\n\nsample8.txt\nPatient be sent for a covid test. Someone be tested positive.\n[@2893ce,0,7) \"Patient\"\npatient\n\n\nsample8.txt\nPatient be sent for a covid test. Someone be tested positive.\n[@2893ce,22,27) \"covid\"\nCOVID-19\n\n\nsample8.txt\nPatient be sent for a covid test. Someone be tested positive.\n[@2893ce,52,60) \"positive\"\npositive\n\n\nsample9.txt\nPatient have contact patient with coronavirus. screening positive coronavirus.\n[@539a7c,0,7) \"Patient\"\npatient\n\n\nsample9.txt\nPatient have contact patient with coronavirus. screening positive coronavirus.\n[@539a7c,21,28) \"patient\"\npatient\n\n\nsample9.txt\nPatient have contact patient with coronavirus. screening positive coronavirus.\n[@539a7c,34,45) \"coronaviru...\"\nCOVID-19\n\n\nsample9.txt\nPatient have contact patient with coronavirus. screening positive coronavirus.\n[@539a7c,57,65) \"positive\"\npositive\n\n\nsample9.txt\nPatient have contact patient with coronavirus. screening positive coronavirus.\n[@539a7c,66,77) \"coronaviru...\"\nCOVID-19\n\n\n\n\n\n\n\n\n\n?Docs(\"sample2.txt\",D,V)\n\n'?Docs(\"sample2.txt\",D,V)'\n\n\n\n\n\n\n\nD\nV\n\n\n\n\nThe patient be tested for COVID-19 like_num. Results be positive. patient underwent no COVID-19 like_num education.\nlemma_concept\n\n\nThe patient be tested for Coronavirus like_num. Results be positive. patient underwent no Coronavirus like_num education.\nlemma\n\n\nThe patient was tested for Coronavirus 2019. Results are positive. Patient underwent no Coronavirus 2019 education.\nraw_text\n\n\n\n\n\n\n\n\n\n# here we get the spans of all POS\nPos(P,D,Word,Lem)&lt;-Docs(P,D,\"lemma_concept\"),pos(D)-&gt;(Word,Lem).\n?Pos(\"sample8.txt\",D,Word,Lem)\n\n'?Pos(\"sample8.txt\",D,Word,Lem)'\n\n\n\n\n\n\n\nD\nWord\nLem\n\n\n\n\npatient be sent for a COVID-19 test. Someone be tested positive.\n[@1edc3c,0,7) \"patient\"\nNOUN\n\n\npatient be sent for a COVID-19 test. Someone be tested positive.\n[@1edc3c,22,30) \"COVID-19\"\nNOUN\n\n\npatient be sent for a COVID-19 test. Someone be tested positive.\n[@1edc3c,31,35) \"test\"\nNOUN\n\n\npatient be sent for a COVID-19 test. Someone be tested positive.\n[@1edc3c,37,44) \"Someone\"\nPRON\n\n\npatient be sent for a COVID-19 test. Someone be tested positive.\n[@1edc3c,55,63) \"positive\"\nADJ\n\n\n\n\n\n\n\n\n\n# here we look for concept rule matches where the matched word is also tagged via POS\nPosConceptMatches(Path,Doc,Span,Label) &lt;- \n    Docs(Path,Doc,\"lemma_concept\"),\n    ConceptTagRules(Pattern, Label, \"pos\"),\n    rgx(Pattern,Doc) -&gt; (Span),\n    Pos(Path,Doc,Span,POSLabel).\n\nAs we can see for example in sample8.txt, Someone changed to other_experiencer.\n\nTargetMatches(Path,Doc, Span, Label) &lt;- \n    Docs(Path,Doc,\"pos_concept\"),\n    TargetTagRules(Pattern, Label), rgx(Pattern,Doc) -&gt; (Span).\n\nNow that we have finished rewriting our documents, lets look at the rewritting of the example\n\nfor doc,doc_type in sess.export('?Docs(\"sample9.txt\",D,V)').itertuples(index=False,name=None):\n    print(doc_type)\n    print(doc)\n    print(\"=\"*80)\n\nraw_text\nPatient had contact patient with coronavirus. screening positive coronavirus.\n================================================================================\nlemma\nPatient have contact patient with coronavirus. screening positive coronavirus.\n================================================================================\ntarget_concept\npatient have contact patient with COVID-19. positive coronavirus screening.\n================================================================================\nlemma_concept\npatient have contact patient with COVID-19. screening positive COVID-19.\n================================================================================\npos_concept\npatient have contact patient with COVID-19. screening positive COVID-19.\n================================================================================\n\n\n\n\nSplitting text by sentence and section\nWe have now finished our rewriting, This section replaces the sectionizer, and the parts of the context and postprocessing sections that deal with sentence splitting logic.\n\nBreaking text into sections\n\n# we get section spans and their content using our regex pattern and the rgx_split ie function\nSections(P,D,Sec,Content)&lt;-Docs(P,D,\"target_concept\"),\n    rgx_split($section_delimeter_pattern,D)-&gt;(SecSpan,Content),\n    as_str(SecSpan)-&gt;(Sec).\n?Sections(P,D,Sec,Content)\n\nPositiveSections(P,D,Sec,Content)&lt;-Sections(P,D,Sec,Content),SectionTags(Sec,Tag),PositiveSectionTags(Tag).\n?PositiveSections(P,D,Sec,Content)\n\n'?Sections(P,D,Sec,Content)'\n\n\n\n\n\n\n\nP\nD\nSec\nContent\n\n\n\n\nsample3.txt\nProblem List: like_num. Pneumonia like_num. COVID-19 like_num\nProblem List:\n[@882253,13,62) \" like_num....\"\n\n\n\n\n\n\n\n\n'?PositiveSections(P,D,Sec,Content)'\n\n\n\n\n\n\n\nP\nD\nSec\nContent\n\n\n\n\nsample3.txt\nProblem List: like_num. Pneumonia like_num. COVID-19 like_num\nProblem List:\n[@882253,13,62) \" like_num....\"\n\n\n\n\n\n\n\n\n\n\nBreaking texts into sentences\n\nSents(P,S)&lt;-Docs(P,D,\"target_concept\"),split_sentence(D)-&gt;(S).\n?Sents(P,S)\n\n'?Sents(P,S)'\n\n\n\n\n\n\n\nP\nS\n\n\n\n\nsample1.txt\n[@931cb5,0,43) \"patient pr...\"\n\n\nsample1.txt\n[@931cb5,44,93) \"His family...\"\n\n\nsample1.txt\n[@931cb5,94,130) \"COVID-19 r...\"\n\n\nsample10.txt\n[@f3a9fd,0,31) \"patient be...\"\n\n\nsample10.txt\n[@f3a9fd,32,59) \"results ca...\"\n\n\nsample2.txt\n[@e4b074,0,44) \"The patien...\"\n\n\nsample2.txt\n[@e4b074,45,65) \"Results be...\"\n\n\nsample2.txt\n[@e4b074,66,115) \"patient un...\"\n\n\nsample3.txt\n[@882253,0,23) \"Problem Li...\"\n\n\nsample3.txt\n[@882253,24,43) \"Pneumonia ...\"\n\n\nsample3.txt\n[@882253,44,61) \"COVID-19 l...\"\n\n\nsample4.txt\n[@77c574,0,23) \"neg COVID-...\"\n\n\nsample5.txt\n[@ffb7c7,0,29) \"positive C...\"\n\n\nsample6.txt\n[@b2612f,0,35) \"The patien...\"\n\n\nsample7.txt\n[@a2c41c,0,82) \"Elevated c...\"\n\n\nsample8.txt\n[@3db2e4,0,36) \"patient be...\"\n\n\nsample8.txt\n[@3db2e4,37,74) \"other_expe...\"\n\n\nsample9.txt\n[@6d2862,0,43) \"patient ha...\"\n\n\nsample9.txt\n[@6d2862,44,75) \"positive c...\"\n\n\n\n\n\n\n\n\n\n\nPair of sentences\nWe will show 3 ways of getting pairs of adjacent sentences, The first is simply to make an ie function out of them\n\nAlternative 1, build a dedicated ie function\n\nfrom itertools import pairwise\n\ndef sentence_pairs(text):\n    yield from pairwise(split_sentence(text))\n\nsess.register('sentence_pairs',sentence_pairs,[(str,Span)],[Span,Span])\n\n\nSentPairs_ver1(P,S1,S2)&lt;-Docs(P,D,\"target_concept\"),sentence_pairs(D)-&gt;(S1,S2).\n?SentPairs_ver1(P,S1,S2)\n\n'?SentPairs_ver1(P,S1,S2)'\n\n\n\n\n\n\n\nP\nS1\nS2\n\n\n\n\nsample1.txt\n[@931cb5,0,43) \"patient pr...\"\n[@931cb5,44,93) \"His family...\"\n\n\nsample1.txt\n[@931cb5,44,93) \"His family...\"\n[@931cb5,94,130) \"COVID-19 r...\"\n\n\nsample10.txt\n[@f3a9fd,0,31) \"patient be...\"\n[@f3a9fd,32,59) \"results ca...\"\n\n\nsample2.txt\n[@e4b074,0,44) \"The patien...\"\n[@e4b074,45,65) \"Results be...\"\n\n\nsample2.txt\n[@e4b074,45,65) \"Results be...\"\n[@e4b074,66,115) \"patient un...\"\n\n\nsample3.txt\n[@882253,0,23) \"Problem Li...\"\n[@882253,24,43) \"Pneumonia ...\"\n\n\nsample3.txt\n[@882253,24,43) \"Pneumonia ...\"\n[@882253,44,61) \"COVID-19 l...\"\n\n\nsample8.txt\n[@3db2e4,0,36) \"patient be...\"\n[@3db2e4,37,74) \"other_expe...\"\n\n\nsample9.txt\n[@6d2862,0,43) \"patient ha...\"\n[@6d2862,44,75) \"positive c...\"\n\n\n\n\n\n\n\n\nThe weakness in this approach is that we had build an IE function to do the extraction from scratch, obfuscating the fact that it and the split_sentence ie function share some logic. In our case since generating adjacent pairs is simple using itertools, this wasn’t so bad.\n\n\nGenerate pairs declaratively, and build a filter ie function\n\ndef is_adjacent(span1,span2):\n    yield span1.doc==span2.doc and span1.end +1 == span2.start\n\nsess.register('is_adjacent',is_adjacent,[Span,Span],[bool])\n\n\nSentPairs_ver2(P,S1,S2)&lt;-Sents(P,S1),Sents(P,S2),is_adjacent(S1,S2)-&gt;(True).\n?SentPairs_ver2(P,S1,S2)\n\n'?SentPairs_ver2(P,S1,S2)'\n\n\n\n\n\n\n\nP\nS1\nS2\n\n\n\n\nsample1.txt\n[@931cb5,0,43) \"patient pr...\"\n[@931cb5,44,93) \"His family...\"\n\n\nsample1.txt\n[@931cb5,44,93) \"His family...\"\n[@931cb5,94,130) \"COVID-19 r...\"\n\n\nsample10.txt\n[@f3a9fd,0,31) \"patient be...\"\n[@f3a9fd,32,59) \"results ca...\"\n\n\nsample2.txt\n[@e4b074,0,44) \"The patien...\"\n[@e4b074,45,65) \"Results be...\"\n\n\nsample2.txt\n[@e4b074,45,65) \"Results be...\"\n[@e4b074,66,115) \"patient un...\"\n\n\nsample3.txt\n[@882253,0,23) \"Problem Li...\"\n[@882253,24,43) \"Pneumonia ...\"\n\n\nsample3.txt\n[@882253,24,43) \"Pneumonia ...\"\n[@882253,44,61) \"COVID-19 l...\"\n\n\nsample8.txt\n[@3db2e4,0,36) \"patient be...\"\n[@3db2e4,37,74) \"other_expe...\"\n\n\nsample9.txt\n[@6d2862,0,43) \"patient ha...\"\n[@6d2862,44,75) \"positive c...\"\n\n\n\n\n\n\n\n\nThis is simpler than the first alternative, and we get to reuse the Sent rules, however it might seem a little bother some to implement and register an ie function for every so called “WHERE” clause we would like to implement.\n\n\nThird alternative, a generic boolean expression evaluator\n\nSentPairs(P,S1,S2)&lt;-Sents(P,S1),Sents(P,S2),expr_eval(\"{0}.end +1 == {1}.start\",S1,S2)-&gt;(True).\n?SentPairs(P,S1,S2)\n\n'?SentPairs(P,S1,S2)'\n\n\n\n\n\n\n\nP\nS1\nS2\n\n\n\n\nsample1.txt\n[@931cb5,0,43) \"patient pr...\"\n[@931cb5,44,93) \"His family...\"\n\n\nsample1.txt\n[@931cb5,44,93) \"His family...\"\n[@931cb5,94,130) \"COVID-19 r...\"\n\n\nsample10.txt\n[@f3a9fd,0,31) \"patient be...\"\n[@f3a9fd,32,59) \"results ca...\"\n\n\nsample2.txt\n[@e4b074,0,44) \"The patien...\"\n[@e4b074,45,65) \"Results be...\"\n\n\nsample2.txt\n[@e4b074,45,65) \"Results be...\"\n[@e4b074,66,115) \"patient un...\"\n\n\nsample3.txt\n[@882253,0,23) \"Problem Li...\"\n[@882253,24,43) \"Pneumonia ...\"\n\n\nsample3.txt\n[@882253,24,43) \"Pneumonia ...\"\n[@882253,44,61) \"COVID-19 l...\"\n\n\nsample8.txt\n[@3db2e4,0,36) \"patient be...\"\n[@3db2e4,37,74) \"other_expe...\"\n\n\nsample9.txt\n[@6d2862,0,43) \"patient ha...\"\n[@6d2862,44,75) \"positive c...\"\n\n\n\n\n\n\n\n\nThis alternative used a cheeky ie function from the standard library called expr_eval that lets us evaluate simple pythonic expression by writing them in a format similar to python’s format strings. This ie function is quite useful for replacing simple filters but becomes error prone for large complex expressions.\n\n\n\n\nTagging Covid Mentions\nNext we will tag covid mentions based on their context. This section replaces the rest of the Context and postprocessing rules\n\n# first we get the covid mentions and their surrounding sentences, using the span_contained ie function\nCovidMentions(Path, Span) &lt;- Docs(Path,D,\"target_concept\"), rgx(\"COVID-19\",D) -&gt; (Span).\nCovidMentionSents(P,Mention,Sent)&lt;-CovidMentions(P,Mention),Sents(P,Sent),span_contained(Mention,Sent)-&gt;(True).\n\n?CovidMentions(Path, Span)\n?CovidMentionSents(P,Mention,Sent)\n\n'?CovidMentions(Path,Span)'\n\n\n\n\n\n\n\nPath\nSpan\n\n\n\n\nsample1.txt\n[@931cb5,34,42) \"COVID-19\"\n\n\nsample1.txt\n[@931cb5,84,92) \"COVID-19\"\n\n\nsample1.txt\n[@931cb5,94,102) \"COVID-19\"\n\n\nsample2.txt\n[@e4b074,26,34) \"COVID-19\"\n\n\nsample2.txt\n[@e4b074,87,95) \"COVID-19\"\n\n\nsample3.txt\n[@882253,44,52) \"COVID-19\"\n\n\nsample4.txt\n[@77c574,4,12) \"COVID-19\"\n\n\nsample5.txt\n[@ffb7c7,9,17) \"COVID-19\"\n\n\nsample6.txt\n[@b2612f,26,34) \"COVID-19\"\n\n\nsample8.txt\n[@3db2e4,22,30) \"COVID-19\"\n\n\nsample9.txt\n[@6d2862,34,42) \"COVID-19\"\n\n\n\n\n\n\n\n\n'?CovidMentionSents(P,Mention,Sent)'\n\n\n\n\n\n\n\nP\nMention\nSent\n\n\n\n\nsample1.txt\n[@931cb5,34,42) \"COVID-19\"\n[@931cb5,0,43) \"patient pr...\"\n\n\nsample1.txt\n[@931cb5,84,92) \"COVID-19\"\n[@931cb5,44,93) \"His family...\"\n\n\nsample1.txt\n[@931cb5,94,102) \"COVID-19\"\n[@931cb5,94,130) \"COVID-19 r...\"\n\n\nsample2.txt\n[@e4b074,26,34) \"COVID-19\"\n[@e4b074,0,44) \"The patien...\"\n\n\nsample2.txt\n[@e4b074,87,95) \"COVID-19\"\n[@e4b074,66,115) \"patient un...\"\n\n\nsample3.txt\n[@882253,44,52) \"COVID-19\"\n[@882253,44,61) \"COVID-19 l...\"\n\n\nsample4.txt\n[@77c574,4,12) \"COVID-19\"\n[@77c574,0,23) \"neg COVID-...\"\n\n\nsample5.txt\n[@ffb7c7,9,17) \"COVID-19\"\n[@ffb7c7,0,29) \"positive C...\"\n\n\nsample6.txt\n[@b2612f,26,34) \"COVID-19\"\n[@b2612f,0,35) \"The patien...\"\n\n\nsample8.txt\n[@3db2e4,22,30) \"COVID-19\"\n[@3db2e4,0,36) \"patient be...\"\n\n\nsample9.txt\n[@6d2862,34,42) \"COVID-19\"\n[@6d2862,0,43) \"patient ha...\"\n\n\n\n\n\n\n\n\nNow we define how to derive covid tags using sections, context and the different postprocessing rule types. Notice how easy it is to convey complex control flow that combines multiple data sources elegantly using Spannerlog.\n\n# note that for ease of debugging, we extended our head to track which rule a fact was derived from\n\n# a tag is positive if it is contained in a positive section\nCovidTags(Path,Mention,'positive','section')&lt;-\n    PositiveSections(Path,D,Title,Section),\n    CovidMentions(Path,Mention),\n    span_contained(Mention,Section)-&gt;(True).\n\n# Context rules tags\nCovidTags(Path,Mention,Tag,'sentence context')&lt;-\n    CovidMentionSents(Path,Mention,Sent),\n    SentenceContextRules(Pattern,Tag,DisambiguationPattern),\n    rgx(Pattern,Sent)-&gt;(ContextSpan),\n    span_contained(Mention,ContextSpan)-&gt;(True),\n    rgx_is_match(DisambiguationPattern,Sent)-&gt;(False).\n\n# post processing based on pattern\nCovidTags(Path,Mention,Tag,'post pattern')&lt;-\n    CovidMentionSents(Path,Mention,Sent),\n    PostprocessPatternRules(Pattern,Tag),\n    rgx(Pattern,Sent)-&gt;(ContextSpan),\n    span_contained(Mention,ContextSpan)-&gt;(True).\n\n# post processing based on pattern and existing attributes\n# notice the recursive call to CovidTags\nCovidTags(Path,Mention,Tag,\"post attribute change\")&lt;-\n    CovidTags(Path,Mention,OldTag,Derivation),\n    PostprocessRulesWithAttributes(Pattern,OldTag,Tag),\n    CovidMentionSents(Path,Mention,Sent),\n    rgx(Pattern,Sent)-&gt;(ContextSpan),\n    span_contained(Mention,ContextSpan)-&gt;(True).\n\n# post processing based on pattern in the next sentence\nCovidTags(Path,Mention,Tag,\"next sentence\")&lt;-\n    CovidMentionSents(Path,Mention,Sent),\n    SentPairs(Path,Sent,NextSent),\n    PostprocessPatternRules(Pattern,Tag),\n    rgx(Pattern,NextSent)-&gt;(ContextSpan).\n\n?CovidTags(Path,Mention,Tag,Derivation)\n\n'?CovidTags(Path,Mention,Tag,Derivation)'\n\n\n\n\n\n\n\nPath\nMention\nTag\nDerivation\n\n\n\n\nsample1.txt\n[@931cb5,84,92) \"COVID-19\"\nnegated\nsentence context\n\n\nsample1.txt\n[@931cb5,84,92) \"COVID-19\"\npositive\nsentence context\n\n\nsample1.txt\n[@931cb5,94,102) \"COVID-19\"\npositive\nsentence context\n\n\nsample2.txt\n[@e4b074,87,95) \"COVID-19\"\nIGNORE\npost pattern\n\n\nsample2.txt\n[@e4b074,87,95) \"COVID-19\"\nfuture\nsentence context\n\n\nsample2.txt\n[@e4b074,87,95) \"COVID-19\"\nnegated\nsentence context\n\n\nsample3.txt\n[@882253,44,52) \"COVID-19\"\npositive\nsection\n\n\nsample4.txt\n[@77c574,4,12) \"COVID-19\"\nIGNORE\npost pattern\n\n\nsample4.txt\n[@77c574,4,12) \"COVID-19\"\nfuture\nsentence context\n\n\nsample4.txt\n[@77c574,4,12) \"COVID-19\"\nnegated\nsentence context\n\n\nsample5.txt\n[@ffb7c7,9,17) \"COVID-19\"\nfuture\nsentence context\n\n\nsample5.txt\n[@ffb7c7,9,17) \"COVID-19\"\nno_future\npost attribute change\n\n\nsample5.txt\n[@ffb7c7,9,17) \"COVID-19\"\npositive\nsentence context\n\n\nsample6.txt\n[@b2612f,26,34) \"COVID-19\"\npatient_experiencer\nsentence context\n\n\n\n\n\n\n\n\n\n\nDocument Classificaiton\nIn the following section we will aggregate tags on mentions into document classification in two stages, aggregation of tags per mention and aggregation of mentions per document. This section replaces the document classification state in the original pipeline.\n\nAggregatedCovidTags(Path,Mention,agg_mention(Tag))&lt;-\n    CovidTags(Path,Mention,Tag,Derivation).\n\n?AggregatedCovidTags(Path,Mention,Tag)\n\nDocumentTags(Path,agg_doc_tags(Tag))&lt;-\n    AggregatedCovidTags(Path,Mention,Tag).\n\n?DocumentTags(Path,Tag)\n\n'?AggregatedCovidTags(Path,Mention,Tag)'\n\n\n\n\n\n\n\nPath\nMention\nTag\n\n\n\n\nsample1.txt\n[@931cb5,84,92) \"COVID-19\"\nnegated\n\n\nsample1.txt\n[@931cb5,94,102) \"COVID-19\"\npositive\n\n\nsample2.txt\n[@e4b074,87,95) \"COVID-19\"\nIGNORE\n\n\nsample3.txt\n[@882253,44,52) \"COVID-19\"\npositive\n\n\nsample4.txt\n[@77c574,4,12) \"COVID-19\"\nIGNORE\n\n\nsample5.txt\n[@ffb7c7,9,17) \"COVID-19\"\npositive\n\n\nsample6.txt\n[@b2612f,26,34) \"COVID-19\"\nuncertain\n\n\n\n\n\n\n\n\n'?DocumentTags(Path,Tag)'\n\n\n\n\n\n\n\nPath\nTag\n\n\n\n\nsample1.txt\nPOS\n\n\nsample2.txt\nUNK\n\n\nsample3.txt\nPOS\n\n\nsample4.txt\nUNK\n\n\nsample5.txt\nPOS\n\n\nsample6.txt\nUNK\n\n\n\n\n\n\n\n\n\nHandling unmentioned paths:\nAt this step, we assign a classification result ‘UNK’ to paths not identified in the previous DataFrame result. This occurs when our pipeline doesn’t detect any mention of COVID-19 or its synonyms in the text of those paths. As a result, these paths are excluded from all types of relations, consistent with our primary focus on COVID-19 entities.\nAnd with this we have completed the pipeline. In the next section we will look at the entire code base, compare lines of code and analyze the advantages of the spannerlib implementation form a software engineering perspective.",
    "crumbs": [
      "Tutorials",
      "Rewriting a real codebase"
    ]
  },
  {
    "objectID": "tutorials/rewriting_a_real_codebase.html#end-to-end-implementation",
    "href": "tutorials/rewriting_a_real_codebase.html#end-to-end-implementation",
    "title": "Rewriting a real code base",
    "section": "End to End implementation",
    "text": "End to End implementation\n\nimports and configurations\n# imports\nfrom glob import glob\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pathlib import Path\nfrom spannerlib import get_magic_session,Session,Span\nsess = get_magic_session()\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# configurations\nslog_file = Path('covid_data/covid_logic.pl')\ninput_dir = Path('covid_data/sample_inputs')\ndata_dir = Path('covid_data/rules_data')\n\n\nSpannerlog Code\n%%spannerlog\nLemmas(P,D,Word,Lem)&lt;-Docs(P,D,\"raw_text\"),lemma(D)-&gt;(Word,Lem)\n\nLemmaConceptMatches(Path,Doc,Span,Label) &lt;- \\\n    Docs(Path,Doc,\"lemma\"),\\\n    ConceptTagRules(Pattern, Label, \"lemma\"),\\\n    rgx(Pattern,Doc) -&gt; (Span)\n\n# here we get the spans of all POS\nPos(P,D,Word,Lem)&lt;-Docs(P,D,\"lemma_concept\"),pos(D)-&gt;(Word,Lem)\n\n# here we look for concept rule matches where the matched word is also tagged via POS\nPosConceptMatches(Path,Doc,Span,Label) &lt;- \\\n    Docs(Path,Doc,\"lemma_concept\"),\\\n    ConceptTagRules(Pattern, Label, \"pos\"),\\\n    rgx(Pattern,Doc) -&gt; (Span),\\\n    Pos(Path,Doc,Span,POSLabel)\n\nTargetMatches(Path,Doc, Span, Label) &lt;- \\\n    Docs(Path,Doc,\"pos_concept\"),\\\n    TargetTagRules(Pattern, Label), rgx(Pattern,Doc) -&gt; (Span)\n\n# we get section spans and their content using our regex pattern and the rgx_split ie function\nSections(P,D,Sec,Content)&lt;-Docs(P,D,\"target_concept\"),\\\n    rgx_split($section_delimeter_pattern,D)-&gt;(SecSpan,Content),\\\n    as_str(SecSpan)-&gt;(Sec)\n\nPositiveSections(P,D,Sec,Content)&lt;-Sections(P,D,Sec,Content),SectionTags(Sec,Tag),PositiveSectionTags(Tag)\n\nSents(P,S)&lt;-Docs(P,D,\"target_concept\"),split_sentence(D)-&gt;(S)\n\nSentPairs(P,S1,S2)&lt;-Sents(P,S1),Sents(P,S2),expr_eval(\"{0}.end +1 == {1}.start\",S1,S2)-&gt;(True)\n\n# first we get the covid mentions and their surrounding sentences, using the span_contained ie function\nCovidMentions(Path, Span) &lt;- Docs(Path,D,\"target_concept\"), rgx(\"COVID-19\",D) -&gt; (Span)\nCovidMentionSents(P,Mention,Sent)&lt;-CovidMentions(P,Mention),Sents(P,Sent),span_contained(Mention,Sent)-&gt;(True)\n\n# note that for ease of debugging, we extended our head to track which rule a fact was derived from\n# a tag is positive if it is contained in a positive section\nCovidTags(Path,Mention,'positive','section')&lt;-\\\n    PositiveSections(Path,D,Title,Section),\\\n    CovidMentions(Path,Mention),\\\n    span_contained(Mention,Section)-&gt;(True)\n\n# Context rules tags\nCovidTags(Path,Mention,Tag,'sentence context')&lt;-\\\n    CovidMentionSents(Path,Mention,Sent),\\\n    SentenceContextRules(Pattern,Tag,DisambiguationPattern),\\\n    rgx(Pattern,Sent)-&gt;(ContextSpan),\\\n    span_contained(Mention,ContextSpan)-&gt;(True),\\\n    rgx_is_match(DisambiguationPattern,Sent)-&gt;(False)\n\n# post processing based on pattern\nCovidTags(Path,Mention,Tag,'post pattern')&lt;-\\\n    CovidMentionSents(Path,Mention,Sent),\\\n    PostprocessPatternRules(Pattern,Tag),\\\n    rgx(Pattern,Sent)-&gt;(ContextSpan),\\\n    span_contained(Mention,ContextSpan)-&gt;(True)\n\n# post processing based on pattern and existing attributes\n# notice the recursive call to CovidTags\nCovidTags(Path,Mention,Tag,\"post attribute change\")&lt;-\\\n    CovidTags(Path,Mention,OldTag,Derivation),\\\n    PostprocessRulesWithAttributes(Pattern,OldTag,Tag),\\\n    CovidMentionSents(Path,Mention,Sent),\\\n    rgx(Pattern,Sent)-&gt;(ContextSpan),\\\n    span_contained(Mention,ContextSpan)-&gt;(True)\n\n# post processing based on pattern in the next sentence\nCovidTags(Path,Mention,Tag,\"next sentence\")&lt;-\\\n    CovidMentionSents(Path,Mention,Sent),\\\n    SentPairs(Path,Sent,NextSent),\\\n    PostprocessPatternRules(Pattern,Tag),\\\n    rgx(Pattern,NextSent)-&gt;(ContextSpan)\n\nAggregatedCovidTags(Path,Mention,agg_mention(Tag))&lt;-\\\n    CovidTags(Path,Mention,Tag,Derivation)\n\nDocumentTags(Path,agg_doc_tags(Tag))&lt;-\\\n    AggregatedCovidTags(Path,Mention,Tag)\n\n\nIE and Agg functions:\ndef split_sentence(text):\n    \"\"\"\n    Splits a text into individual sentences. using spacy's sentence detection.\n    \n    Returns:\n        str: Individual sentences extracted from the input text.\n    \"\"\"\n\n    doc = nlp(str(text))\n    start = 0\n    for sentence in doc.sents:\n        end = start+len(sentence.text)\n        # note that we yield a Span object, so we can keep track of the locations of the sentences\n        yield Span(text,start,end)\n        start = end + 1\n\nclass LemmaFromList():\n    def __init__(self,lemma_list):\n        self.lemma_list = lemma_list\n\n    def __call__(self,text):\n        doc = nlp(str(text))\n        for word in doc:\n            start = word.idx\n            end = start + len(word.text)\n            if word.lemma_ in self.lemma_list:\n                yield (Span(text,start,end),word.lemma_)\n            elif word.like_num:\n                yield (Span(text,start,end),'like_num')\n            else:\n                pass\n\nlemma_list = (data_dir/'lemma_words.txt').read_text().split()\nlemmatizer = LemmaFromList(lemma_list)\n\nclass PosFromList():\n    def __init__(self,pos_list):\n        self.pos_list = pos_list\n    def __call__(self,text):\n        doc = nlp(str(text))\n        for word in doc:\n            start = word.idx\n            end = start + len(word.text)\n            if word.pos_ in self.pos_list:\n                yield (Span(text,start,end),word.pos_)\n\npos_annotator = PosFromList([\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"])\n\ndef agg_mention(group):\n    \"\"\"\n    aggregates attribute groups of covid spans\n    \"\"\"\n    if 'IGNORE' in group.values:\n        return 'IGNORE'\n    elif 'negated' in group.values and not 'no_negated' in group.values:\n        return 'negated'\n    elif 'future' in group.values and not 'no_future' in group.values:\n        return 'negated'\n    elif 'other experiencer' in group.values or 'not relevant' in group.values:\n        return 'negated'\n    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:\n        return 'positive'\n    else:\n        return 'uncertain'\n\ndef AggDocumentTags(group):\n    \"\"\"\n    Classifies a document as 'POS', 'UNK', or 'NEG' based on COVID-19 attributes.\n    \"\"\"\n    if 'positive' in group.values:\n        return 'POS'\n    elif 'uncertain' in group.values:\n        return 'UNK'\n    elif 'negated' in group.values:\n        return 'NEG'\n    else:\n        return 'UNK'\n\n\nRegular python utilities\ndef rewrite(text,span_label_pairs):\n    \"\"\"rewrites a string given a dataframe with spans and the string to rewrite them to\n    assumes that the spans belong to the text\n\n    Args:\n        text (str like): string to rewrite\n        span_label_pairs (pd.Dataframe) dataframe with two columns, first is spans in the doc to rewrite\n            second is what to rewrite to\n    Returns:\n        The rewritten string\n    \"\"\"    \n    if isinstance(text,Span):\n        text = text.as_str()\n    span_label_pairs = sorted(list(span_label_pairs.itertuples(index=False,name=None)), key=lambda x: x[0].start)\n\n    rewritten_text = ''\n    current_pos = 0\n    for span,label in span_label_pairs:\n        rewritten_text += text[current_pos:span.start] + label \n        current_pos = span.end\n\n    rewritten_text += text[current_pos:]\n\n    return rewritten_text\n\n\ndef rewrite_docs(docs,span_label,new_version):\n    \"\"\"Given a dataframe of documents of the form (path,doc,version) and a dataframe of spans to rewrite\n    of the form (path,word,from_span,to_tag), rewrites the documents and returns a new dataframe of the form\n    (path,doc,new_version)\n\n    \"\"\"\n    new_tuples =[]\n    span_label.columns = ['P','D','W','L']\n    for path,doc,_ in docs.itertuples(index=False,name=None):\n        span_label_per_doc = span_label[span_label['P'] == path][['W','L']]\n        new_text = rewrite(doc,span_label_per_doc)\n        new_tuples.append((path,new_text,new_version))\n    return pd.DataFrame(new_tuples,columns=['P','D','V'])\n\n\nMain pipeline\ndef main(input_dir,data_dir):\n    sess = Session()\n    # define callback functions\n    sess.register('split_sentence',split_sentence,[(str,Span)],[Span])\n    sess.register('pos',pos_annotator,[(Span,str)],[Span,str])\n    sess.register('lemma',lemmatizer,[(Span,str)],[Span,str])\n    sess.register_agg('agg_mention',agg_mention,[str],[str])\n    sess.register_agg('agg_doc_tags',AggDocumentTags,[str],[str])\n    \n    # bring in code as data\n    sess.import_rel(\"ConceptTagRules\",data_dir/\"concept_tags_rules.csv\" , delim=\",\")\n    sess.import_rel(\"TargetTagRules\",data_dir/\"target_rules.csv\",delim=\",\")\n    sess.import_rel(\"SectionTags\",data_dir/\"section_tags.csv\",delim=\",\")\n    sess.import_rel(\"PositiveSectionTags\",data_dir/\"positive_section_tags.csv\",delim=\",\")\n    sess.import_rel(\"SentenceContextRules\",data_dir/'sentence_context_rules.csv',delim=\"#\")\n    sess.import_rel(\"PostprocessPatternRules\",data_dir/'postprocess_pattern_rules.csv',delim=\"#\")\n    sess.import_rel(\"PostprocessRulesWithAttributes\",data_dir/'postprocess_attributes_rules.csv',delim=\"#\")\n    sess.import_rel(\"NextSentencePostprocessPatternRules\",data_dir/'postprocess_pattern_next_sentence_rules.csv',delim=',')\n\n\n    # we will programatically build a regex that matches all the section patterns\n    section_tags = pd.read_csv(data_dir/'section_tags.csv',names=['literal','tag'])\n    section_delimeter_pattern = section_tags['literal'].str.cat(sep='|')\n    sess.import_var('section_delimeter_pattern',section_delimeter_pattern)\n\n    # bring in data\n    file_paths = [Path(p) for p in glob(str(input_dir/'*.txt'))]\n    raw_docs = pd.DataFrame([\n        [p.name,p.read_text(),'raw_text'] for p in file_paths\n    ],columns=['Path','Doc','Version']\n    )\n    sess.import_rel('Docs',raw_docs)\n\n    # load logic, note that since we did not define the data relations in the logic file,\n    # we need to load the logic after the data has been loaded\n    sess.export(logic_file.read_text())\n\n    ## Rewritting the documents\n    lemma_tags = sess.export('?Lemmas(P,D,W,L)')\n    lemma_docs = rewrite_docs(raw_docs,lemma_tags,'lemma')\n    sess.import_rel('Docs',lemma_docs)\n\n    lemma_concept_matches = sess.export('?LemmaConceptMatches(Path,Doc,Span,Label)')\n    lemma_concepts = rewrite_docs(lemma_docs,lemma_concept_matches,'lemma_concept')\n    sess.import_rel('Docs',lemma_concepts)\n\n    pos_concept_matches = sess.export('?PosConceptMatches(P,D,W,L)')\n    pos_concept_docs = rewrite_docs(lemma_concepts,pos_concept_matches,'pos_concept')\n    sess.import_rel('Docs',pos_concept_docs)\n\n    target_matches = sess.export('?TargetMatches(P,D,W,L)')\n    target_rule_docs = rewrite_docs(pos_concept_docs,target_matches,'target_concept')\n    sess.import_rel('Docs',target_rule_docs)\n\n    ## computing the tags based on the target concept documents\n    doc_tags = sess.export('?DocumentTags(P,T)')\n\n    # handling files with no mentions\n    paths = pd.DataFrame([p.name for p in file_paths],columns=['P'])\n    classification = paths.merge(doc_tags,on='P',how='outer')\n    classification['T']=classification['T'].fillna('UNK')\n    classification\n\n    return classification",
    "crumbs": [
      "Tutorials",
      "Rewriting a real codebase"
    ]
  },
  {
    "objectID": "tutorials/rewriting_a_real_codebase.html#code-comparison",
    "href": "tutorials/rewriting_a_real_codebase.html#code-comparison",
    "title": "Rewriting a real code base",
    "section": "Code comparison",
    "text": "Code comparison\n\nLines of code\nSumming the line of code analysis for both implementations we get:\n\n\n\nImplementation\nCode Type\n~#code lines\n\n\n\n\nOriginal Implementation\nRules Collections\n3903\n\n\n\nVanilla Python\n639\n\n\nSpannerlib Implementation\nData\n378\n\n\n\nIE/AGG functions\n76\n\n\n\nSpannerlog\n80\n\n\n\nVanilla python\n118\n\n\n\nOverall we see that the different type of rules, which were basically data as code and made up the majority of the code base, shrank by a factor of 10 (~3900 to ~380). Moreover the Vanilla python code, over 600 lines long, shrank to less than 300 lines of code, over half of which were either Spannerlog code, or stateless IE/AGG functions.\n\n\nSoftware engineering perspective\nIn order to\n\nfully appreciate the strengths of the spannerlib framework.\nUnderstand our reasoning behind dividing the code into the four modalities mentioned above in our analysis.\n\nIn this section we will refer to our covid pipeline refactoring as an example that helps highlight the benefits of the framework in general.\nWe will remind the reader of a several important concepts:\nDecomposition/Factoring:\n\nThe breaking apart of code into parts that are easier to understand, program and maintain\n\nSeparation of concerns:\n\nA design principle that states that each section in a code should address a separate concern\nThe goal of this principle is to make code easier to program and maintain by having the programmer:\n\nrequired to reason about less concept when working on a section of code.\nrequired to reason about less sections of code when trying to modify an aspect of the program.\n\n\nBug surface area:\n\nIs also affected by complexity of the state of the program\nBug surface area is often divided into compile (static) surface area and run time (dynamic) surface area\n\nLooking at static and dynamic surface area separately is important since static bugs can be caught easily using a compiler/interpreter, can be proved to exist/not exist and do not require building tests to catch.\n\n\nReadability:\n\nA measure of how easy it is to understand code.\n\nDebugability:\n\nA measure of how easy it is to find bugs in a code base.\n\nThis measure does not always coincide with readability, for example, multithreaded code is a clear example where the intent of the code might be easy to understand, but the non deterministic nature of the execution will make debugging hard.\n\n\nBarriers of entry:\n\nThe difficulties in programming effectively in an existing codebase that stem from\n\nThe technical complexity of the codebase\nthe learning curve of the specific libraries, concepts or technologies used in the project\n\n\nSo why does it matter how many lines of code per modality an implementation has? Why do we separate vanilla python code from IE functions? Because different modalities (as well as different programming languages) have different costs with respect to:\n\nreadability\ndebugability\nand bug surface area\n\nper volume of code (which we approximate via lines of code in this discussion).\nThis fact is well known across programming languages, for example: A typical python function f is more readable than its C counterpart g, but fs bug surface area is greater even though g will often be much longer. This is mainly due to the fact the C is a statically typed language that chooses to reduce run time bug surface area in favor of less readability.\nThe same is true to the four different modalities we analyze here:\n\nCode as Data\nDeclerative Code (and specificall python)\nStateless python code (IE functions)\nGeneral python code\n\nAs we move up this list, we have more and more freedom, the code\n\nbecomes less readable\nhas a larger bug surface area\nand is harder to debug\n\nA csv line’s is much simpler to verify than a line of spannerlog than a line of stateless python than a line of generic python. Put more techincally, the scaling factors of these code measures get worse as we move up the chain.\nFor this reason, we do not only care how many lines of code we removed from an implementation, but how many lines of code became more readable debugable etc due to a change in modality. The Rules in the original implementation turned into csv files, which are easier to statically verify, making the reduction of overall complexity more substantial than the reduction in lines of code. The regex patterns, per line of code, are harder to read for a human but are easier for a machine to verify the correctness of.\nAs for the vanilla python code in the original implementation, the reduction of complexity of the code does not simply come from the &gt;2x reduction in lines of code, but comes from the fact that over half of said code in the new implementation is either declarative or stateless. An example of this are the callback functions added to the post processing section.\nSo programming in the spannerlib framework, when the task can be partially modelled as an IE task, simplifies programming not only by reducing total volume of code, but by reducing the bug surface area, readability and debugability of conceptually simpler code by moving it to an appropriate modality with better scaling factors for these measures.\nBut there is another advantages to programming in spannerlib, namely:\n\nBetter separation of concerns\nBetter inspectability of state\n\nThe separation of spannerlib code into Data, Logic and stateless computation matches 4 distinct concerns in programming, each programmed in a modality suited to it namely:\n\nState management - using relational databases\nData representation - using relation databases\nAlgorithmic code - using state less IE functions\nBusiness Logic / Compositional Logic - Decleratve language that can orchestrate IE functions.\n\nThis separation of concerns make the code better factored but also gives us better tools for common code maintenance tasks:\n\nWhen we want to reason about program state\n\nInstead of:\n\ninspecting state by going through long runtime inspections using debugger\nor changing existing code to add more logging or debugging prints\n\nWe can:\n\nquery the DB for the state\n\nExample:\n\nour debugging queries that looked at the document’s per version.\nNote that even when we used vanilla python code, we still saved the state in spannerlib so we could inspect it\n\n\nWhen we want to trace intermediate state, for data provenance\n\nInstead of:\n\nhaving to add support for this throughout the class hierarchy in our code\nor add logs and parse them\n\nWe can:\n\nextend the schema of rules to save auxiliary information.\n\nExample:\n\nAdding the Derived from column in the CovidTags relation so we can see where each tuple came from.\n\n\nWhen writing algorithmic code,\n\nInstead of\n\nreasoning about state or making decisions about data representation.\n\nWe can:\n\nsimply find a relational schema that matches\n\n\nWhen business logic changes,\n\nchangeing the compositional logic of the pipeline but not the core of the product\nwhich is most of the time\nIntead of:\n\nhaving to change both the class structure to support the additional data\nand the pipeline code\n\nWe can\n\nsimply refactor the declarative code, which automatically refactors the data representation with it.\n\nExample:\n\nAdding the PostProcessing Steps\n\n\n\n\n\nBut how much of this is really new?\nSpannerlib’s approach while innovative, is combination of several known techniques and approaches in the IE and Programming Languages literature namely:\n\nDocument Spanners\n\nWhich gives us the Span primitives that simplify a lot of extraction tasks on text.\n\nDeclarative Information Extraction with Embedded Extraction Predicates\n\nWhich allows us to simplify imperative compositional code via declarative code over imperative stateless functions.\nThis is the python in Spannerlog embedding.\n\nGenerative Programming\n\nWhich gives describes techniques for reducing repetitive code by:\n\nUsing a high level programming language as a composition engine of\nA DSL which is suited to the programming domain which\nGenerates lower level code.\n\nThis gives us the Spannerlog in python embedding.\n\n\nSpannerlib’s innovation comes from several key nuances that reduces the barrier of entry to the benefit of formal IE, and enables it to be used as a generative programming engine for a very wide array of tasks.\n\nWe realize, following the IE literature, that declarative query languages, onces paired with imperative callbacks, provide a very generic DSL for function composition, that encompasses a large percentage of pipeline composition code today.\n\nCombining the Generative programming paradigm with formal IE systems.\n\nUnlike existing formal IE systems, like SystemT, we reduce the barrier of entry for new programmers by\n\nreducing the barrier of entry for lay programmers to insert IE functions into our system.\nreduce the learning curve for our system by formally extending existing, well known, and simple declarative languages (Datalog).\n\nImproving adoption and developer velocity by\n\nPutting an emphasis on concise interplay and a tiny interface for communicating between the host language (python) and our framework\nPackaging it as a python library",
    "crumbs": [
      "Tutorials",
      "Rewriting a real codebase"
    ]
  },
  {
    "objectID": "sample_question.html",
    "href": "sample_question.html",
    "title": "Spannerlib",
    "section": "",
    "text": "After familiarizing yourself with the syntax of spannerlog,  let’s explore another example that highlights the power of this declarative language.  given a story.txt file which contains in between its lines parent-child information, the information is provided as follows:  1- X is the parent/father of Y  2- Y is the child/daughter of X\nThe story also has some last names. The task is to identify the last name of all individuals. It’s important to note that you don’t have access to individual last names, but you can deduce them by constructing family trees based on the information provided in the story.\nHere is the first line of the story as an example:\nWhat you should deduce from this line is that Owen is the parent of Aria as it matches the first template of provided information (Y is the daughter of X),  and since Aria’s name was given along with her lastname,  you should also conclude that Owen’s last name is also Walker.\nTake a couple of minutes to think a solution for this problem.\nNow that you understand the complexity of deducing last names based on family trees and extracting the parent-child relationships using Python,  let us show you how easily we can do that by incorporating spannerlog into the equation.\nfrom spannerlib import get_magic_session,Span\n\nsess = get_magic_session()\n\nstory = Span(\"\"\"\nOnce upon a time, there was a little girl named Aria Walker. Aria is the daughter of Owen. Owen was a kind and caring father who adored his daughter's laughter and curiosity.\nIn the neighboring town, there lived a cheerful boy named Eliana. Eliana's eyes sparkled with innocence, Owen is the father of Eliana.\nOwen cherished every moment spent with Eliana, teaching her valuable life lessons and nurturing her dreams.\nOne sunny day, Aria gave birth to a wonderful girl called Mila. Mila is the daughter of Aria, Aria, watched over her with unwavering love and pride.\nAria and Mila formed an instant bond, becoming the best of friends and had a blossoming friendship.\n\nAs the years passed, Mila grew into a confident young girl, thanks to the support and encouragement of her mother, Aria. Mila, in turn,\nbecame a caring sibling to her little brother,Owen is the father of Caleb. Their bond was unbreakable,\nand they shared countless adventures together under the watchful eyes of their mother.\n\nAcross the village, there lived a young girl named Lila. Lila was a spirited and curious child, Lila is the daughter of Benjamin, she was\nalways seeking new adventures in the world around her. \nHer father, Benjamin Smith, was a wise and gentle man who cherished his daughter's laughter and inquisitive nature.\nBenjamin had another child, Daniel is the child of Benjamin, Yaer is the parent of Benjamin.\n\"\"\",name='story')\n\nsess.import_var(\"story\", story)\nparent(X,Y) &lt;- rgx(\"(\\w+)\\s+is\\s+the\\s+daughter\\s+of\\s+(\\w+)\",$story) -&gt; (Y,X).\nparent(X,Y) &lt;- rgx(\"(\\w+)\\s+is\\s+the\\s+child\\s+of\\s+(\\w+)\",$story) -&gt; (Y,X).\nparent(X,Y) &lt;- rgx(\"(\\w+)\\s+is\\s+the\\s+parent\\s+of\\s+(\\w+)\",$story) -&gt; (X,Y).\nparent(X,Y) &lt;- rgx(\"(\\w+)\\s+is\\s+the\\s+father\\s+of\\s+(\\w+)\",$story) -&gt; (X,Y).\nlastname(X,Y)&lt;-rgx(\"([A-Z][a-z]+)\\s([A-Z][a-z]+)\",$story) -&gt; (X,Y).\n?parent(X,Y)\n?lastname(X,Y)\n\n'?parent(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n[@story,86,90) \"Owen\"\n[@story,62,66) \"Aria\"\n\n\n[@story,281,285) \"Owen\"\n[@story,303,309) \"Eliana\"\n\n\n[@story,507,511) \"Aria\"\n[@story,483,487) \"Mila\"\n\n\n[@story,851,855) \"Owen\"\n[@story,873,878) \"Caleb\"\n\n\n[@story,1116,1124) \"Benjamin\"\n[@story,1092,1096) \"Lila\"\n\n\n[@story,1357,1365) \"Benjamin\"\n[@story,1334,1340) \"Daniel\"\n\n\n[@story,1367,1371) \"Yaer\"\n[@story,1389,1397) \"Benjamin\"\n\n\n\n\n\n\n\n\n\n'?lastname(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n[@story,49,53) \"Aria\"\n[@story,54,60) \"Walker\"\n\n\n[@story,1202,1210) \"Benjamin\"\n[@story,1211,1216) \"Smith\""
  },
  {
    "objectID": "sample_question.html#deducing-last-names-based-on-family-trees-with-spannerlog",
    "href": "sample_question.html#deducing-last-names-based-on-family-trees-with-spannerlog",
    "title": "Spannerlib",
    "section": "Deducing Last Names Based on Family Trees with spannerlog",
    "text": "Deducing Last Names Based on Family Trees with spannerlog\nThe following spannerlog code provides a simple yet powerful way to deduce the last name of each person based on the family tree information extracted to the parent relation.\n\nFamily(X,Y) &lt;- lastname(Y,X).\nFamily(X,Y) &lt;- Family(X,Z), parent(Y,Z).\nFamily(X,Y) &lt;- Family(X,Z), parent(Z,Y).\n?Family(X,Y)\n\n'?Family(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n[@story,54,60) \"Walker\"\n[@story,49,53) \"Aria\"\n\n\n[@story,1211,1216) \"Smith\"\n[@story,1202,1210) \"Benjamin\""
  },
  {
    "objectID": "micro_passes.html",
    "href": "micro_passes.html",
    "title": "Micro Passes",
    "section": "",
    "text": "class DummySession():\n    def __init__(self,passes=None,execution_function=None):\n        if passes is None:\n            passes = []\n        self.passes = passes\n        self.engine=Engine()\n        self.should_execute = execution_function is not None\n        self.execution_function = execution_function\n\n    def run_query(self,query):\n        statements = parse_spannerlog(query,split_statements=True)\n        clean_asts = []\n        results = []\n        for statement_nx,statement_lark in statements:\n            ast = statement_nx\n            for pass_ in self.passes:\n                pass_(ast,self.engine)\n            clean_asts.append(ast)\n\n            if self.should_execute:\n                results.append(self.execution_function(ast,self.engine))\n        if self.should_execute:\n            return results\n        else:\n            return clean_asts\n\n\ndef assert_asts(asts,expected=None):\n    if expected is None:\n        return [serialize_tree(ast) for ast in asts]\n    for i,(ast,expected_ast) in enumerate(zip(asts,expected)):\n        serialized = serialize_tree(ast)\n        assert serialized == expected_ast,(f\"AST {i} does not match expected\"\n            f\"\\nAST:\\n{serialized}\\nExpected:\\n{expected_ast}\\n\"\n            f\"Diff is:{DeepDiff(serialized,expected_ast)}\")\n\n\nsess = DummySession()\nasts = sess.run_query(\"\"\"\n            new string(str)\n            string(\"a\")\n            string_length(Str, Len) &lt;- string(Str), Length(Str) -&gt; (Len).\n            \"\"\")\nfor ast in asts:\n    draw(ast)"
  },
  {
    "objectID": "micro_passes.html#scaffolding",
    "href": "micro_passes.html#scaffolding",
    "title": "Micro Passes",
    "section": "",
    "text": "class DummySession():\n    def __init__(self,passes=None,execution_function=None):\n        if passes is None:\n            passes = []\n        self.passes = passes\n        self.engine=Engine()\n        self.should_execute = execution_function is not None\n        self.execution_function = execution_function\n\n    def run_query(self,query):\n        statements = parse_spannerlog(query,split_statements=True)\n        clean_asts = []\n        results = []\n        for statement_nx,statement_lark in statements:\n            ast = statement_nx\n            for pass_ in self.passes:\n                pass_(ast,self.engine)\n            clean_asts.append(ast)\n\n            if self.should_execute:\n                results.append(self.execution_function(ast,self.engine))\n        if self.should_execute:\n            return results\n        else:\n            return clean_asts\n\n\ndef assert_asts(asts,expected=None):\n    if expected is None:\n        return [serialize_tree(ast) for ast in asts]\n    for i,(ast,expected_ast) in enumerate(zip(asts,expected)):\n        serialized = serialize_tree(ast)\n        assert serialized == expected_ast,(f\"AST {i} does not match expected\"\n            f\"\\nAST:\\n{serialized}\\nExpected:\\n{expected_ast}\\n\"\n            f\"Diff is:{DeepDiff(serialized,expected_ast)}\")\n\n\nsess = DummySession()\nasts = sess.run_query(\"\"\"\n            new string(str)\n            string(\"a\")\n            string_length(Str, Len) &lt;- string(Str), Length(Str) -&gt; (Len).\n            \"\"\")\nfor ast in asts:\n    draw(ast)"
  },
  {
    "objectID": "micro_passes.html#convert_primitive_values_to_objects",
    "href": "micro_passes.html#convert_primitive_values_to_objects",
    "title": "Micro Passes",
    "section": "convert_primitive_values_to_objects",
    "text": "convert_primitive_values_to_objects\n\nbool('False')\n\nTrue\n\n\n\nsource\n\nconvert_primitive_values_to_objects\n\n convert_primitive_values_to_objects (ast, session)\n\n\nsess = DummySession(passes=[\n  convert_primitive_values_to_objects\n  ])\nasts = sess.run_query(\"\"\"\nx=\"a\"\nx=True\nx=False\nx=1.3\nx=-3.5\nx=1\nx=-2\nx=y\nx= \"hello \\\nworld\"\n            \"\"\")\nfor ast in asts:\n    draw(ast)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsess = DummySession(passes=[convert_primitive_values_to_objects])\nasts = sess.run_query(\"\"\"\n            x=1\n            S(\"a\",1,2)\n            new R(int,str)\n            ?R($x,X)\n            R(X,sum(Y))&lt;-S(X,Y).\n            \"\"\")\nfor ast in asts:\n    draw(ast)\n\nassert_asts(asts,[{'type': 'assignment',\n  'id': 0,\n  'children': [{'type': 'var_name', 'val': Var(name='x'), 'id': 1},\n   {'type': 'int', 'val': 1, 'id': 3}]},\n {'type': 'add_fact',\n  'id': 0,\n  'children': [{'type': 'relation_name', 'val': 'S', 'id': 1},\n   {'type': 'term_list',\n    'id': 3,\n    'children': [{'type': 'string', 'val': 'a', 'id': 4},\n     {'type': 'int', 'val': 1, 'id': 6},\n     {'type': 'int', 'val': 2, 'id': 8}]}]},\n {'type': 'relation_declaration',\n  'id': 0,\n  'children': [{'type': 'relation_name', 'val': 'R', 'id': 1},\n   {'type': 'decl_term_list',\n    'id': 3,\n    'children': [{'val': int, 'id': 4}, {'val': str, 'id': 5}]}]},\n {'type': 'query',\n  'id': 0,\n  'children': [{'type': 'relation_name', 'val': 'R', 'id': 1},\n   {'type': 'term_list',\n    'id': 3,\n    'children': [{'type': 'var_name', 'val': Var(name='x'), 'id': 4},\n     {'type': 'free_var_name', 'val': FreeVar(name='X'), 'id': 6}]}]},\n {'type': 'rule',\n  'id': 0,\n  'children': [{'type': 'rule_head',\n    'id': 1,\n    'children': [{'type': 'relation_name', 'val': 'R', 'id': 2},\n     {'type': 'term_list',\n      'id': 4,\n      'children': [{'type': 'free_var_name',\n        'val': FreeVar(name='X'),\n        'id': 5},\n       {'type': 'aggregated_free_var',\n        'id': 7,\n        'children': [{'type': 'agg_name', 'val': 'sum', 'id': 8},\n         {'type': 'free_var_name', 'val': FreeVar(name='Y'), 'id': 10}]}]}]},\n   {'type': 'rule_body_relation_list',\n    'id': 12,\n    'children': [{'type': 'relation',\n      'id': 13,\n      'children': [{'type': 'relation_name', 'val': 'S', 'id': 14},\n       {'type': 'term_list',\n        'id': 16,\n        'children': [{'type': 'free_var_name',\n          'val': FreeVar(name='X'),\n          'id': 17},\n         {'type': 'free_var_name', 'val': FreeVar(name='Y'), 'id': 19}]}]}]}]}])"
  },
  {
    "objectID": "micro_passes.html#check-reserved-relation-names",
    "href": "micro_passes.html#check-reserved-relation-names",
    "title": "Micro Passes",
    "section": "Check reserved relation names",
    "text": "Check reserved relation names\n\nsource\n\nCheckReservedRelationNames\n\n CheckReservedRelationNames (reserved_prefix)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    # remove_new_lines_from_strings,\n    CheckReservedRelationNames('spanner_'),\n    ])\nasts = sess.run_query(\"\"\"\n            S(\"a\",1)\n            R(X,Y)&lt;-S(X,Y),T(X,Y).\n            \"\"\")\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n            spanner_S(\"a\",1)\n            \"\"\")\nprint(exc_info.value)\n\nRelation name 'spanner_S' starts with reserved prefix 'spanner_'\n\n\n\ndraw(asts[0])"
  },
  {
    "objectID": "micro_passes.html#derefrence-vars",
    "href": "micro_passes.html#derefrence-vars",
    "title": "Micro Passes",
    "section": "derefrence vars",
    "text": "derefrence vars\n\nsource\n\ndereference_vars\n\n dereference_vars (ast, engine)\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    # remove_new_lines_from_strings,\n    CheckReservedRelationNames('spanner_'),\n    dereference_vars,\n    ])\n\nsess.engine.set_var('y',1)\nsess.engine.set_var('x',\"hello\")\n\nasts = sess.run_query(f\"\"\"\n            z=1\n            x=$y\n            R($x,$y)\n            \"\"\")\nfor ast in asts:\n    draw(ast)\n\nassert_asts(asts,[{'type': 'assignment',\n  'id': 0,\n  'children': [{'type': 'var_name_lhs', 'val': Var(name='z'), 'id': 1},\n   {'type': 'int', 'val': 1, 'id': 3}]},\n {'type': 'assignment',\n  'id': 0,\n  'children': [{'type': 'var_name_lhs', 'val': Var(name='x'), 'id': 1},\n   {'type': int, 'val': 1, 'id': 3}]},\n {'type': 'add_fact',\n  'id': 0,\n  'children': [{'type': 'relation_name', 'val': 'R', 'id': 1},\n   {'type': 'term_list',\n    'id': 3,\n    'children': [{'type': str, 'val': 'hello', 'id': 4},\n     {'type': int, 'val': 1, 'id': 6}]}]}])\n\n\nwith pytest.raises(ValueError) as exc_info:\n    _ = sess.run_query(f\"\"\"\n                R($x,$z)\n                \"\"\")\nassert 'Variable z is not defined' in str(exc_info.value)\nprint(exc_info.value)\n\n\n\n\n\n\n\n\n\n\nVariable z is not defined"
  },
  {
    "objectID": "micro_passes.html#check-read-assignments-got-existing-path",
    "href": "micro_passes.html#check-read-assignments-got-existing-path",
    "title": "Micro Passes",
    "section": "Check read assignments got existing path",
    "text": "Check read assignments got existing path\n\nsource\n\ncheck_referenced_paths_exist\n\n check_referenced_paths_exist (ast, engine)\n\n\n# check that read assignments got a string which is an existing path\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    # remove_new_lines_from_strings,\n    CheckReservedRelationNames('spanner_'),\n    dereference_vars,\n    check_referenced_paths_exist,\n    ])\n\nsess.engine.set_var('y','file.txt')\n\n\nfile = Path(\"file.txt\")\nfile.touch()\n\n# TODO figure out why this doesnt work\nasts = sess.run_query(f\"\"\"\n            x=read(\"file.txt\")\n            z=read(y)\n            \"\"\")\n\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n            x=read(\"not_existing_file.txt\")\n            \"\"\")\nprint(exc_info.value)\n\n\nsess.engine.set_var('w','not_existing_file.txt')\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n            x=read(w)\n            \"\"\")\nprint(exc_info.value)\n\nfile.unlink()\n\nPath('file.txt')\nPath('file.txt')\nPath('not_existing_file.txt')\npath not_existing_file.txt was not found in /Users/dean/tdk/spannerlib/nbs\nPath('not_existing_file.txt')\npath not_existing_file.txt was not found in /Users/dean/tdk/spannerlib/nbs"
  },
  {
    "objectID": "micro_passes.html#inline-aggregation-func-names-on-free-var-nodes",
    "href": "micro_passes.html#inline-aggregation-func-names-on-free-var-nodes",
    "title": "Micro Passes",
    "section": "Inline aggregation func names on free var nodes",
    "text": "Inline aggregation func names on free var nodes\n\nsource\n\ninline_aggregation\n\n inline_aggregation (ast, engine)\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    CheckReservedRelationNames('spanner_'),\n    check_referenced_paths_exist,\n    dereference_vars,\n    inline_aggregation,\n    ])\n\nwith checkLogs():\n    asts = sess.run_query(\"\"\"\n    R(X,sum(Y),count(Z))&lt;-S(X,Y,Z).\n    \"\"\")\nfor ast in asts:\n    draw(ast)\n\nassert_asts(asts,[{'type': 'rule',\n  'id': 0,\n  'children': [{'type': 'rule_head',\n    'id': 1,\n    'children': [{'type': 'relation_name', 'val': 'R', 'id': 2},\n     {'type': 'term_list',\n      'id': 4,\n      'children': [{'type': 'free_var_name',\n        'val': FreeVar(name='X'),\n        'id': 5},\n       {'type': 'free_var_name',\n        'val': FreeVar(name='Y'),\n        'agg': 'sum',\n        'id': 7},\n       {'type': 'free_var_name',\n        'val': FreeVar(name='Z'),\n        'agg': 'count',\n        'id': 12}]}]},\n   {'type': 'rule_body_relation_list',\n    'id': 17,\n    'children': [{'type': 'relation',\n      'id': 18,\n      'children': [{'type': 'relation_name', 'val': 'S', 'id': 19},\n       {'type': 'term_list',\n        'id': 21,\n        'children': [{'type': 'free_var_name',\n          'val': FreeVar(name='X'),\n          'id': 22},\n         {'type': 'free_var_name', 'val': FreeVar(name='Y'), 'id': 24},\n         {'type': 'free_var_name', 'val': FreeVar(name='Z'), 'id': 26}]}]}]}]}])\n\n\n\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    CheckReservedRelationNames('spanner_'),\n    check_referenced_paths_exist,\n    dereference_vars,\n    inline_aggregation,\n    ])\n\nwith checkLogs():\n    asts = sess.run_query(\"\"\"\n    R(X,sum(Y),count(Z))&lt;-S(X,Y),T(X,Y)-&gt;(Z).\n    R(X,sum(Y),count(Z),\"yes\")&lt;-S(X,Y,Z).\n    \"\"\")\nfor ast in asts:\n    draw(ast)"
  },
  {
    "objectID": "micro_passes.html#cast-relations-to-dataclasses",
    "href": "micro_passes.html#cast-relations-to-dataclasses",
    "title": "Micro Passes",
    "section": "Cast relations to dataclasses",
    "text": "Cast relations to dataclasses\n\nsource\n\nrelations_to_dataclasses\n\n relations_to_dataclasses (ast, engine)\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    CheckReservedRelationNames('spanner_'),\n    check_referenced_paths_exist,\n    dereference_vars,\n    inline_aggregation,\n    relations_to_dataclasses\n    ])\n\n# with checkLogs():\nasts = sess.run_query(\"\"\"\n# R(\"hello\",6)\n# R(\"hello\",[4,5))&lt;-True.\n# R(\"hello\",[4,5))&lt;-False.\n# new R(str,span,int,int)\n# ?R(\"hello\",Y)\nR(X,Y)&lt;-S(X,Y),T(X,Y)-&gt;(Y,Z).\nR(X,sum(Y))&lt;-S(X,Y).\nR(X,sum(Y),\"yes\")&lt;-S(X,Y).\n\"\"\")\nfor ast in asts:\n    draw(ast)\n\n\n\n\n\n\n\n\n\n\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n        R(X,Y)&lt;-S(X,sum(Y)),T(X,Y)-&gt;(Y,Z).\n            \"\"\")\nprint(exc_info.value)\nassert \"Aggregations are only allowed in rule heads, not in relation\" in str(exc_info.value)\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n        R(X,Y)&lt;-S(X,Y),T(X,sum(Y))-&gt;(Y,Z).\n            \"\"\")\nprint(exc_info.value)\nassert \"Aggregations are not allowed in IE relations, found in T(X,Y) -&gt; (Y,Z)\" in str(exc_info.value)\n\nAggregations are only allowed in rule heads, not in relation, found in S(X,sum(Y))\nAggregations are not allowed in IE relations, found in T(X,Y) -&gt; (Y,Z)\n\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    CheckReservedRelationNames('spanner_'),\n    check_referenced_paths_exist,\n    dereference_vars,\n    inline_aggregation,\n    relations_to_dataclasses\n    ])\nasts = sess.run_query(\"\"\"\nR(\"hello\",6)\n+R(\"hello\",6)\n-R(\"hello\",6)\nnew R(str,str,int,int)\n?R(\"hello\",Y)\nR(X,Y)&lt;-S(X,Y),T(X,Y)-&gt;(Y,Z).\nR(X,sum(Y))&lt;-S(X,Y).\nR(X,sum(Y),\"yes\")&lt;-S(X,Y).\n\n\"\"\")\nfor ast in asts:\n    draw(ast)\n\nassert_asts(asts,[{'type': 'add_fact',\n  'val': Relation(name='R', terms=['hello', 6], agg=None),\n  'id': 0,\n  'children': []},\n {'type': 'add_fact',\n  'val': Relation(name='R', terms=['hello', 6], agg=None),\n  'id': 0,\n  'children': []},\n {'type': 'remove_fact',\n  'val': Relation(name='R', terms=['hello', 6], agg=None),\n  'id': 0,\n  'children': []},\n {'type': 'relation_declaration',\n  'val': RelationDefinition(name='R', scheme=[str,str,int,int]),\n  'id': 0,\n  'children': []},\n {'type': 'query',\n  'val': Relation(name='R', terms=['hello', FreeVar(name='Y')], agg=None),\n  'id': 0,\n  'children': []},\n {'type': 'rule',\n  'id': 0,\n  'children': [{'type': 'rule_head',\n    'val': Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y')], agg=None),\n    'id': 1},\n   {'type': 'rule_body_relation_list',\n    'id': 9,\n    'children': [{'type': 'relation',\n      'val': Relation(name='S', terms=[FreeVar(name='X'), FreeVar(name='Y')], agg=None),\n      'id': 10},\n     {'type': 'ie_relation',\n      'val': IERelation(name='T', in_terms=[FreeVar(name='X'), FreeVar(name='Y')], out_terms=[FreeVar(name='Y'), FreeVar(name='Z')]),\n      'id': 18}]}]},\n {'type': 'rule',\n  'id': 0,\n  'children': [{'type': 'rule_head',\n    'val': Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y')], agg=[None, 'sum']),\n    'id': 1},\n   {'type': 'rule_body_relation_list',\n    'id': 12,\n    'children': [{'type': 'relation',\n      'val': Relation(name='S', terms=[FreeVar(name='X'), FreeVar(name='Y')], agg=None),\n      'id': 13}]}]},\n {'type': 'rule',\n  'id': 0,\n  'children': [{'type': 'rule_head',\n    'val': Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y'), 'yes'], agg=[None, 'sum', None]),\n    'id': 1},\n   {'type': 'rule_body_relation_list',\n    'id': 14,\n    'children': [{'type': 'relation',\n      'val': Relation(name='S', terms=[FreeVar(name='X'), FreeVar(name='Y')], agg=None),\n      'id': 15}]}]}]\n    )"
  },
  {
    "objectID": "micro_passes.html#verify-referenced-relations-and-functions",
    "href": "micro_passes.html#verify-referenced-relations-and-functions",
    "title": "Micro Passes",
    "section": "verify referenced relations and functions",
    "text": "verify referenced relations and functions\n\ncheck that referenced relations and ie relations:\n\nexist in the symbol table\nare called with the correct arity\nare called with correct constants or vars types\n\n\n\nsource\n\nverify_referenced_relations_and_functions\n\n verify_referenced_relations_and_functions (ast, engine)\n\n\nsess.engine.get_agg_function('sums')\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    CheckReservedRelationNames('spanner_'),\n    check_referenced_paths_exist,\n    dereference_vars,\n    inline_aggregation,\n    relations_to_dataclasses,\n    verify_referenced_relations_and_functions\n    ])\n\n\ndef string_schema(n):\n    return [str]*n\n\nsess.engine.set_var('a',1)\nsess.engine.set_var('b','hello')\nsess.engine.set_relation(RelationDefinition(name='R',scheme=[str,int]))\nsess.engine.set_relation(RelationDefinition(name='F',scheme=[float,Real]))\nsess.engine.set_relation(RelationDefinition(name='S',scheme=[str,int,int]))\nsess.engine.add_fact(Relation(name='R',terms=[Span(\"hello\"),5]))\nsess.engine.set_ie_function(IEFunction(name='T',in_schema=[str,Real],out_schema=[Real,str],func=lambda x,y:(y,x)))\nsess.engine.set_ie_function(IEFunction(name='T2',in_schema=string_schema,out_schema=string_schema,func=lambda x,y:(y,x)))\nsess.engine.set_agg_function(AGGFunction(name='sum',func='sum',in_schema=[int],out_schema=[int]))\nsess.engine.set_agg_function(AGGFunction(name='count',func='count',in_schema=[object],out_schema=[int]))\n\n\nasts = sess.run_query(\"\"\"\nR(\"hello\",6)\nR(\"hello\",$a)\n?R(\"hello\",Y)\nF(1.3,5)\nF(1.3,5.5)\nNewRel(X,Y)&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Z).\nNewRel2(X,Y)&lt;-S(X,Y,3),T2(X,Y)-&gt;(Y,Z).\nNewRel3(X,sum(Y))&lt;-R(X,Y).\nNewRel4(X,count(Y))&lt;-R(X,Y).\nNewRel5(count(X),Y)&lt;-R(X,Y).\n\"\"\")\n\n# for ast in asts:\n#     draw(ast)\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n                x=3\n                Z(\"hello\",x)\n                \"\"\")\nprint(exc_info.value)\nassert \"Adding or removing facts cannot have free variables\" in str(exc_info.value)\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n                Z(\"hello\",4)\n                \"\"\")\nprint(exc_info.value)\nassert \"Relation 'Z' is not defined\" in str(exc_info.value)\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n                R(\"hello\",4,3)\n                \"\"\")\nprint(exc_info.value)\nassert \"Relation 'R' expected schema R(str,int) but got called with R(hello,4,3)\" in str(exc_info.value)\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n                R(4,4)\n                \"\"\")\nprint(exc_info.value)\nassert \"Relation 'R' expected schema R(str,int) but got called with R(4,4)\" in str(exc_info.value)\n\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n                R(\"hello\",$b)\n                \"\"\")\nprint(exc_info.value)\nassert \"Relation 'R' expected schema R(str,int) but got called with R(hello,hello)\" in str(exc_info.value)\n\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n                ?R(4,Y)\n                \"\"\")\nprint(exc_info.value)\nassert \"Relation 'R' expected schema R(str,int) but got called with R(4,Y)\" in str(exc_info.value)\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n                NewRel(X,Y)&lt;-S(X,Y,3),T(X,Y,X)-&gt;(Y,Z).\n                \"\"\")\nprint(exc_info.value)\nassert \"IERelation 'T' input expected schema\" in str(exc_info.value)\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n                NewRel(X,Y)&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,2).\n                \"\"\")\nprint(exc_info.value)\nassert \"IERelation 'T' output expected schema\" in str(exc_info.value)\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n                NewRel(X,Y)&lt;-S(X,Y,3),T3(X,Y)-&gt;(Y,X).\n                \"\"\")\nprint(exc_info.value)\nassert \"ie function 'T3' was not registered\" in str(exc_info.value)\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(f\"\"\"\n            NewRel3(X,sums(Y))&lt;-R(X,Y).\n            \"\"\")\nprint(exc_info.value)\nassert \"agg function 'sums' was not registered\" in str(exc_info.value)\n\n\n# assert  [serialize_tree(ast) for ast in asts] \n# [serialize_tree(ast) for ast in asts]\n\nAdding or removing facts cannot have free variables, found in Z(hello,x)\nRelation 'Z' is not defined\nRelation 'R' expected schema R(str,int) but got called with R(hello,4,3)\nRelation 'R' expected schema R(str,int) but got called with R(4,4)\nRelation 'R' expected schema R(str,int) but got called with R(hello,hello)\nRelation 'R' expected schema R(str,int) but got called with R(4,Y)\nIERelation 'T' input expected schema [str,Real] but got called with [X,Y,X]\nIERelation 'T' output expected schema [Real,str] but got called with [Y,2]\nie function 'T3' was not registered, registered functions are ['T', 'T2']\nagg function 'sums' was not registered, registered functions are ['sum', 'count']"
  },
  {
    "objectID": "micro_passes.html#cast-rules-to-data-classes",
    "href": "micro_passes.html#cast-rules-to-data-classes",
    "title": "Micro Passes",
    "section": "cast rules to data classes",
    "text": "cast rules to data classes\n\nsource\n\nrules_to_dataclasses\n\n rules_to_dataclasses (ast, engine)\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    CheckReservedRelationNames('spanner_'),\n    check_referenced_paths_exist,\n    dereference_vars,\n    inline_aggregation,\n    relations_to_dataclasses,\n    # verify_referenced_relations_and_functions,\n    rules_to_dataclasses\n    ])\n\nasts = sess.run_query(\"\"\"\nR(X,Y,Z)&lt;-S(X,Y),T(X,Y).\nR(X,Y,Z)&lt;-S(X,Y),T(X,Y)-&gt;(Y,Z).\n\"\"\")\nfor ast in asts:\n    draw(ast)\n\nassert  [serialize_tree(ast) for ast in asts] == [{'type': 'rule',\n  'val': Rule(head=Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y'), FreeVar(name='Z')]),\n               body=[Relation(name='S', terms=[FreeVar(name='X'), FreeVar(name='Y')]), \n                     Relation(name='T', terms=[FreeVar(name='X'), FreeVar(name='Y')])]),\n  'id': 0,\n  'children': []},\n {'type': 'rule',\n  'val': Rule(head=Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y'), FreeVar(name='Z')]), \n              body=[Relation(name='S', terms=[FreeVar(name='X'), FreeVar(name='Y')]), \n                    IERelation(name='T', in_terms=[FreeVar(name='X'), FreeVar(name='Y')], out_terms=[FreeVar(name='Y'), FreeVar(name='Z')])]),\n  'id': 0,\n  'children': []}]"
  },
  {
    "objectID": "micro_passes.html#rule-safety",
    "href": "micro_passes.html#rule-safety",
    "title": "Micro Passes",
    "section": "Rule safety",
    "text": "Rule safety\n/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/fastcore/docscrape.py:230: UserWarning: potentially wrong underline length... \nChecks that the Spannerlog Rule is safe \n--- in \nChecks that the Spannerlog Rule is safe\n---...\n  else: warn(msg)\n\nsource\n\nis_rule_safe\n\n is_rule_safe (rule:spannerlib.data_types.Rule)"
  },
  {
    "objectID": "micro_passes.html#checks-that-the-spannerlog-rule-is-safe",
    "href": "micro_passes.html#checks-that-the-spannerlog-rule-is-safe",
    "title": "Micro Passes",
    "section": "*Checks that the Spannerlog Rule is safe",
    "text": "*Checks that the Spannerlog Rule is safe\nIn spannerlog, rule safety is a semantic property that ensures that IE relation’s inputs are limited in the values they can be assigned to by other relations in the rule body. This could include outputs of other IE relations.\nWe call a free variable in a rule body “bound” if it exists in the output of any safe relation in the rule body. For normal relations, they only have output terms, so all their free variables are considered bound.\nWe call a relation in a rule’s body safe if all its input free variables are bound. For normal relations, they don’t have input relations, so they are always considered safe.\nWe call a rule safe if all of its body relations are safe.\nThis basically means that we need to make sure there is at least one order of IE relation evaluation, in which each IE relation input variables is bound by the normal relations and the output relation of the previous IE relations.\nExamples: * rel2(X,Y) &lt;- rel1(X,Z), ie1(X)-&gt;(Y). is a safe rule as the only input free variable, X, exists in the output of the safe relation rel1(X, Z).\n* rel2(Y) &lt;- ie1(Z)-&gt;(Y). is not safe as the input free variable Z does not exist in the output of any safe relation. * rel2(Z,W) &lt;- rel1(X,Y),ie1(Z,Y)-&gt;(W),ie2(W,Y)-&gt;Z. is not safe as both ie functions require each other’s output as input, creating a circular dependency. —*\n\nsource\n\ncheck_rule_safety\n\n check_rule_safety (ast, engine)\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    relations_to_dataclasses,\n    rules_to_dataclasses,\n    check_rule_safety,\n    ])\n\n\n# safe rules\nasts = sess.run_query(\"\"\"\nS(X,Y)&lt;-R(X,Y).\nrel2(X,Y) &lt;- rel1(X,Z), ie1(X)-&gt;(Y).\nrel2(X,Y) &lt;- rel1(X,Z), ie1(X)-&gt;(Y), ie2(Y)-&gt;(Z).\nrel2(X,Y) &lt;- rel1(X), ie1(X)-&gt;(Y), ie2(Y)-&gt;(Z).\n\"\"\")\nt = serialize_tree(asts[0])\n\n# change types of R to make it illegal\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n    rel2(Y) &lt;- ie1(Z)-&gt;(Y).\n    \"\"\")\nprint(exc_info.value)\nassert \"is not safe\" in str(exc_info.value)\n\n\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n    rel2(Z,W) &lt;- rel1(X,Y),ie1(Z,Y)-&gt;(W),ie2(W,Y)-&gt;(Z).\n    \"\"\")\nprint(exc_info.value)\nassert \"is not safe\" in str(exc_info.value)\n\nRule 'rel2(Y) &lt;- ie1(Z) -&gt; (Y).' is not safe:\nthe following free vars where bound by normal relations: set()\nthe following ie relations where safe: set()\nleading to the following free vars being bound: set()\nHowever the following ie relations could not be bound: ['ie1(Z) -&gt; (Y)']\n\nRule 'rel2(Z,W) &lt;- rel1(X,Y),ie1(Z,Y) -&gt; (W),ie2(W,Y) -&gt; (Z).' is not safe:\nthe following free vars where bound by normal relations: {'Y', 'X'}\nthe following ie relations where safe: set()\nleading to the following free vars being bound: {'Y', 'X'}\nHowever the following ie relations could not be bound: ['ie1(Z,Y) -&gt; (W)', 'ie2(W,Y) -&gt; (Z)']"
  },
  {
    "objectID": "micro_passes.html#consistent-free-var-types",
    "href": "micro_passes.html#consistent-free-var-types",
    "title": "Micro Passes",
    "section": "Consistent Free Var types",
    "text": "Consistent Free Var types\n\nsource\n\nconsistent_free_var_types_in_rule\n\n consistent_free_var_types_in_rule (ast, engine)\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    CheckReservedRelationNames('spanner_'),\n    check_referenced_paths_exist,\n    dereference_vars,\n    inline_aggregation,\n    relations_to_dataclasses,\n    verify_referenced_relations_and_functions,\n    rules_to_dataclasses,\n    check_rule_safety,\n    consistent_free_var_types_in_rule,\n    ])\n\n\nsess.engine.set_var('a',1)\nsess.engine.set_var('b','hello')\nsess.engine.set_relation(RelationDefinition(name='R',scheme=[str,int]))\nsess.engine.set_relation(RelationDefinition(name='S',scheme=[str,int,int]))\nsess.engine.set_relation(RelationDefinition(name='S2',scheme=[str,float,float]))\nsess.engine.set_relation(RelationDefinition(name='NewRel',scheme=[str,int]))\nsess.engine.set_ie_function(IEFunction(name='T',in_schema=[str,Real],out_schema=[Real,str],func=lambda x,y:(y,x)))\nsess.engine.set_agg_function(AGGFunction(name='sum',func='sum',in_schema=[int],out_schema=[int]))\nsess.engine.set_agg_function(AGGFunction(name='count',func='count',in_schema=[object],out_schema=[int]))\n\n# legal query\n# with checkLogs():\nasts = sess.run_query(\"\"\"\nNewRel(X,Y)&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Z),R(Z,Y).\nNewRel(X,sum(Y))&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Z),R(Z,Y).\nNewRel2(X,sum(Y),\"yes\")&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Z),R(Z,Y).\nS3(X,Y,'yes')&lt;-R(X,Y).\n\"\"\")\nt = serialize_tree(asts[0])\n\nassert sess.engine.get_relation('NewRel').scheme == [str,int]\nassert sess.engine.get_relation('NewRel2').scheme == [str,int,str]\nassert sess.engine.get_relation('S3').scheme == [str,int,str]\n\n# change types of R to make it illegal\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n    NewRel(X,Y)&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Z),R(Y,Z).\n    \"\"\")\nexc_msg = str(exc_info.value).replace('\\n',' ')\nprint(exc_msg)\nassert \"FreeVar Y is used with type str but was previously defined with type int in relation S\" in exc_msg\n\n# change types of R to make it illegal\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n    NewRel(X,Y)&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Y).\n    \"\"\")\nexc_msg = str(exc_info.value).replace('\\n',' ')\nprint(exc_msg)\nassert \"Y is used with type str but was previously defined with type int in relation S\" in exc_msg\n\n# free var in head, bound by body\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n    NewRel(X,Y,W)&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Z),R(Z,Y).\n    \"\"\")\nexc_msg = str(exc_info.value).replace('\\n',' ')\nprint(exc_msg)\nassert \"FreeVar W is used in the head but was not defined in the body\" in exc_msg\n\n# head free var type mismatch\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n    NewRel(Y,X)&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Z),R(Z,Y).\n    \"\"\")\nexc_msg = str(exc_info.value).replace('\\n',' ')\nprint(exc_msg)\nassert \"expected schema NewRel(str,int) from a previously defined rule to NewRel but got NewRel(int,str)\" in exc_msg\nt\n\n# aggregation got wrong input type\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n    NewRel(sum(X),Y)&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Z),R(Z,Y).\n    \"\"\")\nexc_msg = str(exc_info.value).replace('\\n',' ')\nprint(exc_msg)\nassert \"FreeVar X is aggregated with sum which expects input type int but got str\" in exc_msg\n\n# aggregation caused conflict with previous type\nwith pytest.raises(ValueError) as exc_info:\n    asts = sess.run_query(\"\"\"\n    NewRel(count(X),Y)&lt;-S(X,Y,3),T(X,Y)-&gt;(Y,Z),R(Z,Y).\n    \"\"\")\nexc_msg = str(exc_info.value).replace('\\n',' ')\nprint(exc_msg)\nassert \"expected schema NewRel(str,int) from a previously defined rule to NewRel but got NewRel(int,int)\" in exc_msg\n\nIn rule NewRel(X,Y) &lt;- S(X,Y,3),T(X,Y) -&gt; (Y,Z),R(Y,Z). in relation R FreeVar Y is used with type str but was previously defined with type int in relation S\nIn rule NewRel(X,Y) &lt;- S(X,Y,3),T(X,Y) -&gt; (Y,Y). in ie output relation T FreeVar Y is used with type str but was previously defined with type int in relation S\nIn rule NewRel(X,Y,W) &lt;- S(X,Y,3),T(X,Y) -&gt; (Y,Z),R(Z,Y). FreeVar W is used in the head but was not defined in the body\nIn rule NewRel(Y,X) &lt;- S(X,Y,3),T(X,Y) -&gt; (Y,Z),R(Z,Y). expected schema NewRel(str,int) from a previously defined rule to NewRel but got NewRel(int,str)\nIn rule NewRel(sum(X),Y) &lt;- S(X,Y,3),T(X,Y) -&gt; (Y,Z),R(Z,Y). in head clause NewRel FreeVar X is aggregated with sum which expects input type int but got str\nIn rule NewRel(count(X),Y) &lt;- S(X,Y,3),T(X,Y) -&gt; (Y,Z),R(Z,Y). expected schema NewRel(str,int) from a previously defined rule to NewRel but got NewRel(int,int)"
  },
  {
    "objectID": "micro_passes.html#preprocess-assignments",
    "href": "micro_passes.html#preprocess-assignments",
    "title": "Micro Passes",
    "section": "Preprocess assignments",
    "text": "Preprocess assignments\n\nsource\n\nassignments_to_name_val_tuple\n\n assignments_to_name_val_tuple (ast, engine)\n\n\nsess = DummySession(passes=[\n    convert_primitive_values_to_objects,\n    CheckReservedRelationNames('spanner_'),\n    # check_referenced_paths_exist,\n    # dereference_vars,\n    inline_aggregation,\n    relations_to_dataclasses,\n    # verify_referenced_relations_and_functions,\n    rules_to_dataclasses,\n    check_rule_safety,\n    # consistent_free_var_types_in_rule,\n    assignments_to_name_val_tuple\n    ])\n\n\nfile = Path(\"file.txt\")\nfile.write_text(\"hello file\")\n\n# safe rules\nasts = sess.run_query(\"\"\"\nx=3\ny=\"hello\"\nz=\"hello \\\nworld\"\nf=read(\"file.txt\")\n\"\"\")\n\nfor ast in asts:\n    draw(ast)\nassert [serialize_tree(ast) for ast in asts] == [{'type': 'assignment', 'val': ('x', 3), 'id': 0, 'children': []},\n {'type': 'assignment', 'val': ('y', 'hello'), 'id': 0, 'children': []},\n {'type': 'assignment', 'val': ('z', 'hello world'), 'id': 0, 'children': []},\n {'type': 'read_assignment',\n  'val': ('f', 'file.txt'),\n  'id': 0,\n  'children': []}]"
  },
  {
    "objectID": "primitive_data_types.html",
    "href": "primitive_data_types.html",
    "title": "Primitive datatypes",
    "section": "",
    "text": "source\n\nRule\n\n Rule (head:__main__.Relation,\n       body:List[Union[__main__.Relation,__main__.IERelation]])\n\n*Usage docs: https://docs.pydantic.dev/2.10/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nIERelation\n\n IERelation (name:str, in_terms:List, out_terms:List)\n\n*Usage docs: https://docs.pydantic.dev/2.10/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nAGGFunction\n\n AGGFunction (name:str, func:Union[Callable,str], in_schema:List,\n              out_schema:List)\n\n*Usage docs: https://docs.pydantic.dev/2.10/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nIEFunction\n\n IEFunction (name:str, func:Callable, in_schema:Union[List,Callable],\n             out_schema:Union[List,Callable])\n\n*Usage docs: https://docs.pydantic.dev/2.10/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nRelation\n\n Relation (name:str, terms:List, agg:Optional[List[Optional[str]]]=None)\n\n*Usage docs: https://docs.pydantic.dev/2.10/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nRelationDefinition\n\n RelationDefinition (name:str, scheme:List)\n\n*Usage docs: https://docs.pydantic.dev/2.10/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nFreeVar\n\n FreeVar (name:str)\n\n*Usage docs: https://docs.pydantic.dev/2.10/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nVar\n\n Var (name:str)\n\n*Usage docs: https://docs.pydantic.dev/2.10/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\npretty\n\n pretty (obj)\n\npretty printing dataclasses for user messages, making them look like spannerlog code instead of python code\n\nrule = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y'), FreeVar(name='Z')]),\n    body=[\n        Relation(name='S', terms=[FreeVar(name='X'), Span(doc='hello world',start=0,end=4,name='my_str')]),\n        IERelation(name='T', in_terms=[FreeVar(name='X'), 1], out_terms=[FreeVar(name='Y'), FreeVar(name='Z')])\n    ])\nassert pretty(rule) == 'R(X,Y,Z) &lt;- S(X,[@my_str,0,4) \"hell\"),T(X,1) -&gt; (Y,Z).',pretty(rule)\n\n\nschema = RelationDefinition(name='R', scheme=[int, str, Span])\nassert pretty(schema) == 'R(int,str,Span)'\nie_func_schema = IEFunction(name='f', in_schema=[int, str], out_schema=[str, Span],func=lambda x,y: (y,Span(1,2)))\nassert pretty(ie_func_schema) == 'f(int,str) -&gt; (str,Span)'\n\n\nagg_head = Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y'), FreeVar(name='Z')],agg=[None,'sum',None])\nassert pretty(agg_head) == 'R(X,sum(Y),Z)'\n\n\n#TODO from here, check pretty printing of types\npretty(RelationDefinition(name='R', scheme=[int, str, Span]))\n\n'R(int,str,Span)'\n\n\n\nsource\n\n\nisInt\n\n isInt (s)\n\n\nsource\n\n\nisFloat\n\n isFloat (s)\n\n\nassert _infer_relation_schema([1, 2, 3]) == [ int,int,int]\nassert _infer_relation_schema([1, 'a']) == [ int,str]\nassert _infer_relation_schema([1, 2, Span(\"text\")]) == [int, int, Span]\nassert _infer_relation_schema([1,\"he\",\"\"\"prompt here \n{}\"\"\"]) == [int,str,str]"
  },
  {
    "objectID": "query_optimizations.html",
    "href": "query_optimizations.html",
    "title": "Optimizations Passes",
    "section": "",
    "text": "Note\n\n\n\nNote that these passes depend on the structure of the term graph!\n\n\n\n# TODO here we will implement query optimizations as graph rewriting rules\n\n# Then we will add optimization capabilities to the engine/session probably engine with an interface to session\n\n# we will have 2 types of optimizations:\n# global optimizations\n    # applied to the term graph after every new rule is added\n    # need to make sure that they dont break under adding/removing rules\n# query specific optimizations\n    # only added to the query graph\n    # dont have the restriction of global optimizations\n\n# optimizations will look like the form of graph rewriting rules in the micro passes, meaning they are of the form:\n    # F:(g,engine)-&gt;g\n\n\n# TODO\n# to get started, implement the following 2 optimization passes and make sure that all tests in the package pass if you apply them by default \n# when creating a new session object \n\n# prune uncessary projects and renames\n    # whenever there is a project/rename node whose input in a node with the same schema, merge them\n# remove useless body clauses\n    # for example `A(X) &lt;- B(X), C(Y)` can be simplified to `A(X) &lt;- B(X)`  this is unless they want to not derive anything if C is empty\n    # note that this is not like A(X) &lt;- B(X), C(1), in which case we will derive if 1 is in C and not else. This could be a usefull data dependant switch case pattern we might want to use\n    # so whenever you have a non constant body clause that has a variable that is not used by any other body or head clause, remove it"
  },
  {
    "objectID": "building_your_optimization.html",
    "href": "building_your_optimization.html",
    "title": "Building your own optimization",
    "section": "",
    "text": "# TODO opt redo this notebook after implementation"
  },
  {
    "objectID": "building_your_optimization.html#adding-optimization-passes-to-the-pass-stack",
    "href": "building_your_optimization.html#adding-optimization-passes-to-the-pass-stack",
    "title": "Building your own optimization",
    "section": "Adding Optimization Passes to the Pass Stack",
    "text": "Adding Optimization Passes to the Pass Stack"
  },
  {
    "objectID": "building_your_optimization.html#general-passes",
    "href": "building_your_optimization.html#general-passes",
    "title": "Building your own optimization",
    "section": "General Passes",
    "text": "General Passes\nBefore reading this section, we will briefly explain how passes work. There are five kinds of passes: 1. AST transformation passes - These passes convert the input program into AST. 2. semantic checks passes - These passes check the corectness of the program (i.e. one of the passes asserts that all the relations used in the program were registerred before). 3. AST execution passes - These passes traverse the AST and covert it to a parse graph. In addition they register new relations and handle variables assingments. 4. term graph passes - These passes adds rules into the term graph. 5. execution pass - This pass traverses the parse graph and finds queries. Then it computes them using the term graph."
  },
  {
    "objectID": "building_your_optimization.html#optimization-passes",
    "href": "building_your_optimization.html#optimization-passes",
    "title": "Building your own optimization",
    "section": "Optimization Passes",
    "text": "Optimization Passes\nThere are two kinds of optimization passes: 1. The first one, manipulates rules before they are added to the term graph. 2. The second one, manipulates the structure of the term graph.\nnote: It’s also possible to optimize the execution function/pass (but we won’t discuss it in this tutorial).\nIn this section, we will implement two simple optimization passes, one of each kind."
  },
  {
    "objectID": "building_your_optimization.html#rule-manipulation-optimization",
    "href": "building_your_optimization.html#rule-manipulation-optimization",
    "title": "Building your own optimization",
    "section": "Rule-Manipulation Optimization",
    "text": "Rule-Manipulation Optimization\nOptimizations of this kind traverse the parse_graph and find rules that weren’t added to the term graph. Then, they update each rule - by modifying its body relations list.\nHere are some examples of possible optimization passes of this kind: 1. An optimization that removes duplicated relations from a rule. i.e., the rule A(X) &lt;- B(X), C(X), B(X) contains the relation B(X) twice. The optimization will transform the rule into A(X) &lt;- B(X), C(X).\n\nAn optimization that removes useless relations from a rule. i.e. the rule A(X) &lt;- B(X), C(Y) contains the useless relation C(Y). The optimization will transform the rule into A(X) &lt;- B(X).\n\nBelow is an implementation of the latter example:\n\nOptimization Example: Remove Useless Relations\nBefore jumping into the actual implementation, we will implement it in psuedo code:\n1. a. Add the free variables inside the rule head into a relevant_free_variables set.\n   b. Mark all relations as useless, except those with no free variables (they are always relevant).\n   \n2. Find all relations which contain at least one free variable inside the relevant_free_variables set.\n\n3. Unmark these relations (since they are relevant).\n\n4. Add all free variables of the unmarked relations into the relevant_free_variables set.\n\n5. Repeat steps 2, 3 and 4 until the set of the marked relations converge.\nnote that this is a fixed_point algorithm.\n\nfrom spannerlib.general_utils import fixed_point\nfrom spannerlib.general_utils import get_output_free_var_names\nfrom spannerlib.general_utils import get_input_free_var_names\n\nInstallation NLP failed\ncargo or rustup are not installed in $PATH. please install rust: https://rustup.rs/\n\n\nget_output_free_var_names documentation get_input_free_var_names documentation\n\n# first, lets implement the logic that removes useless relations from a rule\ndef remove_useless_relations(rule):\n        \"\"\"\n        Finds redundant relations and removes them from the rule.\n        \n        @param rule: a rule.\n        \"\"\"\n        # step 1.a. Add the free variables inside the rule head into a relevant_free_variables set.\n        relevant_free_vars = set(rule.head_relation.get_term_list())  \n\n        # step 1.b. Mark all relations as useless, except those with no free variables (they are always relevant).\n        initial_useless_relations_and_types = [(rel, rel_type) for rel, rel_type in zip(rule.body_relation_list, rule.body_relation_type_list)\n                                               if len(get_output_free_var_names(rel)) != 0]\n        # implement steps 2, 3 and 4\n        def step_function(current_useless_relations_and_types):\n            \"\"\"\n            Used by fixed pont algorithm.\n\n            @param current_useless_relations_and_types: current useless relations and their types\n            @return: useless relations after considering the new relevant free vars.\n            \"\"\"\n\n            next_useless_relations_and_types = []\n            \n            # step 2 - Find all relations that has at least on free variable inside the relevant_free_variables set.\n            for relation, rel_type in current_useless_relations_and_types:\n                term_list = get_output_free_var_names(relation)\n                if len(relevant_free_vars.intersection(term_list)) == 0:\n                    next_useless_relations_and_types.append((relation, rel_type))\n                else:\n                    # step 3 - Unmark relation. The relations isn't added to the useless list, and thus it's unmarked.\n                    # step 4 - Add all the free variables of the unmarked relation into the relevant_free_variables set.\n                    relevant_free_vars.update(term_list)\n                    relevant_free_vars.update(get_input_free_var_names(relation))\n\n            return next_useless_relations_and_types\n\n        # step 5 - fixed ponint. note that the distance function returns zero if and only if len(x) equals len(y).\n        useless_relations_and_types = fixed_point(start=initial_useless_relations_and_types, step=step_function, distance=lambda x, y: int(len(x) != len(y)))\n        \n        # this part filters the useless relation from the rule\n        relevant_relations_and_types = set(zip(rule.body_relation_list, rule.body_relation_type_list)).difference(useless_relations_and_types)\n        new_body_relation_list, new_body_relation_type_list = zip(*relevant_relations_and_types)\n        rule.body_relation_list = list(new_body_relation_list)\n        rule.body_relation_type_list = list(new_body_relation_type_list)\n\n\nfrom spannerlib.graphs import GraphBase, EvalState, STATE, TYPE, VALUE\nfrom spannerlib.passes_utils import ParseNodeType\nfrom spannerlib.lark_passes import GenericPass  # base class of all the passes\n    \n\n# finally, the implementation of the optimization pass\nclass RemoveUselessRelationsFromRule(GenericPass):\n    \"\"\"\n    This pass removes duplicated relations from a rule.\n    For example, the rule A(X) &lt;- B(X), C(Y) contains a redundant relation (C(Y)).\n    After this pass the rule will be A(X) &lt;- B(X).\n\n    @note: in the rule A(X) &lt;- B(X, Y), C(Y); C(Y) is not redundant!\n    \"\"\"\n    \n    def __init__(self, parse_graph: GraphBase, **kwargs):\n        self.parse_graph = parse_graph\n            \n    def run_pass(self, **kwargs):\n        # get the new rules in the parse graph\n        rules = self.parse_graph.get_all_nodes_with_attributes(type=ParseNodeType.RULE, state=EvalState.NOT_COMPUTED)\n        for rule_node_id in rules:\n            rule_node = self.parse_graph[rule_node_id]\n            rule = rule_node[VALUE]\n            remove_useless_relations(rule)\n\nModifying the pass stack looks like this:\n\ndef print_pass_stack(pass_stack):\n    \"\"\"prints pass stack in a nice format\"\"\"\n    \n    for pass_ in pass_stack:\n        print(\"\\t\" + pass_.__name__)\n        \nmagic_session = Session()  # reset the magic session\n\noriginal_pass_stack = magic_session.get_pass_stack()  # save the original pass stack\n\nnew_pass_stack = original_pass_stack.copy()\nterm_graph_pass = new_pass_stack.pop()  # remove last pass (this pass adds rules to term graph)\nnew_pass_stack.extend([RemoveUselessRelationsFromRule, term_graph_pass])\n\nmagic_session.set_pass_stack(new_pass_stack)\n\nprint(f\"Pass stack before:\")\nprint_pass_stack(original_pass_stack)\n\nprint(\"\\nPass stack after:\")\nprint_pass_stack(magic_session.get_pass_stack())\n\nPass stack before:\n    RemoveTokens\n    FixStrings\n    CheckReservedRelationNames\n    ConvertSpanNodesToSpanInstances\n    ConvertStatementsToStructuredNodes\n    CheckDefinedReferencedVariables\n    CheckReferencedRelationsExistenceAndArity\n    CheckReferencedIERelationsExistenceAndArity\n    CheckRuleSafety\n    TypeCheckAssignments\n    TypeCheckRelations\n    SaveDeclaredRelationsSchemas\n    ResolveVariablesReferences\n    ExecuteAssignments\n    AddStatementsToNetxParseGraph\n    AddRulesToTermGraph\n\nPass stack after:\n    RemoveTokens\n    FixStrings\n    CheckReservedRelationNames\n    ConvertSpanNodesToSpanInstances\n    ConvertStatementsToStructuredNodes\n    CheckDefinedReferencedVariables\n    CheckReferencedRelationsExistenceAndArity\n    CheckReferencedIERelationsExistenceAndArity\n    CheckRuleSafety\n    TypeCheckAssignments\n    TypeCheckRelations\n    SaveDeclaredRelationsSchemas\n    ResolveVariablesReferences\n    ExecuteAssignments\n    AddStatementsToNetxParseGraph\n    RemoveUselessRelationsFromRule\n    AddRulesToTermGraph\n\n\nNow let’s look at the effect of this pass on the parse graph:\n\ncommands = \"\"\"\nnew Good(int)\nnew Bad(int)\n\nExample(X) &lt;- Good(X), Bad(Y)\n\"\"\"\n\ndef run_commands_and_print_parse_graph(session):\n    session.run_commands(commands)\n    print(session._parse_graph)\n    \n\nprint(\"Parse graph of unmodified pass stack:\\n\")\nrun_commands_and_print_parse_graph(Session()) \n\nprint(\"\\nParse graph after adding optimization pass:\\n\")\nrun_commands_and_print_parse_graph(magic_session)\n\nParse graph of unmodified pass stack:\n\n(__spannerlog_root) (computed) root\n    (0) (computed) relation_declaration: Good(int)\n    (1) (computed) relation_declaration: Bad(int)\n    (2) (computed) rule: Example(X) &lt;- Good(X), Bad(Y)\n\n\nParse graph after adding optimization pass:\n\n(__spannerlog_root) (computed) root\n    (0) (computed) relation_declaration: Good(int)\n    (1) (computed) relation_declaration: Bad(int)\n    (2) (computed) rule: Example(X) &lt;- Good(X)\n\n\n\nNotice the difference in the rule node!"
  },
  {
    "objectID": "building_your_optimization.html#term-graph-structure-optimization",
    "href": "building_your_optimization.html#term-graph-structure-optimization",
    "title": "Building your own optimization",
    "section": "Term Graph Structure Optimization",
    "text": "Term Graph Structure Optimization\nOptimizations of this kind traverse the term_graph and modify its structure."
  },
  {
    "objectID": "building_your_optimization.html#term-graph-structure",
    "href": "building_your_optimization.html#term-graph-structure",
    "title": "Building your own optimization",
    "section": "Term Graph Structure",
    "text": "Term Graph Structure\nBefore reading on, it is important to understand how the TermGraph looks like in order to understand the terminology used - there is detailed documentation inside the class docstring."
  },
  {
    "objectID": "building_your_optimization.html#structure-optimization",
    "href": "building_your_optimization.html#structure-optimization",
    "title": "Building your own optimization",
    "section": "Structure Optimization",
    "text": "Structure Optimization\nHere are some examples of possible optimization passes of this kind: 1. An optimization that removes join nodes which have only one child relation. Note: this optimization already exists so there is no need to implement it.\n\nAn optimization that removes project nodes whose input is a single-column relation.\n\nHere’s the implementation of the second example:"
  },
  {
    "objectID": "building_your_optimization.html#optimization-example-remove-redundant-project-nodes",
    "href": "building_your_optimization.html#optimization-example-remove-redundant-project-nodes",
    "title": "Building your own optimization",
    "section": "Optimization Example: Remove Redundant Project Nodes",
    "text": "Optimization Example: Remove Redundant Project Nodes\nThe following optimization will traverse the term graph and find all project nodes that has input relation with arity of one. In this case, the project node is redundant and therefore, we remove it from the term graph.\nBefore jumping into the actual implementation, we will implement it in a psuedo code:\n1. Find all project nodes and their union nodes parents (inside the term graph).\n\n2. For each project node\n\n    2.1. Check if the arity of the project node's input relation is one, using the following steps:\n        a. get project's node child - we will denote it as child_node.\n        b. if type of child_node is GET_REL or RULE_REL or CALC node child, return true if arity of the relation stored in child_node is one.\n        c. if type of child_node is SELECT, return true if there is only one free variable in the relation stored in the child of the child_node. \n        d. if type of child_node is project:\n              (i). get input relaations from all of it's children nodes\n              (ii). return true if the arity of the join of all the input relations is one.\n              \n    2.2 if has arity of one, remove the node from the graph by connecting it's child to it's parent.\n\n# helper function\ndef is_relation_has_one_free_var(relation) -&gt; bool:\n    \"\"\"\n    Check whether relation is only one free variable.\n\n    @param relation_: a relation or an ie_relation.\n    \"\"\"\n\n    return len(relation.get_term_list()) == 1\n\n\nfrom spannerlib.graphs import *\n# this function implements step 2 in the pseudo code\ndef is_input_relation_of_node_has_arity_of_one(term_graph: TermGraphBase, node_id) -&gt; bool:\n    \"\"\"\n    @param node_id: id of the node.\n    @note: we expect id of project/join node.\n    @return: the arity of the relation that the node gets during the execution.\n    \"\"\"\n\n    # staep 2.1.a: note that this methods suppose to work for both project nodes and join nodes.\n    # project nodes always have one child while join nodes always have more than one child.\n    # for that reason, we traverse all the children of the node.\n    node_ids = term_graph.get_children(node_id)\n    \n    # used to compute arity of final relation\n    free_vars: Set[str] = set()\n\n    for node_id in node_ids:\n        node_attrs = term_graph[node_id]\n        node_type = node_attrs[TYPE]\n        \n        # step 2.1.b\n        if node_type in (TermNodeType.GET_REL, TermNodeType.RULE_REL, TermNodeType.CALC):\n            relation = node_attrs[VALUE]\n            # if relation has more than one free var we can't prune the project\n            if not is_relation_has_one_free_var(relation):\n                return False\n\n            free_vars |= set(relation.get_term_list())\n            \n        # step 2.1.c\n        elif node_type is TermNodeType.SELECT:\n            relation_child_id = next(iter(term_graph.get_children(node_id)))\n            relation = term_graph[relation_child_id][VALUE]\n            if not is_relation_has_one_free_var(relation):\n                return False\n\n            relation_free_vars = [var for var, var_type in zip(relation.get_term_list(), relation.get_type_list()) if var_type is DataTypes.free_var_name]\n            free_vars |= set(relation_free_vars)\n        \n        # step 2.1.d\n        elif node_type is TermNodeType.JOIN:\n            # the input of project node is the same as the input of the join node\n            return is_input_relation_of_node_has_arity_of_one(term_graph, node_id)\n\n    return len(free_vars) == 1\n\n\n# finally, lets implement the optimization pass class\nclass PruneUnnecessaryProjectNodes(GenericPass):\n    \"\"\"\n    This class prunes project nodes that gets a relation with one column (therefore, the project is redundant).\n\n    For example, the rule A(X) &lt;- B(X) will yield the following term graph:\n\n        rule_rel node (of A)\n            union node\n                project node (on X)\n                   get_rel node (get B)\n\n        since we project a relation with one column, after this pass the term graph will be:\n\n        rule_rel node (of A)\n            union node\n                get_rel node (get B)\n\n    \"\"\"\n\n    def __init__(self, term_graph: TermGraphBase, **kwargs):\n        self.term_graph = term_graph\n\n    def run_pass(self, **kwargs):\n        self.prune_project_nodes()\n        \n    def prune_project_nodes(self) -&gt; None:\n        \"\"\"\n        Prunes the redundant project nodes.\n        \"\"\"\n\n        project_nodes = self.term_graph.get_all_nodes_with_attributes(type=TermNodeType.PROJECT)\n        for project_id in project_nodes:\n            if is_input_relation_of_node_has_arity_of_one(self.term_graph, project_id):\n                # step 2.2\n                self.term_graph.add_edge(self.term_graph.get_parent(project_id), self.term_graph.get_child(project_id))\n                self.term_graph.remove_node(project_id)\n\nThe next step is adding this pass to the pass stack:\n\nmagic_session = Session()  # reset the magic_session\n\nnew_pass_stack = magic_session.get_pass_stack()\nnew_pass_stack.append(PruneUnnecessaryProjectNodes)\nmagic_session.set_pass_stack(new_pass_stack)\n\nprint(\"New pass stack:\")\nprint_pass_stack(magic_session.get_pass_stack())\n\nNew pass stack:\n    RemoveTokens\n    FixStrings\n    CheckReservedRelationNames\n    ConvertSpanNodesToSpanInstances\n    ConvertStatementsToStructuredNodes\n    CheckDefinedReferencedVariables\n    CheckReferencedRelationsExistenceAndArity\n    CheckReferencedIERelationsExistenceAndArity\n    CheckRuleSafety\n    TypeCheckAssignments\n    TypeCheckRelations\n    SaveDeclaredRelationsSchemas\n    ResolveVariablesReferences\n    ExecuteAssignments\n    AddStatementsToNetxParseGraph\n    AddRulesToTermGraph\n    PruneUnnecessaryProjectNodes\n\n\nFinally, lets see how this pass modifies the term graph:\n\ncommands = \"\"\"\nnew B(int)\nA(X) &lt;- B(X)\n\"\"\"\n\ndef run_commands_and_print_term_graph(session):\n    session.run_commands(commands)\n    print(session._term_graph)\n    \n\nprint(\"Term graph of unmodified pass stack:\\n\")\nrun_commands_and_print_term_graph(Session()) \n\nprint(\"\\nTerm graph after adding optimization pass:\\n\")\nrun_commands_and_print_term_graph(magic_session)\n\nTerm graph of unmodified pass stack:\n\n(__spannerlog_root) (not_computed) root\n    (A) (not_computed) rule_rel: A(X)\n        (0) (not_computed) union\n            (1) (not_computed) project: ['X']\n                (2) (not_computed) get_rel: B(X)\n\nDependencyGraph is:\n__spannerlog_root\n    A\n\n\nTerm graph after adding optimization pass:\n\n(__spannerlog_root) (not_computed) root\n    (A) (not_computed) rule_rel: A(X)\n        (0) (not_computed) union\n            (2) (not_computed) get_rel: B(X)\n\nDependencyGraph is:\n__spannerlog_root\n    A\n\n\n\nNotice the changes in the term_graph’s structure!"
  },
  {
    "objectID": "building_your_optimization.html#optimization-example-overlapping-rules",
    "href": "building_your_optimization.html#optimization-example-overlapping-rules",
    "title": "Building your own optimization",
    "section": "Optimization Example: Overlapping Rules",
    "text": "Optimization Example: Overlapping Rules\nAnother optimization example, this time without an implementation. It does the following: 1. Finds overlapping structure of rules. 2. Merges the overlapping structure and adds it to the term graph.\n\nDetailed Example Explanation\nLet’s look at the following example:\n&gt;&gt;&gt; D(X,Y) &lt;- A(X),B(Y),C(X,Y,Z)\n&gt;&gt;&gt; E(X,Y) &lt;- A(X),C(X,Y,Z), F(Z)\n&gt;&gt;&gt; x(X,Y) &lt;- E(X,Y)\n&gt;&gt;&gt; x(X,Y) &lt;- D(X,Y)\nWithout merging terms with overlapping structures, this would naively generate something that abstractly looks like this: \n\n\n\n\n\n\n\n\n\nThe weakness with this approach is that A AND C is computed twice.\nA version of the term graph that takes care to merge terms with overlapping structures would look more like this: \n\n\n\n\n\n\n\n\n\nHere, we realized that A,C is a joint component and that we need only compute it once. This would be the automatic equivalent of a smart programmer, refactoring the query above to look like\n&gt;&gt;&gt; TEMP(X,Y,Z) &lt;- A(X), C(X,Y,Z)\n&gt;&gt;&gt; D(X,Y) &lt;- B(Y),TEMP(X,Y,Z)\n&gt;&gt;&gt; E(X,Y) &lt;- TEMP(X,Y,Z), F(Z)\n&gt;&gt;&gt; x(X,Y) &lt;- E(X,Y)\n&gt;&gt;&gt; x(X,Y) &lt;- D(X,Y)\nHere’s a pseudo implementation of this pass:\n1. get all the registered rules by using term_graph.get_all_rules).\n\n2. find overlapping structure between rules (this step can be implemented in many different ways).\n\n3. create new rule that consists of the overlapping structure.\n\n4. add this new rule to the term graph by using term_graph.add_rule_to_term_graph.\n\n5. updated the previous rule to use the newly created rule.\n\n6. added the rules to the term graph.\n\n7. delete the previous versions of the rule from the term graph by using term_graph.remove_rule.\nFor example, if the following rules were registered: 1. D(X,Y) &lt;- A(X),B(Y),C(X,Y,Z) 2. E(X,Y) &lt;- A(X),C(X,Y,Z), F(Z)\nIn the second step of the algorithm, we will find that both rules share the structure A(X),C(X,Y,Z). In the third step we will create a new relation TEMP(X,Y,Z) &lt;- A(X), C(X,Y,Z), and add it to the term graph. In the fifth step we will modify to original rules in the following way: 1. D(X,Y) &lt;- B(Y),TEMP(X,Y,Z) 2. E(X,Y) &lt;- TEMP(X,Y,Z), F(Z)\nAnd then we will add them to the term graph. The last step will delete the old rules from the term graph."
  },
  {
    "objectID": "term_graphs.html",
    "href": "term_graphs.html",
    "title": "Term Graphs",
    "section": "",
    "text": "Here we have functions for manipulating rules into term graphs\n\nAdding constant terms to term graph\n\nsource\n\n\nadd_product_constants\n\n add_product_constants (g, source, terms)\n\n*adds a product node as a father to source, with the constant terms defined in terms if no constant terms are defined, does nothing returns the product node if it was added or the source not if not\nExample - F(X,3,Y,4)-&gt;(Z): source &lt;- product() &lt;- project([X,_C1,Y,_C2]) get_const({’_C1’:3,’_C2’:4}) &lt;-\nexample 2 - F(3,4)-&gt;(Z): get_const({’_C1’:3,’_C2’:4})*\n\nsource\n\n\nadd_project_uniq_free_vars\n\n add_project_uniq_free_vars (g, source, terms)\n\n*add a rename and a project node to leave only the first appearance of each free var in a relation returns the project node if it was added or the source node if not\nExample - R(X,Y,X,3): source &lt;- rename(names=[(1,X),(2,Y),(3,_F3),(4,_F4)]) &lt;- project([X,Y]) return project*\n\nsource\n\n\nadd_select_col_eq\n\n add_select_col_eq (g, source, terms)\n\n*add nodes to filter source to include equality constraints between columns with the same free var if no such constraints are defined, does nothing returns the top most node if it was added or the source node if not\nExample - R(X,Y,X): source &lt;- select(theta(col1=col3)) return project*\n\nsource\n\n\nadd_select_constants\n\n add_select_constants (g, source, terms)\n\n*adds a select node as a father to source, with the constant terms defined in terms if no constant terms are defined, does nothing returns the select node if it was added or the source node if not\nExample - R(X,3): source &lt;- select(theta(col1=3))*\n\nTests\n\n# constants\ng = nx.DiGraph()\ng.add_node('A',schema=['X','Y'])\n\nadd_select_constants(g,'A',[1,FreeVar(name='X')])\ndraw(g)\n\n# equal free vars\ng = nx.DiGraph()\ng.add_node('A',schema=['X','Y'])\n\nadd_select_col_eq(g,'A',[FreeVar(name='X'),FreeVar(name='Y'),FreeVar(name='X'),FreeVar(name='X')])\ndraw(g)\n\n# project uniq free vars - no need \ng = nx.DiGraph()\ng.add_node('A',schema=['X','Y'])\n\nadd_project_uniq_free_vars(g,'A',[FreeVar(name='X'),FreeVar(name='Y')])\ndraw(g)\n\n# project uniq free vars - constants \ng = nx.DiGraph()\ng.add_node('A',schema=['X','Y','Z'])\n\n\nadd_project_uniq_free_vars(g,'A',[FreeVar(name='X'),FreeVar(name='Y'),2])\ndraw(g)\n\n# project uniq free vars\ng = nx.DiGraph()\ng.add_node('A',schema=['X','Y','Z','T'])\n\nadd_project_uniq_free_vars(g,'A',[FreeVar(name='X'),FreeVar(name='Y'),FreeVar(name='X'),1])\ndraw(g)\n\ng = nx.DiGraph()\ng.add_node('A',schema=['X','Y','Z'])\n\nadd_product_constants(g,'A',[FreeVar(name='X'),1,FreeVar(name='Y'),2])\ndraw(g)\n\ng = nx.DiGraph()\n\nadd_product_constants(g,None,[1,2,3])\ndraw(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding body relations\n\nsource\n\n\nadd_relation\n\n add_relation (g, terms, name=None, source=None,\n               mask_constant_select=None)\n\nadds a relation to the graph WLOG a relation of the form R(X,Y,const) should be of the abstract form get(R)&lt;-rename(0:X,1:Y)&lt;-select(2:const)&lt;-project([X,Y]) if source is not None, source replaces get(R) returns (top most node, bottom most node)\n\nsource\n\n\nmask_terms\n\n mask_terms (terms, mask)\n\n\nsource\n\n\nadd_ie_relation\n\n add_ie_relation (g, rel)\n\n*adds an ie relation to the graph WLOG a relation of the form f(X,Y,c1)-&gt;(Z,X,c2) should be of the abstract form project(X,Y) &lt;- product()&lt;-project([X,Y,_C2])&lt;-ie_map(f)&lt;-select(col_5==c2)&lt;-select(col_0==col_4)&lt;-rename(0:X,1:Y,3:Z)&lt;-project([X,Y,Z]) get_const({’_C2’:c1}) &lt;- returns (top most node, bottom most node)*\n\nTests\n\ng=nx.DiGraph()\nadd_relation(g,name='R',terms=[FreeVar(name='X'),FreeVar(name='Y'),1])\ndraw(g)\n\n\n\n\n\ng=nx.DiGraph()\nadd_relation(g,name='R',terms=[FreeVar(name='X'),FreeVar(name='Y')])\ndraw(g)\n\n\n\n\n\ng=nx.DiGraph()\nadd_relation(g,name='R',terms=[FreeVar(name='X'),FreeVar(name='X')])\ndraw(g)\n\n\n\n\n\ng=nx.DiGraph()\nadd_relation(g,name='R',terms=[FreeVar(name='X'),3,FreeVar(name='Y'),FreeVar(name='X'),FreeVar(name='X'),FreeVar(name='Y'),2])\ndraw(g)\n\n\n\n\n\ng=nx.DiGraph()\nadd_ie_relation(g,IERelation(name='f',in_terms=[FreeVar(name='X'),FreeVar(name='Y'),1],out_terms=[FreeVar(name='Z'),FreeVar(name='X'),2]))\ndraw(g)\n\n\n\n\n\ng=nx.DiGraph()\ntop,bottom = add_ie_relation(g,IERelation(name='f',in_terms=[1,2,3],out_terms=[FreeVar(name='Z'),FreeVar(name='X'),2]))\ndraw(g)\ntop,bottom\n\n\n\n\n(5, None)\n\n\n\n\n\nComputing bounding order of rules\n\nsource\n\n\nget_bounding_order\n\n get_bounding_order (rule:spannerlib.data_types.Rule)\n\nGet an order of evaluation for the body of a rule this is a very naive ordering that can be heavily optimized\n\nr = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y'), FreeVar(name='Z')]),\n    body=[\n        IERelation(name='T2', in_terms=[FreeVar(name='X'), FreeVar(name='Y')], out_terms=[FreeVar(name='W'), FreeVar(name='Z')]),\n        IERelation(name='T', in_terms=[FreeVar(name='X'), 1], out_terms=[FreeVar(name='Y'), FreeVar(name='Z')]),\n        Relation(name='S', terms=[FreeVar(name='X'), Span(doc='hello_world',start=1,end=4)]),\n        Relation(name='S2', terms=[FreeVar(name='X'), FreeVar(name='A'),FreeVar(name='B')]),\n\n    ])\n\nprint(pretty(r))\n\norder = get_bounding_order(r)\nassert [o.name for o in order ] == ['S','S2', 'T', 'T2']\norder\n\nR(X,Y,Z) &lt;- T2(X,Y) -&gt; (W,Z),T(X,1) -&gt; (Y,Z),S(X,[@e4ecd6,1,4) \"ell\"),S2(X,A,B)\n\n\n[Relation(name='S', terms=[FreeVar(name='X'), [@e4ecd6,1,4) \"ell\"], agg=None),\n Relation(name='S2', terms=[FreeVar(name='X'), FreeVar(name='A'), FreeVar(name='B')], agg=None),\n IERelation(name='T', in_terms=[FreeVar(name='X'), 1], out_terms=[FreeVar(name='Y'), FreeVar(name='Z')]),\n IERelation(name='T2', in_terms=[FreeVar(name='X'), FreeVar(name='Y')], out_terms=[FreeVar(name='W'), FreeVar(name='Z')])]\n\n\n\n\nAdding rules\n\nsource\n\n\nrule_to_graph\n\n rule_to_graph (rule:spannerlib.data_types.Rule, rule_id)\n\nconverts a rule to a graph\n\nTest\n\nconst_ie_rule = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X')]),\n    body=[\n        IERelation(name='T', in_terms=[1, 2, 3], out_terms=[FreeVar(name='X')]),\n        \n    ])\nprint(pretty(const_ie_rule))\n\nconst_ie_rule2 = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X')]),\n    body=[\n        Relation(name='S', terms=[FreeVar(name='X')]),\n        IERelation(name='T', in_terms=[1, 2, 3], out_terms=[FreeVar(name='X')]),\n        \n    ])\nprint(pretty(const_ie_rule2))\n\naggregate_rule1 = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'),FreeVar(name='Y'),FreeVar(name='X')],agg=[None,'sum','count']),\n    body=[\n        Relation(name='S', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n        Relation(name='T', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n        \n    ])\n\nprint(pretty(aggregate_rule1))\n\naggregate_rule2 = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'),FreeVar(name='Y')],agg=['sum','count']),\n    body=[\n        Relation(name='S', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n        Relation(name='T', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n        \n    ])\nprint(pretty(aggregate_rule2))\n\nR(X) &lt;- T(1,2,3) -&gt; (X)\nR(X) &lt;- S(X),T(1,2,3) -&gt; (X)\nR(X,sum(Y),count(X)) &lt;- S(X,Y),T(X,Y)\nR(sum(X),count(Y)) &lt;- S(X,Y),T(X,Y)\n\n\n\nconst_in_head_rule = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'),FreeVar(name='Y'),FreeVar(name='X'),\"hello\"]),\n    body=[\n        Relation(name='S', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n        Relation(name='T', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n        \n    ])\nprint(pretty(const_in_head_rule))\n\nR(X,Y,X,hello) &lt;- S(X,Y),T(X,Y)\n\n\n\ng = rule_to_graph(const_in_head_rule,0)\ndraw(g)\n\n\n\n\n\ng = rule_to_graph(aggregate_rule1,0)\ndraw(g)\n\ng = rule_to_graph(aggregate_rule2,0)\ndraw(g)\n\n\n\n\n\n\n\n\n# with checkLogs():\ng = rule_to_graph(const_ie_rule2,0)\ndraw(g)\nserialize_graph(g)\n\n\n\n\n([('S', {'rel': 'S', 'schema': ['col_0'], 'rule_id': {0}}),\n  (0, {'op': 'rename', 'schema': ['X'], 'rule_id': {0}}),\n  (1, {'op': 'project', 'schema': ['X'], 'rule_id': {0}}),\n  (2,\n   {'op': 'get_const',\n    'const_dict': {'_C0': 1, '_C1': 2, '_C2': 3},\n    'schema': ['_C0', '_C1', '_C2'],\n    'rule_id': {0}}),\n  (3,\n   {'op': 'ie_map',\n    'func': 'T',\n    'in_arity': 3,\n    'out_arity': 1,\n    'schema': ['col_0', 'col_1', 'col_2', 'col_3'],\n    'rule_id': {0}}),\n  (4,\n   {'op': 'rename',\n    'schema': ['col_0', 'col_1', 'col_2', 'X'],\n    'rule_id': {0}}),\n  (5, {'op': 'rename', 'schema': ['_F0', '_F1', '_F2', 'X'], 'rule_id': {0}}),\n  (6, {'op': 'project', 'schema': ['X'], 'rule_id': {0}}),\n  (7, {'op': 'join', 'schema': ['X'], 'rule_id': {0}}),\n  (8, {'op': 'project', 'schema': ['X'], 'rel': '_R_0', 'rule_id': {0}}),\n  ('R', {'op': 'union', 'rel': 'R', 'schema': ['col_0'], 'rule_id': {0}})],\n [(0, 'S', {}),\n  (1, 0, {}),\n  (3, 2, {}),\n  (4, 3, {}),\n  (5, 4, {}),\n  (6, 5, {}),\n  (7, 1, {}),\n  (7, 6, {}),\n  (8, 7, {}),\n  ('R', 8, {})])\n\n\n\ng = rule_to_graph(const_ie_rule,0)\ndraw(g)\n\n\n\n\n\ng = rule_to_graph(r,0)\nprint(pretty(r))\ndraw(g)\n\nR(X,Y,Z) &lt;- T2(X,Y) -&gt; (W,Z),T(X,1) -&gt; (Y,Z),S(X,[@e4ecd6,1,4) \"ell\"),S2(X,A,B)\n\n\n\n\n\n\n\n\nCompose term graphs\n\nsource\n\n\ngraph_compose\n\n graph_compose (g1, g2, mapping_dict, debug=False)\n\ncompose two graphs with a mapping dict\n\nTest\n\nr1 = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='S', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n        Relation(name='S2', terms=[FreeVar(name='X'), FreeVar(name='A'),1]),\n    ])\n\nr2 = Rule(\n    head=Relation(name='R', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='S', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\nr3 = Rule(\n    head=Relation(name='R2', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='S3', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n        Relation(name='S2', terms=[FreeVar(name='X'), FreeVar(name='A'),1]),\n    ])\nrules = [r1,r2,r3]\nfor r in rules:\n    print(pretty(r))\nt1,t2,t3 = [rule_to_graph(r,i) for i,r in enumerate(rules)]\n\nR(X,Y) &lt;- S(X,Y),S2(X,A,1)\nR(X,Y) &lt;- S(X,Y)\nR2(X,Y) &lt;- S3(X,Y),S2(X,A,1)\n\n\n\ndraw(t1)\ndraw(t2)\ndraw(t3)\n\n\n\n\n\n\n\n\n\n\n\nm= graph_compose(t1,t2,\n    mapping_dict = {'S':'S','R':'R',0:0}) \ndraw(m)\n\n\n\n\n\n\n\nMerge term graphs\n\nsource\n\n\nmerge_term_graphs\n\n merge_term_graphs (gs, exclude_props=['label'], debug=False)\n\nmerge a list of term graphs into one term graph\n\nsource\n\n\nmerge_term_graphs_pair\n\n merge_term_graphs_pair (g1, g2, exclude_props=['label'], debug=False)\n\nmerge two term graphs into one term graph when talking about term graphs, 2 nodes if their data is identical and all of their children are identical but we would also like to merge rules for the same head, so we will also nodes that have the same ‘rel’ attribute\n\nTests\n\nr1 = Rule(\n    head=Relation(name='A', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='B', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\nr2 = Rule(\n    head=Relation(name='A', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='C', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\nr3 = Rule(\n    head=Relation(name='B', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='D', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\nr4 = Rule(\n    head=Relation(name='B', terms=[FreeVar(name='X'), FreeVar(name='Y')]),\n    body=[\n        Relation(name='A', terms=[FreeVar(name='X'),FreeVar(name='Y')]),\n    ])\n\n\nrules = [r1,r2,r3,r4]\nprint([pretty(r) for r in rules])\nfor r in rules:\n    draw(rule_to_graph(r,0))\n\n['A(X,Y) &lt;- B(X,Y)', 'A(X,Y) &lt;- C(X,Y)', 'B(X,Y) &lt;- D(X,Y)', 'B(X,Y) &lt;- A(X,Y)']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nm = merge_term_graphs([rule_to_graph(r,i) for i,r in enumerate(rules)])\ndraw(m)\n\n\n\n\n\nm = merge_term_graphs([t1,t2])\ndraw(m)\n\n\n\n\n\nm = merge_term_graphs([t1,t3])\ndraw(m)\n\n\n\n\n\nm = merge_term_graphs([t1,t2,t3])\ndraw(m)"
  },
  {
    "objectID": "spans_and_pandas.html",
    "href": "spans_and_pandas.html",
    "title": "Spans",
    "section": "",
    "text": "I followed this guide on how to make extension types for pandas\n\nsource\n\nsmall_hash\n\n small_hash (txt, length=6)\n\n*A function that returns a small hash of a string\nArgs: txt (type): string to hash length (int, optional): length of hash. Defaults to 6.\nReturns: type: description*\n\nsource\n\n\nget_span_repr_format\n\n get_span_repr_format ()\n\n*Returns the span representation format.\nReturns: (the span representation format, the number of characters to display in the span text)*\n\nsource\n\n\nset_span_repr_format\n\n set_span_repr_format (format=None, head:int=None)\n\n*Sets the representation format for spans and the number of characters to display in the span text.\nParameters: format (str, optional): The representation format for spans. Defaults to None. head (int, optional): The number of characters to display in the span text. Defaults to None.*\n\nsource\n\n\nie\n\n ie (s:__main__.Span)\n\n\nsource\n\n\nSpan\n\n Span (doc, start=None, end=None, name=None)\n\n*All the operations on a read-only sequence.\nConcrete subclasses must override new or init, getitem, and len.*\n\nassert Span(\"aa\",0,2) == \"aa\"\n\n\ndoc = 'world'\ndf = pd.DataFrame([\n    [Span('hello',0,5),1],\n    [Span(doc,0,5),2],\n    [Span(doc,0,5),3],\n], columns=['span','num'])\ndf\n\n\n\n\n\n\n\n\nspan\nnum\n\n\n\n\n0\n(h, e, l, l, o)\n1\n\n\n1\n(w, o, r, l, d)\n2\n\n\n2\n(w, o, r, l, d)\n3\n\n\n\n\n\n\n\n\ndoc = 'world'\ndf = pd.DataFrame([\n    ['hello',1],\n    ['world',2],\n    ['world',3],\n], columns=['span','num'])\ndf\n\n\n\n\n\n\n\n\nspan\nnum\n\n\n\n\n0\nhello\n1\n\n\n1\nworld\n2\n\n\n2\nworld\n3\n\n\n\n\n\n\n\n\n#TODO from here, ok so We need union types and to make the span class print prettily\ndf.groupby('span').sum()\n\n\n\n\n\n\n\n\nnum\n\n\nspan\n\n\n\n\n\nhello\n1\n\n\nworld\n5\n\n\n\n\n\n\n\n\nstring = \"hello stranger\"\nshort_string = \"hi\"\n\n\ns = Span(string,0,len(string),name ='doc')\ndisplay(s)\n\n[@doc,0,14) \"hello stra...\"\n\n\n\npd.DataFrame({'span':[s]})\n\n\n\n\n\n\n\n\nspan\n\n\n\n\n0\n(h, e, l, l, o, , s, t, r, a, n, g, e, r)\n\n\n\n\n\n\n\n\ndf = pd.DataFrame({'span':[s]}).map(repr)\ndf\n\n\n\n\n\n\n\n\nspan\n\n\n\n\n0\n[@doc,0,14) \"hello stra...\"\n\n\n\n\n\n\n\n\ns2 = Span(short_string)\ndisplay(s2)\n\n[@c22b5f,0,2) \"hi\"\n\n\n\nassert s == 'hello stranger'\nassert s[0:5] == 'hello'\nassert not s == s[0:5]\nassert f\"{s[0:5].as_str()} darkness\" == 'hello darkness'\nassert s[0:5][1:4] == 'ell'"
  },
  {
    "objectID": "tutorials/introduction.html",
    "href": "tutorials/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Note\n\n\n\nthis project is built with nbdev, which is a full literate programming environment built on Jupyter Notebooks. That means that every piece of documentation, including the page you’re reading now, can be accessed as interactive Jupyter notebook.\nThis tutorial will teach you the basics of the spannerlog language and the spannerlib framework.\nSpannerlog is: * Similar to Datalog, but has type safety features * Has support for aggregation functions * Enables using stateless user defined functions called IE functions to derive new relations from existing relations * has some DRY features to help you write spannerlog code effectively * comes with support for Document Spanners using the Span class.\nSpannerlib, via its Session object, enables: * registering IE functions, and aggregations functions to be used as callbacks in spannerlog. * executing spannerlog code programmatically",
    "crumbs": [
      "Tutorials",
      "Intro"
    ]
  },
  {
    "objectID": "tutorials/introduction.html#installation",
    "href": "tutorials/introduction.html#installation",
    "title": "Introduction",
    "section": "Installation",
    "text": "Installation\nprerequisites:\n\nHave Python version 3.8 or above installed\n\nTo download and install spannerlog run the following commands in your terminal:\ngit clone https://github.com/DeanLight/spannerlib\ncd spannerlib\npip install . \nMake sure you are calling the pip version of your current python environment. To install with another python interpreter, run\n&lt;path_to_python_interpreter&gt; -m pip install .\nYou can also install spannerlib in the current Jupyter kernel: \n!git clone https://github.com/DeanLight/spannerlib\n!pip install spannerlib\nIn order to use spannerlib in jupyter notebooks, you must first load it:\n\nimport spannerlib\n\nImporting the spannerlog library automatically loads the %%spannerlog cell magic which accepts spannerlog queries as shown below.\n\nnew uncle(str, str)\nuncle(\"bob\", \"greg\")\n?uncle(X,Y)\n\n'?uncle(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\nbob\ngreg",
    "crumbs": [
      "Tutorials",
      "Intro"
    ]
  },
  {
    "objectID": "tutorials/introduction.html#the-spannerlog-language",
    "href": "tutorials/introduction.html#the-spannerlog-language",
    "title": "Introduction",
    "section": "The Spannerlog Language",
    "text": "The Spannerlog Language\n\nType safe Datalog\nSpannerlog syntax is very similar to datalog, but relations and their types must be declared using the new keyword.\n\n# defining relations\nnew parent(str,str)\n# defining initial facts\nparent('xerces', 'brooke')\nparent('brooke', 'damocles')\n\nRules can be defined that describe how to derive new facts from existing facts. * We call the part to the left of the &lt;- the rule’s head (or head clause). * We call the part to the right of the &lt;- the rule’s body (made up of body clauses).\n\n# you can define relations recursively\n# and use line escapes for long rules to make them more readable\nancestor(X, Y) &lt;- parent(X, Y).\nancestor(X, Y) &lt;- parent(X, Z),\n     ancestor(Z, Y).\n\nderived and existing facts can be queried using the ? operator, with either Logical Variables such as X or constants.\n\n?parent(X,Y)\n\n?ancestor('xerces',Y)\n\n?ancestor('xerces','brooke')\n\n'?parent(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\nbrooke\ndamocles\n\n\nxerces\nbrooke\n\n\n\n\n\n\n\n\n\"?ancestor('xerces',Y)\"\n\n\n\n\n\n\n\nY\n\n\n\n\nbrooke\n\n\ndamocles\n\n\n\n\n\n\n\n\n\"?ancestor('xerces','brooke')\"\n\n\nTrue\n\n\nSpannerlog has built in support for declaring relations for primitive types: * int * str * float * bool\nBut programatically, you can define relations and add facts of any pythonic data type.\n\n\nAggregation\nYou can use aggregation functions in a rule’s head to express group-by-and-aggregate logic. Grouping happens on the non-aggregated variables; the other variables are aggregated by their respective functions.\n\nnumDescendants(X,count(Y)) &lt;- ancestor(X,Y).\n\n?numDescendants(X,N)\n\n'?numDescendants(X,N)'\n\n\n\n\n\n\n\nX\nN\n\n\n\n\nbrooke\n1\n\n\nxerces\n2\n\n\n\n\n\n\n\n\nBuilt-in aggregations include: * min * max * sum * avg * count\nBut you will see in later sections that external aggregation functions can be defined.\n\n\nIE functions\nGiven a pure (stateless) function f(X,Y)-&gt;(Z) we can think of f as deriving information from (x,y) values to generate (z) values. In the relational settings, IE functions are pure functions that take tuples over some input schema and derive a number of new tuples from them over some output schema. We can use IE functions as body clauses to derive new facts.\nIE functions are invoked using the func_name(InputVars...)-&gt;(OutputVars...) syntax.\n\nnew Texts(str)\nTexts(\"Hello darkness my old friend\")\nTexts(\"I've come to talk with you again\")\n\nWords(Word) &lt;- Texts(X), rgx(\"(\\w+)\",X)-&gt;(Word).\n\n?Words(W)\n\n'?Words(W)'\n\n\n\n\n\n\n\nW\n\n\n\n\n[@9a1d0f,0,5) \"Hello\"\n\n\n[@9a1d0f,6,14) \"darkness\"\n\n\n[@9a1d0f,15,17) \"my\"\n\n\n[@9a1d0f,18,21) \"old\"\n\n\n[@9a1d0f,22,28) \"friend\"\n\n\n[@c7e66d,0,1) \"I\"\n\n\n[@c7e66d,2,4) \"ve\"\n\n\n[@c7e66d,5,9) \"come\"\n\n\n[@c7e66d,10,12) \"to\"\n\n\n[@c7e66d,13,17) \"talk\"\n\n\n[@c7e66d,18,22) \"with\"\n\n\n[@c7e66d,23,26) \"you\"\n\n\n[@c7e66d,27,32) \"again\"\n\n\n\n\n\n\n\n\nrgx is part of the built-in IE functions. It returns Spans over the original text. We will learn more Spans later.\n\n\nDRY with variables.\nVariable that appear in rule’s and queries are called logical variables. To help make our code more DRY and easier to read we can also include normal variables and dereference them in rules and in facts using the $ operator. As follows:\n\ndarkness_sentence = \"hello darkness my old friend\"\nsunshine_sentence = \"Im walking on sunshine\"\n\nnew SentenceSentiment(str,bool)\nSentenceSentiment($darkness_sentence, False)\nSentenceSentiment($sunshine_sentence, True)\n\n?SentenceSentiment($darkness_sentence,Y)\n\nword_pattern = \"(\\w+)\"\n\nBadWords(Word) &lt;- SentenceSentiment(Sentence, False), rgx($word_pattern, Sentence)-&gt;(Word).\n\n?BadWords(W)\n\n'?SentenceSentiment($darkness_sentence,Y)'\n\n\n\n\n\n\n\nY\n\n\n\n\nFalse\n\n\n\n\n\n\n\n\n'?BadWords(W)'\n\n\n\n\n\n\n\nW\n\n\n\n\n[@42bf20,0,5) \"hello\"\n\n\n[@42bf20,6,14) \"darkness\"\n\n\n[@42bf20,15,17) \"my\"\n\n\n[@42bf20,18,21) \"old\"\n\n\n[@42bf20,22,28) \"friend\"\n\n\n\n\n\n\n\n\n\n\nSpans\nSpans, or document spanners, are objects that describe an interval of a string. They are available as part of the library and can be used as such:\n\nfrom pathlib import Path\nfrom spannerlib import Span\n\n\nsome_text = \"hello darkness my old friend\"\nfull_span = Span(some_text)\nfull_span,str(full_span)\n\n([@42bf20,0,28) \"hello dark...\", 'hello darkness my old friend')\n\n\nNote that Spans are represented by a triple of [doc_id,i,j). They encode the string doc_id[i:j].\nYou can control the doc_id with the name parameter, it is also initialized automatically as a file’s name when a Path object is fed to a span.\n\nSpan(some_text,name=\"greeting\")\n\n[@greeting,0,28) \"hello dark...\"\n\n\n\nfile_path = Path(\"copilot_data/example_code.py\")\nfile_span = Span(file_path)\nfile_span\n\n[@example_code.py,0,178) \"def f(x,y)...\"\n\n\nSpans can be initialized with specific indices:\n\nfirst_2_words = Span(some_text, start=0, end=14)\nfirst_2_words,str(first_2_words)\n\n([@42bf20,0,14) \"hello dark...\", 'hello darkness')\n\n\nAnd can be sliced like a string to produce new spans with matching indices.\n\ndarkness = first_2_words[6:14]\ndarkness\n\n[@42bf20,6,14) \"darkness\"\n\n\n\ndark = darkness[:4]\ndark\n\n[@42bf20,6,10) \"dark\"\n\n\nDue to an open issue in pandas, Spans and other classes are not printed properly in dataframes.\n\nimport pandas as pd\n\ndf = pd.DataFrame([\n    [full_span, str(full_span)],\n    [file_span, str(file_span)],\n    [first_2_words, str(first_2_words)],\n    [darkness, str(darkness)],\n    [dark, str(dark)]\n])\ndf\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n(h, e, l, l, o, , d, a, r, k, n, e, s, s, , ...\nhello darkness my old friend\n\n\n1\n(d, e, f, , f, (, x, ,, y, ), :, \\n, , , ,...\ndef f(x,y): x+y def g(x,y): return f...\n\n\n2\n(h, e, l, l, o, , d, a, r, k, n, e, s, s)\nhello darkness\n\n\n3\n(d, a, r, k, n, e, s, s)\ndarkness\n\n\n4\n(d, a, r, k)\ndark\n\n\n\n\n\n\n\nWhich is why we need to use the map(repr) workaround to see them clearly. This is also done behind the scenes by spannerlog.\n\ndf.map(repr)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n[@42bf20,0,28) \"hello dark...\"\n'hello darkness my old friend'\n\n\n1\n[@example_code.py,0,178) \"def f(x,y)...\"\n'def f(x,y):\\n x+y \\n\\ndef g(x,y):\\n ret...\n\n\n2\n[@42bf20,0,14) \"hello dark...\"\n'hello darkness'\n\n\n3\n[@42bf20,6,14) \"darkness\"\n'darkness'\n\n\n4\n[@42bf20,6,10) \"dark\"\n'dark'\n\n\n\n\n\n\n\nMany default IE functions use Spans as their return types. To convert a Span (or anything else) to a string, we can use the as_str IE function.\n\nBadWordStrings(Word) &lt;- BadWords(WordSpan),as_str(WordSpan)-&gt;(Word).\n?BadWordStrings(W)\n\n'?BadWordStrings(W)'\n\n\n\n\n\n\n\nW\n\n\n\n\ndarkness\n\n\nfriend\n\n\nhello\n\n\nmy\n\n\nold",
    "crumbs": [
      "Tutorials",
      "Intro"
    ]
  },
  {
    "objectID": "tutorials/introduction.html#python-spannerlog-interactions",
    "href": "tutorials/introduction.html#python-spannerlog-interactions",
    "title": "Introduction",
    "section": "Python Spannerlog Interactions",
    "text": "Python Spannerlog Interactions\n\nExporting query results and changing sessions.\nAll interactions between the spannerlog and python, is mediated through a Session object. For example, calling the export method with a string with spannerlog code will execute it.\n\nfrom spannerlib import Session\n\nsess=Session()\n# exports returns the value of the last statement in our code, which is the query in this case.\nuncle_df = sess.export(\"\"\"\nnew uncle(str, str)\nuncle(\"bob\", \"greg\")\n?uncle(X,Y)\n\"\"\")\nuncle_df\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\nbob\ngreg\n\n\n\n\n\n\n\nIn fact, the magic system that allows us to use %%spannerlog cells, simply sends initializes a session object and runs the code we put in %%spannerlog cells through it. The magic system also prints the results of queries to the string for ease of debugging.\nTo get or change the session that the magic system uses, do the following:\n\nfrom spannerlib import get_magic_session,set_magic_session\n\nmagic_sess = get_magic_session()\nmagic_sess.export(\"?BadWordStrings(W)\")\n\n\n\n\n\n\n\n\nW\n\n\n\n\n0\ndarkness\n\n\n1\nfriend\n\n\n2\nhello\n\n\n3\nmy\n\n\n4\nold\n\n\n\n\n\n\n\nAs you can see, rules run in the magic cells where executed inside the magic_sess object we now have access to. Now lets set the magic system to use the Session bound to the sess variable.\n\nset_magic_session(sess)\n\nNow we can query uncle from the magic system.\n\n?uncle(X,Y)\n\n'?uncle(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\nbob\ngreg\n\n\n\n\n\n\n\n\nUsing sessions programatically, we can not only get results into python, allowing us to post process them, we can also run spannerlog code outside of a jupyter notebook.\n\n\nImporting data to spannerlog\nUsually, our data doesnt come as spannerlog facts, but rather from other relational sources. We can import our data into a session as follows:\n\n# either a path to a csv file, or a dataframe\naunts_data = pd.DataFrame([\n    [\"susan\", \"greg\"],\n    [\"susan\", \"jerry\"]\n], columns=[\"Aunt\", \"Of\"])\n\nsess.import_rel(name=\"Aunts\", data=aunts_data)\n\n\n?Aunts(X,Y)\n\n'?Aunts(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\nsusan\ngreg\n\n\nsusan\njerry\n\n\n\n\n\n\n\n\nWe can also import variables from python into spannerlog.\n\nsess.import_var(name='fox_sent', value='what does the fox say?')\n\n\nFoxyWords(Word) &lt;- rgx(\"(\\w+)\",$fox_sent)-&gt;(Word).\n\n?FoxyWords(W)\n\n'?FoxyWords(W)'\n\n\n\n\n\n\n\nW\n\n\n\n\n[@0ffcc6,0,4) \"what\"\n\n\n[@0ffcc6,5,9) \"does\"\n\n\n[@0ffcc6,10,13) \"the\"\n\n\n[@0ffcc6,14,17) \"fox\"\n\n\n[@0ffcc6,18,21) \"say\"\n\n\n\n\n\n\n\n\nThere are also auxilary methods that help deleting rules if you made an error when defining a rule and do not want to restart the session, such as: * remove_all_rules * remove_rule * remode_head\nFor a full list of methods and options, see the Session class in the reference guide.\n\n\nDefining and registering IE and Aggregation functions\nPart of what makes spannerlib powerful, is that you can define your own callbacks as python functions and register them for use right away. * An IE function is a stateless function that takes a tuple over an input schema and returns as Iterable over an output schema. * An Aggregation function (Agg function for short) is a stateless function that takes a set/list of values and returns a single value. To register our functions we need to tell the session what input/output schema to expect.\n\n\"hello $ world $#\".count(\"$\")\n\n2\n\n\n\ndef char_positions(text,char):\n    # here we return a list of tuples\n    return [(i,) for i,letter in enumerate(text) if char==letter]\n\ndef char_positions_iter(text,char):\n    # we can also return a lazy iterable using python generators\n    for i,letter in enumerate(text):\n        if letter==char:\n            # spannerlib knows to wrap single values in a tuple\n            yield i \n\n# We register our function's input/output schema as a list of pythonic types\nsess.register('char_pos',char_positions_iter,[str,str],[int])\n\n\ndef count_twice(positions):\n    return 2*len(positions)\n\nsess.register_agg('count_twice',count_twice,[int],[int])\n\n\nnew Texts(str)\nTexts(\"hello darkness my old friend$\")\nTexts(\"I need a $ $ $ is what i need\")\n\nDollarPos(Text,Pos) &lt;- Texts(Text),char_pos(Text,\"$\")-&gt;(Pos).\n\n?DollarPos(T,P)\n\nTwiceTotalDollars(Text,count_twice(Pos)) &lt;- DollarPos(Text,Pos).\n?TwiceTotalDollars(T,N)\n\n'?DollarPos(T,P)'\n\n\n\n\n\n\n\nT\nP\n\n\n\n\nI need a $ $ $ is what i need\n9\n\n\nI need a $ $ $ is what i need\n11\n\n\nI need a $ $ $ is what i need\n13\n\n\nhello darkness my old friend$\n28\n\n\n\n\n\n\n\n\n'?TwiceTotalDollars(T,N)'\n\n\n\n\n\n\n\nT\nN\n\n\n\n\nI need a $ $ $ is what i need\n6\n\n\nhello darkness my old friend$\n2\n\n\n\n\n\n\n\n\nIf we want a callback function to work with multiple types, we can either register it with a common super type, or put a tuple of types. For example:\n\n# object is a super type of all python objects, so count will work on anything\nsess.register_agg('count_twice',count_twice,[object],[int])\n\n# now we support Paths as well\ndef char_positions_iter(text,char):\n    if isinstance(text,Path):\n        text = text.read_text()\n    for i,letter in enumerate(text):\n        if letter==char:\n            yield i\n\nsess.register('char_pos',char_positions_iter,[(Path,str),str],[int])\n\nTo inspect which callback functions are available in a sess, we can do:\n\nsess.get_all_functions()\n\n{'ie': {'print': IEFunction(name='print', func=&lt;function print_ie&gt;, in_schema=&lt;function object_arity&gt;, out_schema=[&lt;class 'object'&gt;]),\n  'rgx': IEFunction(name='rgx', func=&lt;function rgx&gt;, in_schema=[&lt;class 'str'&gt;, (&lt;class 'str'&gt;, &lt;class 'spannerlib.span.Span'&gt;)], out_schema=&lt;function span_arity&gt;),\n  'rgx_split': IEFunction(name='rgx_split', func=&lt;function rgx_split&gt;, in_schema=[&lt;class 'str'&gt;, (&lt;class 'str'&gt;, &lt;class 'spannerlib.span.Span'&gt;)], out_schema=[&lt;class 'spannerlib.span.Span'&gt;, &lt;class 'spannerlib.span.Span'&gt;]),\n  'rgx_is_match': IEFunction(name='rgx_is_match', func=&lt;function rgx_is_match&gt;, in_schema=[&lt;class 'str'&gt;, (&lt;class 'str'&gt;, &lt;class 'spannerlib.span.Span'&gt;)], out_schema=[&lt;class 'bool'&gt;]),\n  'expr_eval': IEFunction(name='expr_eval', func=&lt;function expr_eval&gt;, in_schema=&lt;function object_arity&gt;, out_schema=[&lt;class 'object'&gt;]),\n  'not': IEFunction(name='not', func=&lt;function not_ie&gt;, in_schema=[&lt;class 'bool'&gt;], out_schema=[&lt;class 'bool'&gt;]),\n  'as_str': IEFunction(name='as_str', func=&lt;function as_str&gt;, in_schema=[&lt;class 'object'&gt;], out_schema=[&lt;class 'str'&gt;]),\n  'span_contained': IEFunction(name='span_contained', func=&lt;function span_contained&gt;, in_schema=[&lt;class 'spannerlib.span.Span'&gt;, &lt;class 'spannerlib.span.Span'&gt;], out_schema=[&lt;class 'bool'&gt;]),\n  'deconstruct_span': IEFunction(name='deconstruct_span', func=&lt;function deconstruct_span&gt;, in_schema=[&lt;class 'spannerlib.span.Span'&gt;], out_schema=[&lt;class 'str'&gt;, &lt;class 'int'&gt;, &lt;class 'int'&gt;]),\n  'read': IEFunction(name='read', func=&lt;function read&gt;, in_schema=[&lt;class 'str'&gt;], out_schema=[&lt;class 'str'&gt;]),\n  'read_span': IEFunction(name='read_span', func=&lt;function read_span&gt;, in_schema=[&lt;class 'str'&gt;], out_schema=[&lt;class 'spannerlib.span.Span'&gt;]),\n  'json_path': IEFunction(name='json_path', func=&lt;function json_path&gt;, in_schema=[&lt;class 'str'&gt;, &lt;class 'str'&gt;], out_schema=[&lt;class 'str'&gt;, &lt;class 'str'&gt;]),\n  'char_pos': IEFunction(name='char_pos', func=&lt;function char_positions_iter&gt;, in_schema=[(&lt;class 'pathlib.Path'&gt;, &lt;class 'str'&gt;), &lt;class 'str'&gt;], out_schema=[&lt;class 'int'&gt;])},\n 'agg': {'count': AGGFunction(name='count', func='count', in_schema=[&lt;class 'object'&gt;], out_schema=[&lt;class 'int'&gt;]),\n  'sum': AGGFunction(name='sum', func='sum', in_schema=[&lt;class 'numbers.Real'&gt;], out_schema=[&lt;class 'numbers.Real'&gt;]),\n  'avg': AGGFunction(name='avg', func='avg', in_schema=[&lt;class 'numbers.Real'&gt;], out_schema=[&lt;class 'numbers.Real'&gt;]),\n  'max': AGGFunction(name='max', func='max', in_schema=[&lt;class 'numbers.Real'&gt;], out_schema=[&lt;class 'numbers.Real'&gt;]),\n  'min': AGGFunction(name='min', func='min', in_schema=[&lt;class 'numbers.Real'&gt;], out_schema=[&lt;class 'numbers.Real'&gt;]),\n  'count_twice': AGGFunction(name='count_twice', func=&lt;function count_twice&gt;, in_schema=[&lt;class 'object'&gt;], out_schema=[&lt;class 'int'&gt;])}}\n\n\nTo get a nested dict with all callbacks. To see the list of default IE function and their documentation, go to the standard ie function section in the documentation.",
    "crumbs": [
      "Tutorials",
      "Intro"
    ]
  },
  {
    "objectID": "tutorials/extending_code.html",
    "href": "tutorials/extending_code.html",
    "title": "Extending pipelines",
    "section": "",
    "text": "In the previous tutorial, we showed how Spannerlib can be used to build sophisticated LLM agents by combining LLMs with structured IE functions. The agent was able to be elegantly described by a small number of logical statements using spannerlog with the use of very generic IE functions.\nHowever, an elegant codebase is not worth much if it is hard to extend. In this tutorial, we will show how easy it is to extend spannerlib code, demonstrating that the spannerlib framework can be used to create modular code that is easily modifiable.\nOur usecase will be to extend our previous code documentation agent with two of the most commonly used prompt augmentation techniques used when building LLM agents. Namely:\n\nRetrival Augmented Generation (RAG)\nFew-shot Prompting\n\nWe will introduce both briefly here.\nRAG - is a technique that utilized a vector database to dynamically augment an LLM prompt with information that might be relevant and helpful for the LLM in answering the prompt well. RAG requires a vector database, in which documents are stored along side an embedded vector representation of themselves. In RAG, before calling an LLM with a given quetion/task, we:\n\nEmbed the question as a vector.\nLook for similar document in our database, based on vector similarity measures.\nAdd these documents to the prompt.\nCall the LLM function.\n\nIn our use-case, we will demonstrate how to add RAG over stackoverflow posts to add better context to our code completion agent.\nFew-shot Prompting - is a technique for tweaking a prompt to elicit answers that use the type of semantics/style/reasoning that we would like the LLM to perform. To do so, for a given task, we augment the prompt by adding question answer pairs of similar tasks to the prompt. This will help condition the activation of the LLMs towards similar answer that are relevant for our question.\nIn our use-case, we will demonstrate how to add Few-shot Prompting using a database that collected question answer pairs that were given a positive review by a given user, to implement a userfeed back system. This system will enable us to improve our system per user without requiring any fine tuning of the model’s weights.",
    "crumbs": [
      "Tutorials",
      "Extending Code"
    ]
  },
  {
    "objectID": "tutorials/extending_code.html#motivation",
    "href": "tutorials/extending_code.html#motivation",
    "title": "Extending pipelines",
    "section": "",
    "text": "In the previous tutorial, we showed how Spannerlib can be used to build sophisticated LLM agents by combining LLMs with structured IE functions. The agent was able to be elegantly described by a small number of logical statements using spannerlog with the use of very generic IE functions.\nHowever, an elegant codebase is not worth much if it is hard to extend. In this tutorial, we will show how easy it is to extend spannerlib code, demonstrating that the spannerlib framework can be used to create modular code that is easily modifiable.\nOur usecase will be to extend our previous code documentation agent with two of the most commonly used prompt augmentation techniques used when building LLM agents. Namely:\n\nRetrival Augmented Generation (RAG)\nFew-shot Prompting\n\nWe will introduce both briefly here.\nRAG - is a technique that utilized a vector database to dynamically augment an LLM prompt with information that might be relevant and helpful for the LLM in answering the prompt well. RAG requires a vector database, in which documents are stored along side an embedded vector representation of themselves. In RAG, before calling an LLM with a given quetion/task, we:\n\nEmbed the question as a vector.\nLook for similar document in our database, based on vector similarity measures.\nAdd these documents to the prompt.\nCall the LLM function.\n\nIn our use-case, we will demonstrate how to add RAG over stackoverflow posts to add better context to our code completion agent.\nFew-shot Prompting - is a technique for tweaking a prompt to elicit answers that use the type of semantics/style/reasoning that we would like the LLM to perform. To do so, for a given task, we augment the prompt by adding question answer pairs of similar tasks to the prompt. This will help condition the activation of the LLMs towards similar answer that are relevant for our question.\nIn our use-case, we will demonstrate how to add Few-shot Prompting using a database that collected question answer pairs that were given a positive review by a given user, to implement a userfeed back system. This system will enable us to improve our system per user without requiring any fine tuning of the model’s weights.",
    "crumbs": [
      "Tutorials",
      "Extending Code"
    ]
  },
  {
    "objectID": "tutorials/extending_code.html#problem-definition",
    "href": "tutorials/extending_code.html#problem-definition",
    "title": "Extending pipelines",
    "section": "Problem definition",
    "text": "Problem definition\nGiven:\n\nA collection of python files.\nA cursor position in a python file.\nA vector database populated with high quality answers from stack overflow\nA database that contains question answer pairs from previous tasks that got positive feedback from users\n\nReturn:\n\nA doc string of the python function that wraps the position of our cursor.\n\nWe will reuse all previously introduced IE functions and add a new one:\n\nvector_search(query_document,k,namespace)-&gt;(similar_document) which uses an external vector database and given a query document and a number \\(k\\) returns \\(k\\) similar documents from a given namespace in the vector database.\n\nNote that vector DBs include namespaces for their data to enable categorising vectors and querying per category.",
    "crumbs": [
      "Tutorials",
      "Extending Code"
    ]
  },
  {
    "objectID": "tutorials/extending_code.html#tldr",
    "href": "tutorials/extending_code.html#tldr",
    "title": "Extending pipelines",
    "section": "TLDR",
    "text": "TLDR\n%%spannerlog\n# get documents from a vector db based on a prompt\nRagContext(cursor,lex_concat(context))&lt;-\n    DocumentFunctionPrompt(cursor,prompt),\n    vector_search(prompt,4,'stackoverflow')-&gt;(context,similarity_score).\n\n# inject documents into a prompt as context\nRagPrompt(cursor,prompt)&lt;-\n    RagContext(cursor,context),\n    DocumentFunctionPrompt(cursor,document_promps),\n    format($rag_prompt,context,document_promps)-&gt;(prompt).\n\n# format all user approved completion into q,a pairs\nFewShotExamples(user,lex_concat(qa_pair))&lt;-\n    PositiveFeedback(user,q,a),\n    format($single_example_template,q,a)-&gt;(qa_pair).\n\n# build a few shot prompt from approved completions\nFewShotPrompt(user,prompt)&lt;-\n    FewShotExamples(user,examples),\n    format($fewshot_template,examples)-&gt;(prompt).\n\n# combine the few shot prompt with the rag prompt\n# and call the llm to get the answer\nFewShotRagDocument(user,cursor,prompt)&lt;-\n    PerUserCompletion(user,cursor),\n    FewShotPrompt(user,few_shot_prompt),\n    RagPrompt(cursor,rag_prompt),\n    format(\"{} {}\",rag_prompt,few_shot_prompt)-&gt;(prompt),\n    llm($model,prompt)-&gt;(answer).",
    "crumbs": [
      "Tutorials",
      "Extending Code"
    ]
  },
  {
    "objectID": "tutorials/extending_code.html#importing-ie-functions-and-logic-from-previous-tutorials",
    "href": "tutorials/extending_code.html#importing-ie-functions-and-logic-from-previous-tutorials",
    "title": "Extending pipelines",
    "section": "Importing IE functions and Logic from previous tutorials",
    "text": "Importing IE functions and Logic from previous tutorials\n\n# importing dependencies\nimport re\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pathlib import Path\nfrom spannerlib.utils import load_env\nfrom spannerlib import get_magic_session,Session,Span\nimport ast\n\n\n# load openAI api key from env file\nload_env()",
    "crumbs": [
      "Tutorials",
      "Extending Code"
    ]
  },
  {
    "objectID": "tutorials/extending_code.html#ie-functions-and-logic-from-previous-implementations",
    "href": "tutorials/extending_code.html#ie-functions-and-logic-from-previous-implementations",
    "title": "Extending pipelines",
    "section": "IE functions and logic from previous implementations",
    "text": "IE functions and logic from previous implementations\n\nfrom spannerlib.tutorials.basic import llm,format_ie,string_schema,_get_client\nfrom spannerlib.tutorials.copilot import ast_xpath,ast_to_span,lex_concat\n\n\nsess = get_magic_session()\nsess.register('llm',llm,[str,str],[str])\nsess.register('format', format_ie, string_schema,[str])\nsess.register('ast_xpath',ast_xpath,[(str,Path,Span),str],[ast.AST])\nsess.register('ast_to_span',ast_to_span,[(str,Span,Path),ast.AST],[Span])\nsess.register_agg('lex_concat',lex_concat,[(str,Span)],[str])\n\n\ncode_file = Path('copilot_data/example_code.py')\n\nexample_files = pd.DataFrame([(Span(code_file),)])\ncursors =pd.DataFrame([(Span(code_file,16,17),)])\n\nsess.import_rel('Files',example_files)\nsess.import_rel('Cursors',cursors)\n\nfunc_document_template = \"\"\"system: based on the following context:\n{}\nExplain the following function:\n{}\nIn the format of a doc string.\n\"\"\"\nsess.import_var('func_document_template',func_document_template)\n\n\nFuncDefSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(text, \"//FunctionDef\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    expr_eval(\"{0}.name\",node)-&gt;(name).\n\nFuncCallSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(text, \"//Call/func/Name\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    as_str(span)-&gt;(name).\n\nCursorWrappingFunc(cursor,name)&lt;-\n    Cursors(cursor),\n    FuncDefSpan(span,name),\n    span_contained(cursor,span)-&gt;(True).\n\nMentions(lex_concat(caller_span),called_name)&lt;-\n    FuncCallSpan(called_span,called_name),\n    FuncDefSpan(caller_span,caller_name),\n    span_contained(called_span,caller_span)-&gt;(True).\n\nmodel = 'gpt-3.5-turbo'\nDocumentFunctionPrompt(cursor,prompt)&lt;-\n    CursorWrappingFunc(cursor,name),\n    Mentions(mentions,name),\n    FuncDefSpan(def_span,name),\n    as_str(def_span)-&gt;(def_string),\n    format($func_document_template,mentions,def_string)-&gt;(prompt).\n\nDocumentFunction(cursor,answer)&lt;-\n    DocumentFunctionPrompt(cursor,prompt),\n    llm($model,prompt)-&gt;(answer).",
    "crumbs": [
      "Tutorials",
      "Extending Code"
    ]
  },
  {
    "objectID": "tutorials/extending_code.html#adding-rag",
    "href": "tutorials/extending_code.html#adding-rag",
    "title": "Extending pipelines",
    "section": "Adding RAG",
    "text": "Adding RAG\n\nbuilding a vecdb IE function\nIf the implementation details are not of interest, feel free to move to the next section.\n\nimport faiss\nimport numpy as np\nimport openai\nfrom collections import defaultdict\nfrom openai import OpenAI\n\ndef get_openai_embeddings(texts):\n    client = _get_client()\n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",  # or another embedding model\n        input=texts\n    )\n    embeddings = [item.embedding for item in response.data]\n    return np.array(embeddings)\n\n\nclass VecDB():\n    def __init__(self):\n        self.index_map={}# namespace: index\n        self.doc_map=defaultdict(list)# namespace: list of docs\n        self.dim = 1536\n    def add_index(self,namespace):\n        self.index_map[namespace] = faiss.IndexFlatL2(self.dim)\n\n    def add_docs(self,documents,namespace='default'):\n        if not namespace in self.index_map:\n            self.add_index(namespace)\n        documents = [str(doc) for doc in documents]\n        embeddings = get_openai_embeddings(documents)\n        self.index_map[namespace].add(embeddings.astype('float32'))\n        self.doc_map[namespace].extend(documents)\n\n    def search(self, query, k=1,namespace='default'):\n        query_embedding = get_openai_embeddings([query])[0]\n        index = self.index_map[namespace]\n        documents = self.doc_map[namespace]\n        D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n        return [(documents[i], float(D[0][j])) for j, i in enumerate(I[0])]\n\n\ndocuments = [\n    \"FAISS is a library for efficient similarity search.\",\n    \"Vector databases are crucial for RAG pipelines.\",\n    \"FAISS was developed by Facebook AI Research.\",\n    \"RAG combines retrieval and generation for better results.\"\n]\n\n\ndb=VecDB()\ndb.add_docs(documents)\ndb.search(\"RAG?\",4)\n\n[('RAG combines retrieval and generation for better results.',\n  0.22323353588581085),\n ('Vector databases are crucial for RAG pipelines.', 0.3760342001914978),\n ('FAISS was developed by Facebook AI Research.', 0.5168014168739319),\n ('FAISS is a library for efficient similarity search.', 0.5336617231369019)]\n\n\n\nsess.register('vector_search',db.search,[(str,Span),int,str],[str,float])\n\n\n\nAdding stack overflow posts to vector DB\n\ndocs = Path('copilot_data/stackoverflow_posts.txt').read_text().split('DELIM')\ndocs = [doc.strip() for doc in docs]\ndocs\n\n['1. **Use clear and concise language**\\n   Always strive for clarity in your documentation. Use simple, straightforward language and provide examples:\\n\\n   ```python\\n   def calculate_area(length, width):\\n       \"\"\"\\n       Calculate the area of a rectangle.\\n\\n       :param length: The length of the rectangle\\n       :param width: The width of the rectangle\\n       :return: The area of the rectangle\\n       \"\"\"\\n       return length * width\\n   ```',\n '2. **Include code examples with comments**\\n   Provide relevant code snippets with inline comments to explain each step:\\n\\n   ```javascript\\n   // Function to calculate factorial\\n   function factorial(n) {\\n       if (n === 0 || n === 1) {\\n           return 1; // Base case: 0! and 1! are 1\\n       } else {\\n           return n * factorial(n - 1); // Recursive case\\n       }\\n   }\\n   ```',\n \"3. **Structure your documentation with markdown**\\n   Use markdown to structure your documentation for better readability:\\n\\n   ```markdown\\n   # My Project\\n\\n   ## Installation\\n   ```bash\\n   npm install my-project\\n   ```\\n\\n   ## Usage\\n   ```javascript\\n   const myProject = require('my-project');\\n   myProject.doSomething();\\n   ```\\n   ```\",\n '4. **Write for your audience with examples**\\n   Adjust your language and examples based on your audience:\\n\\n   ```python\\n   # For beginners\\n   name = input(\"What\\'s your name? \")\\n   print(f\"Hello, {name}!\")\\n\\n   # For advanced users\\n   def greet(name: str) -&gt; str:\\n       return f\"Hello, {name}!\"\\n   ```',\n '5. **Keep it up-to-date with version information**\\n   Include version information and update logs:\\n\\n   ```python\\n   \"\"\"\\n   MyModule - A helpful utility\\n\\n   Version: 1.2.3\\n   Last Updated: 2024-07-30\\n\\n   Changelog:\\n   - 1.2.3: Fixed bug in process_data function\\n   - 1.2.2: Added new feature X\\n   \"\"\"\\n\\n   def process_data(data):\\n       # Implementation here\\n       pass\\n   ```',\n '6. **Use diagrams and visuals with code**\\n   Include ASCII diagrams or links to visual aids in your code comments:\\n\\n   ```python\\n   def binary_search(arr, target):\\n       \"\"\"\\n       Performs binary search.\\n\\n       ASCII Diagram:\\n       [1, 3, 5, 7, 9]\\n        ^     ^     ^\\n       low   mid   high\\n\\n       :param arr: Sorted array\\n       :param target: Target value\\n       :return: Index of target or -1 if not found\\n       \"\"\"\\n       # Implementation here\\n       pass\\n   ```',\n '7. **Provide a table of contents with code sections**\\n   For longer documents, include a table of contents with links to code sections:\\n\\n   ```markdown\\n   # Table of Contents\\n   1. [Installation](#installation)\\n   2. [Usage](#usage)\\n   3. [API Reference](#api-reference)\\n\\n   ## Installation\\n   ```bash\\n   pip install mypackage\\n   ```\\n\\n   ## Usage\\n   ```python\\n   import mypackage\\n   mypackage.function()\\n   ```\\n\\n   ## API Reference\\n   ```python\\n   def function(param1, param2):\\n       \"\"\"Detailed function description\"\"\"\\n       pass\\n   ```\\n   ```',\n '8. **Use consistent formatting**\\n   Maintain consistent formatting throughout your documentation:\\n\\n   ```python\\n   def function_one(param1: int, param2: str) -&gt; bool:\\n       \"\"\"Does something.\"\"\"\\n       pass\\n\\n   def function_two(param1: float, param2: list) -&gt; dict:\\n       \"\"\"Does something else.\"\"\"\\n       pass\\n   ```',\n '9. **Include a \"Getting Started\" section with code**\\n   Provide a quick start guide with simple code examples:\\n\\n   ```python\\n   # Getting Started with MyLibrary\\n\\n   # 1. Import the library\\n   import mylibrary\\n\\n   # 2. Create an instance\\n   my_instance = mylibrary.MyClass()\\n\\n   # 3. Use a basic function\\n   result = my_instance.do_something()\\n\\n   # 4. Print the result\\n   print(result)\\n   ```',\n '10. **Document error messages and troubleshooting steps**\\n    Include common error messages and their solutions:\\n\\n    ```python\\n    try:\\n        result = 10 / 0\\n    except ZeroDivisionError as e:\\n        print(f\"Error: {e}\")\\n        print(\"Solution: Ensure the divisor is not zero.\")\\n    ```',\n '11. **Use version control for documentation**\\n    Show how to include documentation in version control:\\n\\n    ```bash\\n    # Add documentation to git\\n    git add docs/\\n\\n    # Commit changes\\n    git commit -m \"Updated API documentation for v2.0\"\\n\\n    # Push to remote repository\\n    git push origin main\\n    ```',\n '12. **Provide examples of input and output**\\n    When documenting functions or APIs, include examples of expected inputs and outputs:\\n\\n    ```python\\n    def square(n):\\n        \"\"\"\\n        Return the square of a number.\\n\\n        :param n: The number to square\\n        :return: The square of the input number\\n\\n        Example:\\n        &gt;&gt;&gt; square(4)\\n        16\\n        &gt;&gt;&gt; square(-3)\\n        9\\n        \"\"\"\\n        return n ** 2\\n    ```',\n '13. **Use docstrings for inline documentation**\\n    Use docstrings to provide inline documentation:\\n\\n    ```python\\n    class MyClass:\\n        \"\"\"\\n        A class that represents MyClass.\\n\\n        Attributes:\\n            attr1 (int): Description of attr1\\n            attr2 (str): Description of attr2\\n        \"\"\"\\n\\n        def __init__(self, attr1, attr2):\\n            self.attr1 = attr1\\n            self.attr2 = attr2\\n\\n        def my_method(self, param1):\\n            \"\"\"\\n            Description of my_method.\\n\\n            :param param1: Description of param1\\n            :return: Description of return value\\n            \"\"\"\\n            pass\\n    ```',\n '14. **Include a changelog in your code**\\n    Maintain a changelog to track major changes:\\n\\n    ```python\\n    \"\"\"\\n    Changelog:\\n\\n    v1.1.0 (2024-07-30):\\n    - Added new feature X\\n    - Fixed bug in function Y\\n\\n    v1.0.1 (2024-07-15):\\n    - Updated documentation\\n    - Performance improvements\\n\\n    v1.0.0 (2024-07-01):\\n    - Initial release\\n    \"\"\"\\n\\n    # Your code here\\n    ```',\n \"15. **Provide context and explanations in comments**\\n    Don't just describe what something does, explain why it's important:\\n\\n    ```python\\n    # We use a cache to store expensive computation results\\n    # This significantly improves performance for repeated calls\\n    cache = {}\\n\\n    def expensive_function(n):\\n        if n in cache:\\n            return cache[n]\\n        result = # ... some expensive computation\\n        cache[n] = result\\n        return result\\n    ```\",\n '16. **Use links effectively in documentation**\\n    Link to related sections or external resources:\\n\\n    ```python\\n    \"\"\"\\n    For more information on this module, see:\\n    - [API Documentation](https://example.com/api-docs)\\n    - [Usage Examples](https://example.com/examples)\\n    - Related function: `other_function()`\\n    \"\"\"\\n\\n    def my_function():\\n        pass\\n\\n    def other_function():\\n        pass\\n    ```',\n \"17. **Include a search function (for online docs)**\\n    For online documentation, implement a search feature. Here's a simple JavaScript example:\\n\\n    ```javascript\\n    function searchDocs() {\\n        var input = document.getElementById('searchInput').value.toLowerCase();\\n        var elements = document.getElementsByClassName('searchable');\\n        \\n        for (var i = 0; i &lt; elements.length; i++) {\\n            var content = elements[i].textContent.toLowerCase();\\n            if (content.includes(input)) {\\n                elements[i].style.display = 'block';\\n            } else {\\n                elements[i].style.display = 'none';\\n            }\\n        }\\n    }\\n    ```\",\n '18. **Write clear method and function signatures**\\n    Clearly document the parameters, return values, and any exceptions:\\n\\n    ```python\\n    def process_data(data: List[Dict[str, Any]],\\n                     options: Optional[Dict[str, Any]] = None) -&gt; Tuple[List[Any], int]:\\n        \"\"\"\\n        Process the input data according to specified options.\\n\\n        :param data: A list of dictionaries containing the input data\\n        :param options: Optional dictionary of processing options\\n        :return: A tuple containing the processed data and a status code\\n        :raises ValueError: If the input data is empty or invalid\\n        \"\"\"\\n        if not data:\\n            raise ValueError(\"Input data cannot be empty\")\\n        \\n        # Processing logic here\\n        \\n        return processed_data, status_code\\n    ```',\n '19. **Use meaningful variable and function names**\\n    Choose descriptive names that convey the purpose or functionality:\\n\\n    ```python\\n    def calculate_total_price(item_prices: List[float], tax_rate: float) -&gt; float:\\n        \"\"\"\\n        Calculate the total price including tax.\\n\\n        :param item_prices: List of individual item prices\\n        :param tax_rate: The tax rate as a decimal (e.g., 0.08 for 8%)\\n        :return: The total price including tax\\n        \"\"\"\\n        subtotal = sum(item_prices)\\n        tax_amount = subtotal * tax_rate\\n        total_price = subtotal + tax_amount\\n        return total_price\\n    ```',\n '20. **Include a license and contribution guidelines**\\n    For open-source projects, clearly state the license and provide contribution guidelines:\\n\\n    ```python\\n    \"\"\"\\n    MyProject - A helpful Python utility\\n\\n    Copyright (c) 2024 Your Name\\n\\n    Licensed under the MIT License.\\n    See LICENSE file for details.\\n\\n    Contribution Guidelines:\\n    1. Fork the repository\\n    2. Create a new branch for your feature\\n    3. Write tests for your changes\\n    4. Ensure all tests pass\\n    5. Submit a pull request\\n\\n    For more details, see CONTRIBUTING.md\\n    \"\"\"\\n\\n    # Your code here\\n    ```']\n\n\n\ndb.add_docs(docs,namespace='stackoverflow')\n\n\nVecDBQueryExample(relevant_docs,similarity_score)&lt;-\n    vector_search('python',4,'stackoverflow')-&gt;(relevant_docs,similarity_score).\n\n?VecDBQueryExample(relevant_docs,similarity_score)\n\n'?VecDBQueryExample(relevant_docs,similarity_score)'\n\n\n\n\n\n\n\nrelevant_docs\nsimilarity_score\n\n\n\n\n16. **Use links effectively in documentation** Link to related sections or external resources: ```python \"\"\" For more information on this module, see: - [API Documentation](https://example.com/api-docs) - [Usage Examples](https://example.com/examples) - Related function: `other_function()` \"\"\" def my_function(): pass def other_function(): pass ```\n0.463158\n\n\n20. **Include a license and contribution guidelines** For open-source projects, clearly state the license and provide contribution guidelines: ```python \"\"\" MyProject - A helpful Python utility Copyright (c) 2024 Your Name Licensed under the MIT License. See LICENSE file for details. Contribution Guidelines: 1. Fork the repository 2. Create a new branch for your feature 3. Write tests for your changes 4. Ensure all tests pass 5. Submit a pull request For more details, see CONTRIBUTING.md \"\"\" # Your code here ```\n0.474433\n\n\n4. **Write for your audience with examples** Adjust your language and examples based on your audience: ```python # For beginners name = input(\"What's your name? \") print(f\"Hello, {name}!\") # For advanced users def greet(name: str) -&gt; str: return f\"Hello, {name}!\" ```\n0.479981\n\n\n9. **Include a \"Getting Started\" section with code** Provide a quick start guide with simple code examples: ```python # Getting Started with MyLibrary # 1. Import the library import mylibrary # 2. Create an instance my_instance = mylibrary.MyClass() # 3. Use a basic function result = my_instance.do_something() # 4. Print the result print(result) ```\n0.455388\n\n\n\n\n\n\n\n\n\n\nExtending our pipeline\n\n# let us recall the prompt we get from the original implementation\n\n\n?DocumentFunctionPrompt(C,P)\n\n'?DocumentFunctionPrompt(C,P)'\n\n\n\n\n\n\n\nC\nP\n\n\n\n\n[@example_code.py,16,17) \"x\"\nsystem: based on the following context: def g(x,y): return f(x,y)**2 def method(self, y): return f(self.x, y) Explain the following function: def f(x,y): x+y In the format of a doc string.\n\n\n\n\n\n\n\n\nWe use our previous prompt to query our stackoverflow database for relevant posts. We concat our relevant documents to one string so we can format them in a new prompt.\n\nRagContext(cursor,lex_concat(context))&lt;-\n    DocumentFunctionPrompt(cursor,prompt),\n    vector_search(prompt,4,'stackoverflow')-&gt;(context,similarity_score).\n?RagContext(cursor,context)\n\n'?RagContext(cursor,context)'\n\n\n\n\n\n\n\ncursor\ncontext\n\n\n\n\n[@example_code.py,16,17) \"x\"\n1. **Use clear and concise language** Always strive for clarity in your documentation. Use simple, straightforward language and provide examples: ```python def calculate_area(length, width): \"\"\" Calculate the area of a rectangle. :param length: The length of the rectangle :param width: The width of the rectangle :return: The area of the rectangle \"\"\" return length * width ``` 12. **Provide examples of input and output** When documenting functions or APIs, include examples of expected inputs and outputs: ```python def square(n): \"\"\" Return the square of a number. :param n: The number to square :return: The square of the input number Example: &gt;&gt;&gt; square(4) 16 &gt;&gt;&gt; square(-3) 9 \"\"\" return n ** 2 ``` 13. **Use docstrings for inline documentation** Use docstrings to provide inline documentation: ```python class MyClass: \"\"\" A class that represents MyClass. Attributes: attr1 (int): Description of attr1 attr2 (str): Description of attr2 \"\"\" def __init__(self, attr1, attr2): self.attr1 = attr1 self.attr2 = attr2 def my_method(self, param1): \"\"\" Description of my_method. :param param1: Description of param1 :return: Description of return value \"\"\" pass ``` 15. **Provide context and explanations in comments** Don't just describe what something does, explain why it's important: ```python # We use a cache to store expensive computation results # This significantly improves performance for repeated calls cache = {} def expensive_function(n): if n in cache: return cache[n] result = # ... some expensive computation cache[n] = result return result ```\n\n\n\n\n\n\n\n\nNow we will build a rag template\n\nrag_prompt = \"\"\"system: Based on the following context\n{}\nanswer the following question\n{}\n\"\"\"\n\nsess.import_var('rag_prompt',rag_prompt)\n\nAnd compose our old prompt with the context we got, ultimately sending it to our llm.\n\nRagPrompt(cursor,prompt)&lt;-\n    RagContext(cursor,context),\n    DocumentFunctionPrompt(cursor,document_promps),\n    format($rag_prompt,context,document_promps)-&gt;(prompt).\n\nRagCompletion(cursor,answer)&lt;-\n    RagPrompt(cursor,prompt),\n    llm($model,prompt)-&gt;(answer).\n\n?RagCompletion(cursor,answer)\n\n'?RagCompletion(cursor,answer)'\n\n\n\n\n\n\n\ncursor\nanswer\n\n\n\n\n[@example_code.py,16,17) \"x\"\npython def f(x, y): \"\"\" Calculate the sum of two numbers. :param x: The first number to be added :param y: The second number to be added :return: The sum of x and y Example: &gt;&gt;&gt; f(2, 3) 5 &gt;&gt;&gt; f(-2, 5) 3 \"\"\" return x + y",
    "crumbs": [
      "Tutorials",
      "Extending Code"
    ]
  },
  {
    "objectID": "tutorials/extending_code.html#adding-user-feedback",
    "href": "tutorials/extending_code.html#adding-user-feedback",
    "title": "Extending pipelines",
    "section": "Adding user feedback",
    "text": "Adding user feedback\n\nGetting positive user feedback data.\nHere is some example feedback data. Showing a completion that was liked by bob and joe respectively.\n\npositive_feedback =pd.DataFrame([\n    ['bob',\"\"\"\ndef calculate_area(length, width):\n    return length * width\n\"\"\",\n\"\"\"Calculate the area of a rectangle.\n\nArgs:\n    length (float): The length of the rectangle.\n    width (float): The width of the rectangle.\n\nReturns:\n    float: The area of the rectangle.\n\nExample:\n    &gt;&gt;&gt; calculate_area(5, 3)\n    15.0\n\"\"\"],\n['joe',\"\"\"\ndef factorial(n):\n    if n &lt; 0:\n        raise ValueError(\"Factorial is not defined for negative numbers\")\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n\n\"\"\",\n\"\"\"\n    Calculate the factorial of a non-negative integer.\n\n    This function computes the factorial of a given non-negative integer using\n    a recursive approach.\n\n    :param n: The non-negative integer to calculate the factorial for.\n    :type n: int\n    :returns: The factorial of the input number.\n    :rtype: int\n    :raises ValueError: If the input is negative.\n\n    :Example:\n\n    &gt;&gt;&gt; factorial(5)\n    120\n    &gt;&gt;&gt; factorial(0)\n    1\n\n    .. note::\n       The factorial of 0 is defined to be 1.\n\n    .. warning::\n       This function may cause a stack overflow for very large inputs due to its recursive nature.\n    \"\"\"]\n],columns=['user','q','a'])\npositive_feedback\n\n\n\n\n\n\n\n\nuser\nq\na\n\n\n\n\n0\nbob\ndef calculate_area(length, width): return...\nCalculate the area of a rectangle. Args: ...\n\n\n1\njoe\ndef factorial(n): if n &lt; 0: raise...\nCalculate the factorial of a non-negative...\n\n\n\n\n\n\n\n\nsess.import_rel('PositiveFeedback',positive_feedback)\n\n\n\nBuilding a few shot prompt and pipeline\n\nfewshot_prompt_template = \"\"\"system: answer similar to the following:\n{}\"\"\"\n\nfew_shot_single_example_prompt_template = \"\"\"\nuser: {}\nassistant: {}\n\"\"\"\n\nsess.import_var('fewshot_template',fewshot_prompt_template)\nsess.import_var('single_example_template',few_shot_single_example_prompt_template)\n\n\nsess.remove_head('FewShotRagDocumentPrompt')\n\nNow we simply get the feedback of the relevant user and compose it into a prompt\n\nFewShotExamples(user,lex_concat(qa_pair))&lt;-\n    PositiveFeedback(user,q,a),\n    format($single_example_template,q,a)-&gt;(qa_pair).\n\n?FewShotExamples('bob',Q)\n\nFewShotPrompt(user,prompt)&lt;-\n    FewShotExamples(user,examples),\n    format($fewshot_template,examples)-&gt;(prompt).\n\n?FewShotPrompt('bob',prompt)\n\n\"?FewShotExamples('bob',Q)\"\n\n\n\n\n\n\n\nQ\n\n\n\n\nuser: def calculate_area(length, width): return length * width assistant: Calculate the area of a rectangle. Args: length (float): The length of the rectangle. width (float): The width of the rectangle. Returns: float: The area of the rectangle. Example: &gt;&gt;&gt; calculate_area(5, 3) 15.0\n\n\n\n\n\n\n\n\n\"?FewShotPrompt('bob',prompt)\"\n\n\n\n\n\n\n\nprompt\n\n\n\n\nsystem: answer similar to the following: user: def calculate_area(length, width): return length * width assistant: Calculate the area of a rectangle. Args: length (float): The length of the rectangle. width (float): The width of the rectangle. Returns: float: The area of the rectangle. Example: &gt;&gt;&gt; calculate_area(5, 3) 15.0\n\n\n\n\n\n\n\n\nFinally we compose our RAG and Few-shot prompt to get one single prompt.\n\nFewShotRagDocumentPrompt(user,cursor,prompt)&lt;-\n    FewShotPrompt(user,few_shot_prompt),\n    RagPrompt(cursor,rag_prompt),\n    format(\"{} {}\",rag_prompt,few_shot_prompt)-&gt;(prompt).\n\n\ndisplay(sess.export('?FewShotRagDocumentPrompt(\"bob\",C,P)')['P'][0])\n\n'system: Based on the following context\\n1. **Use clear and concise language**\\n   Always strive for clarity in your documentation. Use simple, straightforward language and provide examples:\\n\\n   ```python\\n   def calculate_area(length, width):\\n       \"\"\"\\n       Calculate the area of a rectangle.\\n\\n       :param length: The length of the rectangle\\n       :param width: The width of the rectangle\\n       :return: The area of the rectangle\\n       \"\"\"\\n       return length * width\\n   ```\\n12. **Provide examples of input and output**\\n    When documenting functions or APIs, include examples of expected inputs and outputs:\\n\\n    ```python\\n    def square(n):\\n        \"\"\"\\n        Return the square of a number.\\n\\n        :param n: The number to square\\n        :return: The square of the input number\\n\\n        Example:\\n        &gt;&gt;&gt; square(4)\\n        16\\n        &gt;&gt;&gt; square(-3)\\n        9\\n        \"\"\"\\n        return n ** 2\\n    ```\\n13. **Use docstrings for inline documentation**\\n    Use docstrings to provide inline documentation:\\n\\n    ```python\\n    class MyClass:\\n        \"\"\"\\n        A class that represents MyClass.\\n\\n        Attributes:\\n            attr1 (int): Description of attr1\\n            attr2 (str): Description of attr2\\n        \"\"\"\\n\\n        def __init__(self, attr1, attr2):\\n            self.attr1 = attr1\\n            self.attr2 = attr2\\n\\n        def my_method(self, param1):\\n            \"\"\"\\n            Description of my_method.\\n\\n            :param param1: Description of param1\\n            :return: Description of return value\\n            \"\"\"\\n            pass\\n    ```\\n15. **Provide context and explanations in comments**\\n    Don\\'t just describe what something does, explain why it\\'s important:\\n\\n    ```python\\n    # We use a cache to store expensive computation results\\n    # This significantly improves performance for repeated calls\\n    cache = {}\\n\\n    def expensive_function(n):\\n        if n in cache:\\n            return cache[n]\\n        result = # ... some expensive computation\\n        cache[n] = result\\n        return result\\n    ```\\nanswer the following question\\nsystem: based on the following context:\\ndef g(x,y):\\n    return f(x,y)**2\\ndef method(self, y):\\n        return f(self.x, y)\\nExplain the following function:\\ndef f(x,y):\\n    x+y\\nIn the format of a doc string.\\n\\n system: answer similar to the following:\\n\\nuser: \\ndef calculate_area(length, width):\\n    return length * width\\n\\nassistant: Calculate the area of a rectangle.\\n\\nArgs:\\n    length (float): The length of the rectangle.\\n    width (float): The width of the rectangle.\\n\\nReturns:\\n    float: The area of the rectangle.\\n\\nExample:\\n    &gt;&gt;&gt; calculate_area(5, 3)\\n    15.0\\n\\n'\n\n\nNow lets assume we have completion requests reaching our agents from multiple users.\n\nper_user_completion = pd.DataFrame(\n    [['bob',Span(code_file,16,17)]]\n)\nsess.import_rel('PerUserCompletion',per_user_completion)\n\nWe Generate the prompt based on the user and their cursor position and call the llm.\n\nFewShotCompletion(user,cursor,answer)&lt;-\n    PerUserCompletion(user,cursor),\n    FewShotRagDocumentPrompt(user,cursor,prompt),\n    llm($model,prompt)-&gt;(answer).\n\n\nprint(sess.export(\"?FewShotCompletion('bob',C,A)\")['A'][0])\n\nCalculate the sum of two numbers.\n\nArgs:\n    x (float): The first number.\n    y (float): The second number.\n\nReturns:\n    float: The sum of x and y.\n\nExample:\n    &gt;&gt;&gt; f(3, 5)\n    8",
    "crumbs": [
      "Tutorials",
      "Extending Code"
    ]
  },
  {
    "objectID": "tutorials/extending_code.html#putting-it-all-together",
    "href": "tutorials/extending_code.html#putting-it-all-together",
    "title": "Extending pipelines",
    "section": "Putting it all together",
    "text": "Putting it all together\nSo with the addition of a single simple IE function, we extended our pipeline’s logic:\n\nFuncDefSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(text, \"//FunctionDef\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    expr_eval(\"{0}.name\",node)-&gt;(name).\n\nFuncCallSpan(span,name)&lt;-\n    Files(text),\n    ast_xpath(text, \"//Call/func/Name\")-&gt;(node),\n    ast_to_span(text,node)-&gt;(span),\n    as_str(span)-&gt;(name).\n\nCursorWrappingFunc(cursor,name)&lt;-\n    Cursors(cursor),\n    FuncDefSpan(span,name),\n    span_contained(cursor,span)-&gt;(True).\n\nMentions(lex_concat(caller_span),called_name)&lt;-\n    FuncCallSpan(called_span,called_name),\n    FuncDefSpan(caller_span,caller_name),\n    span_contained(called_span,caller_span)-&gt;(True).\n\nmodel = 'gpt-3.5-turbo'\nDocumentFunctionPrompt(cursor,prompt)&lt;-\n    CursorWrappingFunc(cursor,name),\n    Mentions(mentions,name),\n    FuncDefSpan(def_span,name),\n    as_str(def_span)-&gt;(def_string),\n    format($func_document_template,mentions,def_string)-&gt;(prompt).\n\nDocumentFunction(cursor,answer)&lt;-\n    DocumentFunctionPrompt(cursor,prompt),\n    llm($model,prompt)-&gt;(answer).\n\nBy adding the following rules\n\nRagContext(cursor,lex_concat(context))&lt;-\n    DocumentFunctionPrompt(cursor,prompt),\n    vector_search(prompt,4,'stackoverflow')-&gt;(context,similarity_score).\n\nRagPrompt(cursor,prompt)&lt;-\n    RagContext(cursor,context),\n    DocumentFunctionPrompt(cursor,document_promps),\n    format($rag_prompt,context,document_promps)-&gt;(prompt).\n\nRagCompletion(cursor,answer)&lt;-\n    RagPrompt(cursor,prompt),\n    llm($model,prompt)-&gt;(answer).\n\nFewShotExamples(user,lex_concat(qa_pair))&lt;-\\\n    PositiveFeedback(user,q,a),\n    format($single_example_template,q,a)-&gt;(qa_pair).\n\nFewShotPrompt(user,prompt)&lt;-\n    FewShotExamples(user,examples),\n    format($fewshot_template,examples)-&gt;(prompt).\n\nFewShotRagDocumentPrompt(user,cursor,prompt)&lt;-\n    FewShotPrompt(user,few_shot_prompt),\n    RagPrompt(cursor,rag_prompt),\n    format(\"{} {}\",rag_prompt,few_shot_prompt)-&gt;(prompt).\n\nFewShotCompletion(user,cursor,answer)&lt;-\n    PerUserCompletion(user,cursor),\n    FewShotRagDocumentPrompt(user,cursor,prompt),\n    llm($model,prompt)-&gt;(answer).\n\nAnd that was all it took to add RAG and FewShot prompting to our pipeline.\nNote that since we kept our different completion rules, we can also compare them easily. We went from smart addition of context from our code base.\n\nfor cursor,answer in sess.export('?DocumentFunction(cursor,answer)').itertuples(index=False,name=None):\n    print(answer)\n\n\"\"\"\nThis function calculates the sum of two inputs x and y.\n\"\"\"\n\n\n\nTo adding context to stack overflow, resulting in a docstring the follows best practice:\n\nfor cursor,answer in sess.export('?RagCompletion(cursor,answer)').itertuples(index=False,name=None):\n    print(f'{answer}'.replace('```',''))\n\npython\ndef f(x, y):\n    \"\"\"\n    Calculate the sum of two numbers.\n\n    :param x: The first number to be added\n    :param y: The second number to be added\n    :return: The sum of x and y\n\n    Example:\n    &gt;&gt;&gt; f(2, 3)\n    5\n    &gt;&gt;&gt; f(-2, 5)\n    3\n    \"\"\"\n    return x + y\n\n\n\nTo adding user feedback, resulting in a docstring for bob that follows his preffered doc string formatting style:\n\nfor cursor,answer in sess.export('?FewShotCompletion(\"bob\",cursor,answer)').itertuples(index=False,name=None):\n    print(answer)\n\nCalculate the sum of two numbers.\n\nArgs:\n    x (float): The first number.\n    y (float): The second number.\n\nReturns:\n    float: The sum of x and y.\n\nExample:\n    &gt;&gt;&gt; f(3, 5)\n    8\n\n\nThis is by no means a production ready agent, but it does demonstrate that complex agents can be programmed and modified easily using few generic IE functions and small elegnat chunks of declerative logic.",
    "crumbs": [
      "Tutorials",
      "Extending Code"
    ]
  },
  {
    "objectID": "tutorials/basic_tasks.html",
    "href": "tutorials/basic_tasks.html",
    "title": "Basic Examples",
    "section": "",
    "text": "Exported source\n# importing dependencies\nimport re\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pathlib import Path\nfrom spannerlib.utils import load_env\nfrom spannerlib import get_magic_session,Session,Span\n# load openAI api key\nload_env()\nThis tutorials aim to show how to use the spannerlib framework for simple use cases. To illustrate the simplicity of using spannerlib, all required IE functions will be implemented from scratch.",
    "crumbs": [
      "Tutorials",
      "Basic Examples"
    ]
  },
  {
    "objectID": "tutorials/basic_tasks.html#finding-identical-sentences-in-a-corpus-of-documents",
    "href": "tutorials/basic_tasks.html#finding-identical-sentences-in-a-corpus-of-documents",
    "title": "Basic Examples",
    "section": "Finding identical sentences in a corpus of documents",
    "text": "Finding identical sentences in a corpus of documents\n\nTLDR\n%%spannerlog\n\n# split documents into sentences with the split ie function\nSents(doc_id,sent)&lt;-\n    Docs(doc_id,text),split(text)-&gt;(sent).\n\n# get all pairs of equal sentences using the eq_content_spans ie function\n# make sure we only get unique pairs by using the span_lt ie function to avoid symmetry\nEqualSentsUnique(doc_id1,sent1,doc_id2,sent2)&lt;-\n    Sents(doc_id1,sent1),\n    Sents(doc_id2,sent2),\n    span_lt(sent1,sent2)-&gt;(True),\n    eq_content_spans(sent1,sent2)-&gt;(True).\n\n\nWalkthrough\nIn this example, we would like to get a collection of documents. And find identical sentences among them. For example, given the following documents:\n\ninput_documents = pd.DataFrame([\n    ('doc1', 'The quick brown fox jumps over the lazy dog. Im walking on Sunshine.'),\n    ('doc2', 'Im walking on Sunshine. Lorem ipsum. Im walking on Sunshine.'),\n    ('doc3', 'All you need is love. The quick brown fox jumps over the lazy dog.'),\n])\ninput_documents\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\ndoc1\nThe quick brown fox jumps over the lazy dog. I...\n\n\n1\ndoc2\nIm walking on Sunshine. Lorem ipsum. Im walkin...\n\n\n2\ndoc3\nAll you need is love. The quick brown fox jump...\n\n\n\n\n\n\n\nWe would like to compute that the first sentence of doc1 is equal to the second sentence of doc3 etc..\n\n\nBuilding IE functions\nTo do so, we need 2 IE functions: * One that extracts the Span of all sentences from a document, called split * The other which would let us know when sentences have identical content but are not actually the same sentence. * this will require them to have the same content when ignoring whitespace * but not be equal spans. Lets call this function eq_content_spans\nLets implement them and register them to our session object. We will use the Span class to save indexed substrings.\n\nsource\n\n\nsplit\n\n split (text)\n\n\n\nExported source\n# this implementation is naive\n# the standard library has a rgx_split ie function that does this in a more efficient way\ndef split(text):\n    split_indices = [ pos for pos,char in enumerate(text) if char == '.' ]\n    start = 0\n    for pos,char in enumerate(text):\n        if char == '.':\n            yield Span(text, start, pos)\n            start = pos+1\n\n\n\nsource\n\n\neq_content_spans\n\n eq_content_spans (span1, span2)\n\n\n\nExported source\ndef eq_content_spans(span1, span2):\n    # notice that we are yielding a boolean value\n    yield span1 != span2 and str(span1).strip() == str(span2).strip()\n\n\n\nprint(list(split('The quick brown fox jumps over the lazy dog. Im walking on Sunshine.')))\n\n[[@3ba775,0,43) \"The quick ...\", [@3ba775,44,67) \" Im walkin...\"]\n\n\n\n\nBuilding spannerlog rules\n\n# we register the functions and their input and output schema\nsess = get_magic_session()\nsess.register('split', split, [str],[Span])\nsess.register('eq_content_spans', eq_content_spans,[Span,Span],[bool])\n\nNow let us import our data\n\nsess.import_rel('Docs', input_documents)\n\nLets make sure we can see our data in using Spannerlog\n\n?Docs(doc_id,text)\n\n'?Docs(doc_id,text)'\n\n\n\n\n\n\n\ndoc_id\ntext\n\n\n\n\ndoc1\nThe quick brown fox jumps over the lazy dog. Im walking on Sunshine.\n\n\ndoc2\nIm walking on Sunshine. Lorem ipsum. Im walking on Sunshine.\n\n\ndoc3\nAll you need is love. The quick brown fox jumps over the lazy dog.\n\n\n\n\n\n\n\n\nNow let us build rules that allow us to find identical sentences:\n\n# this rule gives us\nSents(doc_id,sent)&lt;-\n    Docs(doc_id,text),split(text)-&gt;(sent).\n?Sents(doc_id,sent)\n\n# this rule find equal pairs of sentences\nEqualSents(doc_id1,sent1,doc_id2,sent2)&lt;-\n    Sents(doc_id1,sent1),\n    Sents(doc_id2,sent2),\n    eq_content_spans(sent1,sent2)-&gt;(True).\n?EqualSents(doc_id1,sent1,doc_id2,sent2)\n\n'?Sents(doc_id,sent)'\n\n\n\n\n\n\n\ndoc_id\nsent\n\n\n\n\ndoc1\n[@3ba775,0,43) \"The quick ...\"\n\n\ndoc1\n[@3ba775,44,67) \" Im walkin...\"\n\n\ndoc2\n[@06bc2d,0,22) \"Im walking...\"\n\n\ndoc2\n[@06bc2d,23,35) \" Lorem ips...\"\n\n\ndoc2\n[@06bc2d,36,59) \" Im walkin...\"\n\n\ndoc3\n[@9c32df,0,20) \"All you ne...\"\n\n\ndoc3\n[@9c32df,21,65) \" The quick...\"\n\n\n\n\n\n\n\n\n'?EqualSents(doc_id1,sent1,doc_id2,sent2)'\n\n\n\n\n\n\n\ndoc_id1\nsent1\ndoc_id2\nsent2\n\n\n\n\ndoc1\n[@3ba775,0,43) \"The quick ...\"\ndoc3\n[@9c32df,21,65) \" The quick...\"\n\n\ndoc1\n[@3ba775,44,67) \" Im walkin...\"\ndoc2\n[@06bc2d,0,22) \"Im walking...\"\n\n\ndoc1\n[@3ba775,44,67) \" Im walkin...\"\ndoc2\n[@06bc2d,36,59) \" Im walkin...\"\n\n\ndoc2\n[@06bc2d,0,22) \"Im walking...\"\ndoc1\n[@3ba775,44,67) \" Im walkin...\"\n\n\ndoc2\n[@06bc2d,0,22) \"Im walking...\"\ndoc2\n[@06bc2d,36,59) \" Im walkin...\"\n\n\ndoc2\n[@06bc2d,36,59) \" Im walkin...\"\ndoc1\n[@3ba775,44,67) \" Im walkin...\"\n\n\ndoc2\n[@06bc2d,36,59) \" Im walkin...\"\ndoc2\n[@06bc2d,0,22) \"Im walking...\"\n\n\ndoc3\n[@9c32df,21,65) \" The quick...\"\ndoc1\n[@3ba775,0,43) \"The quick ...\"\n\n\n\n\n\n\n\n\n\n\nOops, handling symmetric queries\nNotice that we got each pair twice. This is because while we think of a sentence pair as a set with two sentences, the tuples (x,y) and (y,x) are actually different. To remedy this we can limit our pairs to pairs where the first sentence is in a smaller position (ie position in memory).\nWe can do that by introducing a span_lt ie function\n\nsource\n\n\nspan_lt\n\n span_lt (span1, span2)\n\n\n\nExported source\ndef span_lt(span1, span2):\n    yield span1 &lt; span2\n\n\n\nsess.register('span_lt', span_lt, [Span,Span],[bool])\n\n\nEqualSentsUniqe(doc_id1,sent1,doc_id2,sent2)&lt;-\n    Sents(doc_id1,sent1),\n    Sents(doc_id2,sent2),\n    span_lt(sent1,sent2)-&gt;(True),\n    eq_content_spans(sent1,sent2)-&gt;(True).\n    \n?EqualSentsUniqe(doc_id1,sent1,doc_id2,sent2)\n\n'?EqualSentsUniqe(doc_id1,sent1,doc_id2,sent2)'\n\n\n\n\n\n\n\ndoc_id1\nsent1\ndoc_id2\nsent2\n\n\n\n\ndoc2\n[@06bc2d,0,22) \"Im walking...\"\ndoc1\n[@3ba775,44,67) \" Im walkin...\"\n\n\ndoc2\n[@06bc2d,0,22) \"Im walking...\"\ndoc2\n[@06bc2d,36,59) \" Im walkin...\"\n\n\ndoc2\n[@06bc2d,36,59) \" Im walkin...\"\ndoc1\n[@3ba775,44,67) \" Im walkin...\"\n\n\ndoc3\n[@9c32df,21,65) \" The quick...\"\ndoc1\n[@3ba775,0,43) \"The quick ...\"",
    "crumbs": [
      "Tutorials",
      "Basic Examples"
    ]
  },
  {
    "objectID": "tutorials/basic_tasks.html#building-llm-agents-using-spannerlib",
    "href": "tutorials/basic_tasks.html#building-llm-agents-using-spannerlib",
    "title": "Basic Examples",
    "section": "Building LLM agents using spannerlib",
    "text": "Building LLM agents using spannerlib\n\nMotivation\nMost LLM based applications do not include a single call to an LLM, but are rather built using LLM agents. LLM agents are programs that * Wrap LLMs in control logic. * Either deterministic * Or decided by an LLM * Offloads structured reasoning to tools, whose output is fed into some of the Prompts that are used by the agent’s LLMs.\nIn this tutorial we will show how to build a simple agent that might be used to build the backend of a more nuanced ChatGPT-like like system.\nOur agent takes: * A question from a user\nAnd has previous knowledge encoding * How to best answer questions on different topics. * How to a user prefers his answers to be formatted.\nFor example, given the question “How do we measure the distance between stars” we might note that in cases of scientific questions, it is better to answer using mathematical notation and formulas, rather than pure english. Moreover, some users would like the answer to be styled in an academic way, with dense prose ending with citations, like so:\n\nThe distance between stars is measured in astronomical units (AU), parsecs (pc), or light-years (ly).\n\nAstronomical Unit (AU): An AU is the average distance between the Earth and the Sun, which is approximately 93 million miles or 150 million kilometers. It is more commonly used to measure distances within our solar system.\nParsec (pc): A parsec is a unit of distance used in astronomy, defined as the distance at which an object would have a parallax angle of one arcsecond. One parsec is equal to approximately 3.26 light-years or 3.09 x 10^13 kilometers.\nLight-year (ly): A light-year is the distance that light travels in one year, which is approximately 9.46 trillion kilometers or about 5.88 trillion miles.\n\nTo measure the distance to a star, astronomers use methods such as parallax and spectroscopic parallax. Parallax involves observing a star from two different points in Earth’s orbit and measuring the apparent shift in position. Spectroscopic parallax uses the star’s spectral type and luminosity to estimate its distance.\nIn summary, astronomers use a combination of trigonometric and spectroscopic methods to measure the distance between stars in order to better understand the vastness of our universe.\nReferences: 1. Carroll, B. W., & Ostlie, D. A. (2007). An introduction to modern astrophysics (2nd ed.). Pearson Addison-Wesley. 2. Bennett, J., Donahue, M., Schneider, N., & Voit, M. (2014). The cosmic perspective (7th ed.). Pearson.\n\nWhile other users will like a shorter answer in simple language like so:\n\nOne common way to measure the distance between stars is through parallax. Parallax is the apparent shift in position of an object when viewed from two different points.\nThe parallax angle, denoted by p, is the angle formed between a line from the observer to the nearer star and a line from the observer to the farther star. The parallax angle is related to the distance to the star (d) by the formula:\nd = 1 / p\nwhere d is the distance to the star in parsecs (pc) and p is the parallax angle in arcseconds (“).\nA parsec is a unit of distance often used in astronomy, equal to about 3.26 light-years. So when we measure the parallax angle of a star, we can use the formula above to calculate its distance from Earth.\n\n\n\nProblem definition\nSo given: * A relation of the form (user,question) * A relation of the form (topic,topicSpecificIntructions) * A relation of the form (user,userSpecificInstructions)\nWe would like to output for each user and question, an answer that fits both the user’s specified preferences and the instructions fitting for that topic.\n\n\nDefining our IE functions\nFor building the agent we need 2 basic building blocks as IE functions: * an llm function of the form llm(model:str,prompt:str)-&gt;(answer:str) * a printf like function format for formatting a prompt from template and other strings which will be of the form format(template:str,s_1:str,...s_n:str)-&gt;(prompt:str)\nThe following section will involve technical details such as data type conversions and interfacing with OpenAI’s API. If this is not of interest to the reader, please skip to the next section.\n\n\nTLDR\nBasic call to LLM\n%%spannerlog\n\nmodel = 'gpt-3.5-turbo'\nprompt = \"user: sing it with me, love, what is it good for?\"\n\nTestLLM(answer)&lt;-\n    llm($model,$prompt)-&gt;(answer).\n\n?TestLLM(answer)\nStyle and topic based completion\n%%spannerlog\nmodel = 'gpt-3.5-turbo'\n# asking the llm to generate the topic based on the question\nTopicSelection(question ,topic)&lt;-\n    Questions(user,question),\n    format($topic_selection_template,question)-&gt;(prompt),\n    llm($model,prompt)-&gt;(topic).\n\n# composing a prompt based on topic and user preference \n# and calling an llm to generate the answer\nStyleBasedQA(question,user,answer)&lt;-\n    Questions(user,question),\n    TopicSelection(question,topic),\n    UserAnswerPreference(user,user_style),\n    TopicSpecificStyle(topic,topic_style),\n    format($custom_style_prompt,topic_style,user_style,question)-&gt;(prompt),\n    llm($model,prompt)-&gt;(answer).\n\n%%python\nsess.export('?StyleBasedQA(question,user,answer)')\n\n\nWalkthrough\nHere is the implemetation:\nTo implement the llm ie function, we need to wrap some LLM api as an IE function. We want an ie function that takes a string and returns a string. Since most LLM api expect a dict of the form {'role':role,'content':message} we will write converters that parse them from prompt strings of the form\nrole: content\nrole: content\n\nsource\n\n\nllm\n\n llm (model, question)\n\n\n\nExported source\nfrom spannerlib.ie_func.basic import rgx_split\nfrom functools import cache\nimport openai\nfrom joblib import Memory\nmemory = Memory(\"cachedir\", verbose=0)\n\n\n\n\nExported source\n@cache\ndef _get_client():\n    return openai.Client()\n\n# we use the rgx_split function to split the string into messages\ndef _str_to_messages (string_prompt):\n    return [\n        {\n            'role': str(role).replace(': ',''),\n            'content': str(content)\n        } for role,content in rgx_split('system:\\s|assistant:\\s|user:\\s', string_prompt.strip())\n    ]\ndef _messages_to_string(msgs):\n    return ''.join([f\"{msg['content']}\" for msg in msgs])\n\n# the specific API we are going to call using the messages interface\ndef _openai_chat(model, messages):\n    client = _get_client()\n    respone = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        seed=42\n    )\n    return [dict(respone.choices[0].message)]\n\n# we disk cache our function to spare my openAI credits\n@memory.cache\ndef llm(model, question):\n    q_msgs = _str_to_messages(question)\n    a_msgs = _openai_chat(model, q_msgs)\n    answer = _messages_to_string(a_msgs)\n    # avoid bad fromatting of the answer in the notebook due to nested code blocks\n    answer = answer.replace('```','')\n    return [answer]\n\n\n\nllm('gpt-3.5-turbo','user: Hello, who are you?')\n\n['Hello! I am a AI-powered virtual assistant designed to help and provide information to users. How can I assist you today?']\n\n\nNow we can register the llm function as an ie function and use it from in spannerlog code.\nNow let see how we can use spannerlib to build a simple pipeline that varries the LLM prompt in a data dependant way. Imagine that we have a chatbot that we want to act differently based on the topic of conversation and the user preference.\nTo enable us to format prompts from data, we will use a prompt formatting function with a printf like syntax. It is available in spannerlib’s stdlib, but we will reproduce it here.\n\nsource\n\n\nformat_ie\n\n format_ie (f_string, *params)\n\n\n\nExported source\ndef format_ie(f_string,*params):\n    yield f_string.format(*params)\n\n# note that since the schema is dynamic we need to define a function that returns the schema based on the arity\nstring_schema = lambda x: ([str]*x)\n\n\n\nsess.register('llm', llm, [str,str],[str])\nsess.register('format', format_ie, string_schema,[str])\n\n\n\nSeeing our llm IE function in action\n\nmodel = 'gpt-3.5-turbo'\nprompt = \"user: sing it with me, love, what is it good for?\"\n\nTestLLM(answer)&lt;-\n    llm($model,$prompt)-&gt;(answer).\n\n?TestLLM(answer)\n\n'?TestLLM(answer)'\n\n\n\n\n\n\n\nanswer\n\n\n\n\nAbsolutely nothing!\n\n\n\n\n\n\n\n\n\n\nExample data\nWe will model our data as follows:\n\n# a binary relation that stores how the user prefers the formatted\nuser_answer_preference = pd.DataFrame([\n    ('Bob', 'please answer in prose. Make sure you add references in the end like a bibliography.'),\n    ('Joe', 'Prose is hard for me to read quickly. Format the answer in bullet points.'),\n    ('Sally', 'Try to avoid complicated jargon and use simple language.'),\n])\n\n# relation that stores per topic style\ntopic_specific_style = pd.DataFrame([\n    ('history', 'please answer in a narrative form, use shakespearing language and lots of examples'),\n    ('science', 'Use mathematical notation and formulas to explain the concepts.'),\n    ('hiphop', 'introduce the answer with a rap verse.'),\n    ('other', 'Be polite and neutral in your answer. Avoid controversial topics.'),\n])\n\nsess.import_rel('UserAnswerPreference', user_answer_preference)\nsess.import_rel('TopicSpecificStyle', topic_specific_style)\n\n\n\nBuliding the agent logic in spannerlog\nNow we will build our pipeline as follows: * We will make an llm call to help us decide the topic of the question * Based on the topic and the user, we will formulate the final prompt for the llm to answer.\n\nprompts = pd.DataFrame([\n    ('topic_selection',\n\"\"\"\nsystem: Please select a topic from the following list: [history, science, hiphop, other]\nbased on the question provided by the user. You are only allowed to say the topic name, nothing else.\n\nuser: {}\n\"\"\"),\n    ('custom_style_prompt',\n\"\"\"\nsystem: Answer the question of the user in the following style:\n\ntopic specific style instructions: {}\n\nuser specific style instructions: {}\n\nuser: {}\n\"\"\"\n)\n])\nsess.import_rel('Prompts', prompts)\nprompts\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\ntopic_selection\nsystem: Please select a topic from the follow...\n\n\n1\ncustom_style_prompt\nsystem: Answer the question of the user in th...\n\n\n\n\n\n\n\n\n?Prompts(prompt_id,prompt)\n\n'?Prompts(prompt_id,prompt)'\n\n\n\n\n\n\n\nprompt_id\nprompt\n\n\n\n\ncustom_style_prompt\nsystem: Answer the question of the user in the following style: topic specific style instructions: {} user specific style instructions: {} user: {}\n\n\ntopic_selection\nsystem: Please select a topic from the following list: [history, science, hiphop, other] based on the question provided by the user. You are only allowed to say the topic name, nothing else. user: {}\n\n\n\n\n\n\n\n\nNow given a relation of user and question\n\nquestions= pd.DataFrame([\n    ('Bob', 'Who won the civil war?'),\n    ('Joe', 'Who won the civil war?'),\n    ('Sally', 'Who won the civil war?'),\n    ('Bob', 'How do we measure the distance between stars?'),\n    ('Joe', 'How do we measure the distance between stars?'),\n    ('Sally', 'How do we measure the distance between stars?'),\n    ('Bob', 'Who are the most well known rappers?'),\n    ('Joe', 'Who are the most well known rappers?'),\n    ('Sally', 'Who are the most well known rappers?'),\n])\nsess.import_rel('Questions', questions)\n\nNow lets combine all of this into a pipeline. We will break our pipeline into more debugable bits, and add some extra logical variables in the rule heads for ease of inspection.\n\nmodel = 'gpt-3.5-turbo'\nTopicPrompt(q,p)&lt;-\n    Questions(user,q),\n    Prompts('topic_selection',template),\n    format(template,q)-&gt;(p).\n    \n?TopicPrompt(q,p)\n\nTopicSelection(q,t)&lt;-\n    TopicPrompt(q,p),\n    llm($model,p)-&gt;(t).\n\n?TopicSelection(q,t)\n\nStylePrompt(q,p,topic,user)&lt;-\n    Questions(user,q),\n    TopicSelection(q,topic),\n    UserAnswerPreference(user,user_style),\n    TopicSpecificStyle(topic,topic_style),\n    Prompts('custom_style_prompt',prompt_template),\n    format(prompt_template,topic_style,user_style,q)-&gt;(p).\n\n\nStyle_Based_QA(question,topic,user,prompt,answer)&lt;-\n    StylePrompt(question,prompt,topic,user),\n    llm($model,prompt)-&gt;(answer).\n\n'?TopicPrompt(q,p)'\n\n\n\n\n\n\n\nq\np\n\n\n\n\nHow do we measure the distance between stars?\nsystem: Please select a topic from the following list: [history, science, hiphop, other] based on the question provided by the user. You are only allowed to say the topic name, nothing else. user: How do we measure the distance between stars?\n\n\nWho are the most well known rappers?\nsystem: Please select a topic from the following list: [history, science, hiphop, other] based on the question provided by the user. You are only allowed to say the topic name, nothing else. user: Who are the most well known rappers?\n\n\nWho won the civil war?\nsystem: Please select a topic from the following list: [history, science, hiphop, other] based on the question provided by the user. You are only allowed to say the topic name, nothing else. user: Who won the civil war?\n\n\n\n\n\n\n\n\n'?TopicSelection(q,t)'\n\n\n\n\n\n\n\nq\nt\n\n\n\n\nHow do we measure the distance between stars?\nscience\n\n\nWho are the most well known rappers?\nhiphop\n\n\nWho won the civil war?\nhistory\n\n\n\n\n\n\n\n\n\n\nExecuting our agent and returning the results to python\n\ncompletions = sess.export('?Style_Based_QA(question,topic,user,prompt,answer)')\ncompletions\n\n\n\n\n\n\n\n\nquestion\ntopic\nuser\nprompt\nanswer\n\n\n\n\n0\nHow do we measure the distance between stars?\nscience\nBob\nsystem: Answer the question of the user in th...\nThe distance between stars is measured in astr...\n\n\n1\nHow do we measure the distance between stars?\nscience\nJoe\nsystem: Answer the question of the user in th...\n- To measure the distance between stars, astro...\n\n\n2\nHow do we measure the distance between stars?\nscience\nSally\nsystem: Answer the question of the user in th...\nTo measure the distance between stars, astrono...\n\n\n3\nWho are the most well known rappers?\nhiphop\nBob\nsystem: Answer the question of the user in th...\nStraight outta Compton, a city so gritty, Let ...\n\n\n4\nWho are the most well known rappers?\nhiphop\nJoe\nsystem: Answer the question of the user in th...\nYo, when it comes to rappers, there's a whole ...\n\n\n5\nWho are the most well known rappers?\nhiphop\nSally\nsystem: Answer the question of the user in th...\nYo, when it comes to rappers, there's a whole ...\n\n\n6\nWho won the civil war?\nhistory\nBob\nsystem: Answer the question of the user in th...\nIn sooth, fair user, the tale of the Civil War...\n\n\n7\nWho won the civil war?\nhistory\nJoe\nsystem: Answer the question of the user in th...\n- The Civil War, dear user, was a bloody confl...\n\n\n8\nWho won the civil war?\nhistory\nSally\nsystem: Answer the question of the user in th...\nIn sooth, good sir, the tale of the Civil War ...\n\n\n\n\n\n\n\nAnd as you can see, we have our agent running.\n\nfor (question,topic,user,prompt,answer) in completions.itertuples(name=None, index=False):\n    print(f\"{user}: {question}\\nassistant: {answer}\\n\\n\")\n    print(\"=\"*80)\n\nBob: How do we measure the distance between stars?\nassistant: The distance between stars is measured in astronomical units (AU), parsecs (pc), or light-years (ly). \n\n1. **Astronomical Unit (AU)**: An AU is the average distance between the Earth and the Sun, which is approximately 93 million miles or 150 million kilometers. It is more commonly used to measure distances within our solar system.\n\n2. **Parsec (pc)**: A parsec is a unit of distance used in astronomy, defined as the distance at which an object would have a parallax angle of one arcsecond. One parsec is equal to about 3.26 light-years or 3.09 x 10^13 kilometers.\n\n3. **Light-year (ly)**: A light-year is the distance that light travels in one year, which is approximately 9.46 trillion kilometers or about 5.88 trillion miles.\n\nTo measure the distance to a star, astronomers use methods such as parallax and spectroscopic parallax. Parallax involves measuring the apparent shift of a star against its background as the Earth orbits the Sun. Spectroscopic parallax measures the star's luminosity and temperature to determine its distance.\n\nIn summary, the distance between stars is measured in AU, parsecs, or light-years, using methods such as parallax and spectroscopic techniques.\n\nReferences:\n- \"Astronomical Unit.\" NASA, https://solarsystem.nasa.gov/asteroids-comets-and-meteors/comets/overview/. Accessed 15 Dec. 2022.\n- \"Parsec.\" European Southern Observatory, https://www.eso.org/public/usa/. Accessed 15 Dec. 2022.\n- \"Light-Year.\" European Southern Observatory, https://www.eso.org/public/usa/. Accessed 15 Dec. 2022.\n\n\n================================================================================\nJoe: How do we measure the distance between stars?\nassistant: - To measure the distance between stars, astronomers use a method called parallax, which is based on trigonometry.\n- Parallax involves observing a star from two different points in Earth's orbit and measuring the angle between the star and distant background stars. \n- By knowing the distance between the two observation points (Earth's orbit), the angle of parallax, and some basic trigonometry, astronomers can calculate the distance to the star. \n- The formula used is d = 1 / tan(p), where d is the distance to the star in parsecs and p is the parallax angle in arcseconds.\n- By measuring the parallax angle with precision instruments, astronomers can determine the distance to stars within a certain range of accuracy.\n\n\n================================================================================\nSally: How do we measure the distance between stars?\nassistant: To measure the distance between stars, astronomers use a method called parallax. Parallax is the apparent shift in the position of an object when viewed from different angles. With parallax, astronomers measure the angle subtended by the star's position at different points in the Earth's orbit. The distance to the star can be calculated using the formula:\n\n\\[ D = \\frac{1}{\\text{parallax angle}} \\]\n\nwhere \\( D \\) is the distance to the star. The parallax angle is usually measured in arcseconds. The smaller the parallax angle, the greater the distance to the star. This method allows astronomers to measure distances to nearby stars accurately.\n\n\n================================================================================\nBob: Who are the most well known rappers?\nassistant: Straight outta Compton, a city so gritty,\nLet me tell you about the rappers who made it to the top, no pity.\nFrom the East Coast to the West, they've all left a mark,\nIcons like Tupac, Biggie, and Jay-Z, they're the ones that spark.\n\nOther legends include Eminem, with his lyrical prowess,\nAnd Kanye West, pushing boundaries, never settling for less.\nWe can't forget about the women who've paved the way,\nNicki Minaj, Cardi B, Queen Latifah, they slay.\n\nBut the rap game is vast, with talents galore,\nFrom Kendrick Lamar to Lil Wayne, they all score.\nSo if you're looking for names that shine bright,\nThese rappers will definitely be in sight.\n\nReferences:\n1. Tupac Shakur - Biography. (n.d.). Retrieved from https://www.biography.com/musician/tupac-shakur\n2. The Notorious B.I.G. - Biography. (n.d.). Retrieved from https://www.biography.com/musician/the-notorious-big\n3. Jay-Z - Biography. (n.d.). Retrieved from https://www.biography.com/musician/jay-z\n4. Eminem - Biography. (n.d.). Retrieved from https://www.biography.com/musician/eminem\n5. Kanye West - Biography. (n.d.). Retrieved from https://www.biography.com/musician/kanye-west\n6. Nicki Minaj - Biography. (n.d.). Retrieved from https://www.biography.com/musician/nicki-minaj\n\n\n================================================================================\nJoe: Who are the most well known rappers?\nassistant: Yo, when it comes to rappers, there's a whole crew,\nHere are some of the most well-known for you:\n\n- Eminem: Known for his lyrical genius and raw talent\n- Jay-Z: Business mogul and rap legend with a smooth flow\n- Tupac Shakur: Iconic figure known for his powerful lyrics and impact on the industry\n- Notorious B.I.G: Legendary rapper with a unique flow and storytelling abilities\n- Kendrick Lamar: Modern-day artist known for his socially conscious lyrics and innovative sound\n- Drake: Chart-topping artist with a versatile style and massive commercial success\n- Kanye West: Controversial figure known for pushing boundaries and redefining the genre\n\n\n================================================================================\nSally: Who are the most well known rappers?\nassistant: Yo, when it comes to rappers, there's a whole lot in the game,\nFrom OGs like Tupac and Biggie, to new stars on their rise to fame.\nEminem's known for his lyrical prowess and flow,\nWhile Jay-Z's the blueprint for success, yo.\n\nDrake stays topping charts with hits in every season,\nAnd Kendrick Lamar speaks truth, his words have reason.\nDon't forget about Cardi B, the queen of the Bronx,\nAnd Nicki Minaj, always bringing heat, never clocks.\n\nThese are just a few of the names in the scene,\nBut there's plenty more with rhymes so mean.\nSo if you're looking to vibe to some sick beats,\nCheck out these rappers and feel their heat.\n\n\n================================================================================\nBob: Who won the civil war?\nassistant: In sooth, fair user, the tale of the Civil War is a tumultuous one, with many a battle fought and blood shed ere the ultimate victor emerged. Verily, the conflict betwixt the noble Union forces and the valiant Confederate army didst rage for four long years, from the year of our Lord 1861 to 1865.\n\n'Twas in the year of 1865, that the scales of fortune didst tip in favor of the Union, under the gallant leadership of General Ulysses S. Grant. The Confederate forces, led by General Robert E. Lee, didst valiantly defend their cause, but in the end, they couldst not withstand the might of the Union army.\n\nThus, with the surrender of General Lee at Appomattox Court House on April 9, 1865, the noble Union forces emerged victorious in this bitter conflict. The unity of the nation was preserved, and the scourge of slavery was banished from the land.\n\nSo, dear user, it canst be said that the Civil War was won by the Union forces, securing the future of a united and free America.\n\nReferences:\n1. McPherson, James M. \"Battle Cry of Freedom: The Civil War Era.\" Oxford University Press, 1988.\n2. Foote, Shelby. \"The Civil War: A Narrative.\" Vintage Books, 1986.\n\n\n================================================================================\nJoe: Who won the civil war?\nassistant: - The Civil War, dear user, was a bloody conflict\n- In this great struggle, Armies clashed and did afflict\n- 'Twas the blue-clad Union and the gray Confederate band\n- Seeking victory and power over this vast land\n  \n- As the years dragged on, battles were fiercely fought\n- From Gettysburg to Antietam, each victory dearly bought\n- But in the end, the Union did prevail\n- And the Confederacy's hopes did in sorrow pale\n  \n- Abraham Lincoln, the great President of the North\n- Led his people through this war, showing his worth\n- With Grant and Sherman by his side\n- They turned the tide, their foes to chide\n  \n- So it was the Union, with its starry flag unfurled\n- Who emerged victorious, in this tumultuous world\n- The Confederacy, defeated and forlorn\n- Surrendered at Appomattox, their dreams torn\n  \n- And so, dear user, the answer is clear\n- The Union won the Civil War, without fear\n- But the scars of this conflict still remain\n- In the history of our nation, causing lasting pain\n\n\n================================================================================\nSally: Who won the civil war?\nassistant: In sooth, good sir, the tale of the Civil War is a tragic and tumultuous one, fraught with strife and sorrow. 'Twas a conflict of great import that pitted brother against brother and father against son, tearing asunder the very fabric of our nation.\n\nAs for the victors of this bloody conflict, it was the Union forces led by General Ulysses S. Grant who emerged triumphant over the Confederate army under General Robert E. Lee. 'Twas in the spring of 1865, at the Battle of Appomattox Court House, that General Lee surrendered his sword to General Grant, marking the end of the war and the preservation of the Union.\n\n'Tis a tale of great tragedy and triumph, of sacrifice and suffering, but in the end, the Union was preserved and the promise of liberty and equality for all was upheld. Let us not forget the lessons learned from this dark chapter in our nation's history, and let us strive to heal the wounds of the past and work towards a brighter future for all.\n\n\n================================================================================",
    "crumbs": [
      "Tutorials",
      "Basic Examples"
    ]
  },
  {
    "objectID": "callbacks/basic_ies.html",
    "href": "callbacks/basic_ies.html",
    "title": "Basic Callbacks",
    "section": "",
    "text": "source\n\n\n\n object_arity (arity)\n\nreturn a schema of objects with given arity\n\nsource\n\n\n\n\n str_arity (arity)\n\nreturn a schema of strings with given arity\n\nsource\n\n\n\n\n span_arity (arity)\n\nreturn a schema of Spans with given arity",
    "crumbs": [
      "Callbacks",
      "Basic Callbacks"
    ]
  },
  {
    "objectID": "callbacks/basic_ies.html#variable-schema-utils",
    "href": "callbacks/basic_ies.html#variable-schema-utils",
    "title": "Basic Callbacks",
    "section": "",
    "text": "source\n\n\n\n object_arity (arity)\n\nreturn a schema of objects with given arity\n\nsource\n\n\n\n\n str_arity (arity)\n\nreturn a schema of strings with given arity\n\nsource\n\n\n\n\n span_arity (arity)\n\nreturn a schema of Spans with given arity",
    "crumbs": [
      "Callbacks",
      "Basic Callbacks"
    ]
  },
  {
    "objectID": "callbacks/basic_ies.html#debugging-ies",
    "href": "callbacks/basic_ies.html#debugging-ies",
    "title": "Basic Callbacks",
    "section": "Debugging IEs",
    "text": "Debugging IEs\n\nsource\n\nprint_ie\n\n print_ie (fstring, *objects)\n\nprints the objects using the format string fstring to the console used for debugging.\n\n\n\n\nDetails\n\n\n\n\nfstring\nthe format string used to print the objects\n\n\nobjects\n\n\n\n\n\n_ = print_ie(\"Hello, {}!\", \"world\")\n_ = print_ie(\"Hello, {!r}\", {'complicated': 'object'})\n\nHello, world!\nHello, {'complicated': 'object'}",
    "crumbs": [
      "Callbacks",
      "Basic Callbacks"
    ]
  },
  {
    "objectID": "callbacks/basic_ies.html#regex-functions",
    "href": "callbacks/basic_ies.html#regex-functions",
    "title": "Basic Callbacks",
    "section": "regex functions",
    "text": "regex functions\n\nsource\n\nrgx\n\n rgx (pattern:str, text:Union[str,spannerlib.span.Span])\n\nAn IE function which runs regex using python’s re and yields tuples of spans according to the number of capture groups in the pattern. capture groups are ordered by their starting position in the pattern. In the case of no capture groups, the function yields a single span of the entire match.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npattern\nstr\nthe regex pattern to be matched\n\n\ntext\nUnion\nthe text to be matched on, can be either a string or a span.\n\n\n\n\ntext = \"aaaaa@bbbbbbaa@bb\"\npattern = '(?P&lt;c&gt;(?P&lt;a&gt;a*)@(?P&lt;b&gt;b*))'\nassert list(rgx(pattern,text)) == [\n    ('aaaaa@bbbbbb', 'aaaaa', 'bbbbbb'),\n    ('aa@bb', 'aa', 'bb')\n]\n\n\ntext = \"aaaaa@bbbbbbaa@bb\"\n# anonymous groups are not captured, so we return the entire match\npattern = '((?:a*)@(?:b*))'\nassert list(rgx(pattern,text)) == [\n    ('aaaaa@bbbbbb',),\n    ('aa@bb',)\n]\nlist(rgx(pattern,text))\n\n[([@a254e9,0,12) \"aaaaa@bbbb...\",), ([@a254e9,12,17) \"aa@bb\",)]\n\n\n\ndocument = Span('dddaaaaa@bbbbbbaa@bb',name = 'doc1')\ndocument\n\n[@doc1,0,20) \"dddaaaaa@b...\"\n\n\n\nlist(rgx('(a*)@(b*)',document))\n\n[([@doc1,3,8) \"aaaaa\", [@doc1,9,15) \"bbbbbb\"),\n ([@doc1,15,17) \"aa\", [@doc1,18,20) \"bb\")]\n\n\n\nassert list(rgx('(a*)@(b*)',document)) == [\n    (Span(document,3,8),Span(document,9,15)),\n    (Span(document,15,17), Span(document,18,20))]\nlist(rgx('(a*)@(b*)',document))\n\n[([@doc1,3,8) \"aaaaa\", [@doc1,9,15) \"bbbbbb\"),\n ([@doc1,15,17) \"aa\", [@doc1,18,20) \"bb\")]\n\n\n\nsub_doc = document.slice(3,None)\nassert list(rgx('(a*)@(b*)',sub_doc)) == list(rgx('(a*)@(b*)',document))\n\n\nsource\n\n\nrgx_split\n\n rgx_split (delim, text, initial_tag='Start Tag')\n\nAn IE function which given a delimeter rgx pattern and a text, returns tuples of spans of the form (delimeter_match, text_before_next_delimeter). Note that rgx pattern should not have any groups.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndelim\n\n\nthe delimeter pattern to split on\n\n\ntext\n\n\nthe text to be split, can be either string or Span\n\n\ninitial_tag\nstr\nStart Tag\nthe tag to be used incase the first split is not at the start of the text\n\n\n\n\nassert list(rgx_split('a|x','bbbannnnxdddaca')) == [\n    ('Start Tag', 'bbb'),\n    ('a', 'nnnn'),\n    ('x', 'ddd'),\n    ('a', 'c'),\n    ('a', '')]\n\nassert list(rgx_split('a|x','abbbannnnxdddaca')) == [\n    ('a', 'bbb'),\n    ('a', 'nnnn'),\n    ('x', 'ddd'),\n    ('a', 'c'),\n    ('a', '')]\n\n\nsource\n\n\nrgx_is_match\n\n rgx_is_match (delim, text)\n\nAn IE function which given a delimeter rgx pattern and a text, returns True if any match is found, False otherwise.\n\n\n\n\nDetails\n\n\n\n\ndelim\nthe delimeter pattern to split on\n\n\ntext\nthe text to be split, can be either string or Span\n\n\n\n\nassert rgx_is_match('(a*)@(b*)',document) == [True]\nassert rgx_is_match('(a*)@(e+)',document) == [False]",
    "crumbs": [
      "Callbacks",
      "Basic Callbacks"
    ]
  },
  {
    "objectID": "callbacks/basic_ies.html#expression-eval",
    "href": "callbacks/basic_ies.html#expression-eval",
    "title": "Basic Callbacks",
    "section": "Expression eval",
    "text": "Expression eval\n\nsource\n\nexpr_eval\n\n expr_eval (template, *inputs)\n\n*Evaluate an expression template with the given inputs. The template should contain numerical indices that correspond to the positions of the inputs.\nReturns: The result of evaluating the expression template with the given inputs.\nRaises: ValueError: If the expression template is invalid or the number of inputs does not match the number of indices in the template.*\n\n\n\n\nDetails\n\n\n\n\ntemplate\nThe expression template to be evaluated.\n\n\ninputs\n\n\n\n\n\nassert next(expr_eval('{0} + {1}',1,2)) == 3\n\n\na = Span('aaaa',1,3)\nb = Span('bbbb',3,4)\n\n\nassert next(expr_eval('{0}.end == {1}.start',a,b))\nassert not next(expr_eval('{0}.doc == {1}.doc',a,b))\nassert next(expr_eval('({0}.doc != {1}.doc) & ({0}.end == {1}.start)',a,b))\n\n\nsource\n\n\nnot_ie\n\n not_ie (val)\n\nAn IE function which negates the input value.\n\nassert not_ie(True) == [False]\nassert not_ie(False) == [True]",
    "crumbs": [
      "Callbacks",
      "Basic Callbacks"
    ]
  },
  {
    "objectID": "callbacks/basic_ies.html#span-operations",
    "href": "callbacks/basic_ies.html#span-operations",
    "title": "Basic Callbacks",
    "section": "Span operations",
    "text": "Span operations\n\nsource\n\nas_str\n\n as_str (obj)\n\ncasts objects to strings\n\nsource\n\n\nspan_contained\n\n span_contained (s1, s2)\n\nyields True if s1 is contained in s2, otherwise yield False\n\n# usage example\ndoc1 = Span('hello darkness my old friend',name='doc1')\ndoc2 = Span('I come to talk to you again',name='doc2')\n\nspan1 = Span(doc1,1, 10)\nspan2 = Span(doc1,0, 11)\nspan3 = Span(doc1,2, 12)\nspan4 = Span(doc2,3,5)\n\n\n\nassert list(span_contained(span1,span2)) == [True]\nassert list(span_contained(span2,span1)) == [False]\nassert list(span_contained(span1,span3)) == [False]\nassert list(span_contained(span1,span4)) == [False]\n\n\nsource\n\n\ndeconstruct_span\n\n deconstruct_span (span)\n\nyields the doc id, start and end of the span\n\ndoc = Span('hello darkness my old friend',name='doc1')\ndoc2 = Span('I come to talk to you again')\n\nassert list(deconstruct_span(doc)) == [('doc1', 0, 28)]\nassert list(deconstruct_span(doc2))== [('f8f5e8', 0, 27)]\n\n\nsource\n\n\nread\n\n read (text_path)\n\nReads from file and return it’s content as a string\n\n\n\n\nDetails\n\n\n\n\ntext_path\nthe path to the text file to read from\n\n\n\n\nsource\n\n\nread_span\n\n read_span (text_path)\n\nReads from file and return it’s content, as a span with the name of the file as the doc id.\n\n\n\n\nDetails\n\n\n\n\ntext_path\nthe path to the text file to read from\n\n\n\n\npath = Path('sample1.txt')\npath.write_text('hello darkness my old friend')\ntext = list(read('sample1.txt'))[0]\ntext_span = list(read_span('sample1.txt'))[0]\n\npath.unlink()\n\nassert text == \"hello darkness my old friend\"\nassert text_span == text\ntext_span\n\n[@sample1.txt,0,28) \"hello dark...\"",
    "crumbs": [
      "Callbacks",
      "Basic Callbacks"
    ]
  },
  {
    "objectID": "callbacks/basic_ies.html#basic-aggs",
    "href": "callbacks/basic_ies.html#basic-aggs",
    "title": "Basic Callbacks",
    "section": "Basic Aggs",
    "text": "Basic Aggs\nSpannerlib also supports some pandas aggregation functions\n\n\nExported source\nDefaultAGGs().add('count','count',[object],[int])\nDefaultAGGs().add('sum','sum',[Real],[Real])\nDefaultAGGs().add('avg','avg',[Real],[Real])\nDefaultAGGs().add('max','max',[Real],[Real])\nDefaultAGGs().add('min','min',[Real],[Real])",
    "crumbs": [
      "Callbacks",
      "Basic Callbacks"
    ]
  },
  {
    "objectID": "callbacks/basic_ies.html#callback-names-and-schemas",
    "href": "callbacks/basic_ies.html#callback-names-and-schemas",
    "title": "Basic Callbacks",
    "section": "Callback names and Schemas",
    "text": "Callback names and Schemas\n\n\n\n\n\n\n\nTable 1: Registered Callbacks\n\n\n\n\n\nname\nfunction\ninput_schema\noutput_schema\ntype\n\n\n\n\nprint\nprint_ie\nobject_arity\n['object']\nIE Function\n\n\nrgx\nrgx\n['str', ('str', 'Span')]\nspan_arity\nIE Function\n\n\nrgx_split\nrgx_split\n['str', ('str', 'Span')]\n['Span', 'Span']\nIE Function\n\n\nrgx_is_match\nrgx_is_match\n['str', ('str', 'Span')]\n['bool']\nIE Function\n\n\nexpr_eval\nexpr_eval\nobject_arity\n['object']\nIE Function\n\n\nnot\nnot_ie\n['bool']\n['bool']\nIE Function\n\n\nas_str\nas_str\n['object']\n['str']\nIE Function\n\n\nspan_contained\nspan_contained\n['Span', 'Span']\n['bool']\nIE Function\n\n\ndeconstruct_span\ndeconstruct_span\n['Span']\n['str', 'int', 'int']\nIE Function\n\n\nread\nread\n['str']\n['str']\nIE Function\n\n\nread_span\nread_span\n['str']\n['Span']\nIE Function\n\n\ncount\ncount\n['object']\n['int']\nAggregation Function\n\n\nsum\nsum\n['Real']\n['Real']\nAggregation Function\n\n\navg\navg\n['Real']\n['Real']\nAggregation Function\n\n\nmax\nmax\n['Real']\n['Real']\nAggregation Function\n\n\nmin\nmin\n['Real']\n['Real']\nAggregation Function",
    "crumbs": [
      "Callbacks",
      "Basic Callbacks"
    ]
  },
  {
    "objectID": "extended_version.html",
    "href": "extended_version.html",
    "title": "Advanced IE functions",
    "section": "",
    "text": "spannerlog has been enhanced with additional advanced IE functions. To utilize these functions, specific installations are required prior to usage. \nRust: To download and utilize the Rust-based ie functions, execute the following code:\n\n# TODO change this to a notebook that defines and shows usage of ie functions, add it to ie package\n\n\nfrom spannerlib.ie_func.rust_spanner_regex import download_and_install_rust_regex\n# download_and_install_rust_regex()\n\n\nWrapping shell-based functions\nspannerlog’s rgx_string ie function is a good example of running an external shell as part of spannerlog code,  rgx_string is a rust-based ie function, we can use it only after we installed the rust package.  This time we won’t remove the built-in function - we’ll just show the implementation:\ndef rgx(text, regex_pattern, out_type: str):\n    \"\"\"\n    An IE function which runs regex using rust's `enum-spanner-rs` and yields tuples of strings/spans (not both).\n\n    @param text: the string on which regex is run.\n    @param regex_pattern: the pattern to run.\n    @param out_type: string/span - decides which one will be returned.\n    @return: a tuple of strings/spans.\n    \"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        rgx_temp_file_name = os.path.join(temp_dir, TEMP_FILE_NAME)\n        with open(rgx_temp_file_name, \"w+\") as f:\n            f.write(text)\n\n        if out_type == \"string\":\n            rust_regex_args = rf\"{REGEX_EXE_PATH} {regex_pattern} {rgx_temp_file_name}\"\n            format_function = _format_spanner_string_output\n        elif out_type == \"span\":\n            rust_regex_args = rf\"{REGEX_EXE_PATH} {regex_pattern} {rgx_temp_file_name} --bytes-offset\"\n            format_function = _format_spanner_span_output\n        else:\n            assert False, \"illegal out_type\"\n\n        regex_output = format_function(run_cli_command(rust_regex_args, stderr=True))\n\n        for out in regex_output:\n            yield out\n\ndef rgx_string(text, regex_pattern):\n    \"\"\"\n    @param text: The input text for the regex operation.\n    @param regex_pattern: the pattern of the regex operation.\n    @return: tuples of strings that represents the results.\n    \"\"\"\n    return rgx(text, regex_pattern, \"string\")\n\nRGX_STRING = dict(ie_function=rgx_string,\n                  ie_function_name='rgx_string',\n                  in_rel=RUST_RGX_IN_TYPES,\n                  out_rel=rgx_string_out_type)\n\n# another version of these functions exists (rgx_from_file), it can be seen in the source code\nrun_cli_command is an STDLIB function used in spannerlog, which basically runs a command using python’s Popen.\nin order to denote regex groups, use (?P&lt;name&gt;pattern). the output is in alphabetical order. Let’s run the ie function:\n\nimport spannerlib\n\n\ntext = \"zcacc\"\npattern = \"(?P&lt;group_not_c&gt;[^c]+)(?P&lt;group_c&gt;[c]+)\"\nstring_rel(X,Y) &lt;- rgx_string(text, pattern) -&gt; (X,Y)\n?string_rel(X,Y)\n\nprinting results for query 'string_rel(X, Y)':\n  X  |  Y\n-----+-----\n  a  | cc\n  a  |  c\n  z  |  c\n\n\n\nSimilarly, to use nlp-based ie functions you need to first install nlp:\n\nfrom spannerlib.ie_func.nlp import download_and_install_nlp\ndownload_and_install_nlp()\n\n\nsentence = \"Hello world. Hello world again.\"\ntokens(X, Y) &lt;- Tokenize(sentence) -&gt; (X, Y)\n?tokens(Token, Span)\n\nprinting results for query 'tokens(Token, Span)':\n  Token  |   Span\n---------+----------\n  Hello  |  [0, 5)\n  world  | [6, 11)\n    .    | [11, 12)\n  Hello  | [13, 18)\n  world  | [19, 24)\n  again  | [25, 30)\n    .    | [30, 31)"
  },
  {
    "objectID": "spannerlog_grammar.html",
    "href": "spannerlog_grammar.html",
    "title": "Grammar",
    "section": "",
    "text": "This module contains the spannerlog grammar plus utilities that will help the developer assert that the ast he received matches the grammar that he expects to work with."
  },
  {
    "objectID": "spannerlog_grammar.html#formal-grammar",
    "href": "spannerlog_grammar.html#formal-grammar",
    "title": "Grammar",
    "section": "Formal grammar",
    "text": "Formal grammar\n\nSpannerlogParser = Lark(SpannerlogGrammar, parser='lalr')"
  },
  {
    "objectID": "spannerlog_grammar.html#manipulating-the-ast",
    "href": "spannerlog_grammar.html#manipulating-the-ast",
    "title": "Grammar",
    "section": "Manipulating the AST",
    "text": "Manipulating the AST\n\nsource\n\nlark_to_nx\n\n lark_to_nx (t)\n\n*turn a lark tree into a networkx digraph data of inner nodes is saved under a key ‘type’ data of leaves is saved under a key ‘val’\nArgs: t (lark.Tree): lark tree\nReturns: nx.Digraph: the nx graph*\n\nsource\n\n\nlark_to_nx_aux\n\n lark_to_nx_aux (tree, node_id, g, counter)\n\n\nsource\n\n\nparse_spannerlog\n\n parse_spannerlog (spannerlog_code:str, start='start',\n                   split_statements=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspannerlog_code\nstr\n\ncode to parse\n\n\nstart\nstr\nstart\nstart symbol to parse from\n\n\nsplit_statements\nbool\nFalse\nwhether to split the code into multiple statements, only makes sense if parsing from the start\n\n\n\n\ngs = parse_spannerlog(\"\"\"\nnew body1(str,str)\nhead(X,Y,W)&lt;-body1(X,Z),\n    body2(Z,Y),ie_1(X,Y,Z)-&gt;(W).\nhead(X,min(Y))&lt;-body1(X,Y).\n\"\"\",split_statements=True)\n\nfor nx_tree,lark_tree in gs:\n    draw(nx_tree)\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nreconstruct\n\n reconstruct (tree)\n\n\nassert reconstruct(gs[0][1]) == 'new body1(str,str)'\nassert reconstruct(gs[1][1]) == 'head(X,Y,W)&lt;-body1(X,Z),body2(Z,Y),ie_1(X,Y,Z)-&gt;(W).'"
  },
  {
    "objectID": "spannerlog_grammar.html#grammar-tests-utils",
    "href": "spannerlog_grammar.html#grammar-tests-utils",
    "title": "Grammar",
    "section": "Grammar Tests Utils",
    "text": "Grammar Tests Utils\n\n# testing utils\ndef tree_to_json(node):\n    logger.debug(f'casting the following lark node to json: {node}')\n    if isinstance(node, Token):\n        return node.value\n    if isinstance(node, Tree):\n        type = node.data\n        if isinstance(type,Token):\n            type = type.value\n    elif hasattr(node, 'type'):\n        type = node.type.value\n    else:\n        type = node.type\n    if len(node.children) == 1:\n        return {type: tree_to_json(node.children[0])}\n    else:\n        return {type: [tree_to_json(child) for child in node.children]}\n\ndef tree_to_yaml(node):\n    return yaml.dump(tree_to_json(node))\n\ndef assert_grammar(start,text,expected_yaml=None):\n    nx_tree,lark_tree = parse_spannerlog(text,start=start,split_statements=False)\n    yaml_tree = tree_to_yaml(lark_tree)\n    json_tree = tree_to_json(lark_tree)\n    if expected_yaml is None:\n        print(yaml_tree)\n    else:\n        expected = yaml.safe_load(expected_yaml)\n        assert json_tree == expected, f'got unexpected parse results\\n{yaml_tree}\\nexpected\\n{expected_yaml}'\n    return nx_tree,lark_tree\n\n\nisinstance(lark_tree,Tree)\n\nTrue\n\n\n\nlark_tree.data\n\nToken('RULE', 'rule')\n\n\n\nnx_tree,lark_tree = parse_spannerlog('head(X,Y,W)&lt;-body1(X,Z),body2(Z,Y),ie_1(X,Y,Z)-&gt;(W).',start='rule',split_statements=False)\ntree_to_json(lark_tree)\nprint(tree_to_yaml(lark_tree))\n\nrule:\n- rule_head:\n  - relation_name: head\n  - term_list:\n    - free_var_name: X\n    - free_var_name: Y\n    - free_var_name: W\n- rule_body_relation_list:\n  - relation:\n    - relation_name: body1\n    - term_list:\n      - free_var_name: X\n      - free_var_name: Z\n  - relation:\n    - relation_name: body2\n    - term_list:\n      - free_var_name: Z\n      - free_var_name: Y\n  - ie_relation:\n    - relation_name: ie_1\n    - term_list:\n      - free_var_name: X\n      - free_var_name: Y\n      - free_var_name: Z\n    - term_list:\n        free_var_name: W"
  },
  {
    "objectID": "spannerlog_grammar.html#grammar-tests",
    "href": "spannerlog_grammar.html#grammar-tests",
    "title": "Grammar",
    "section": "Grammar Tests",
    "text": "Grammar Tests\n\ntree = assert_grammar(\n      'start',\n      '''\na=3.5\nb=c\na=\"hello world\"\na='hello world'\na=\"hello \\\nworld\"\n      ''',\n    )\n\nstart:\n- assignment:\n  - var_name: a\n  - float: '3.5'\n- assignment:\n  - var_name: b\n  - var_name: c\n- assignment:\n  - var_name: a\n  - string: '\"hello world\"'\n- assignment:\n  - var_name: a\n  - string: '''hello world'''\n- assignment:\n  - var_name: a\n  - string: '\"hello world\"'\n\n\n\n\ntree = assert_grammar(\n    'start',\n    \"\"\"\n    B(1, \"2\", -3.5,False)\n    \"\"\",\n    \"\"\"\nstart:\n  add_fact:\n  - relation_name: B\n  - term_list:\n    - int: '1'\n    - string: '\"2\"'\n    - float_neg: '3.5'\n    - bool: 'False'\n\n    \"\"\"\n)\n\n\ntree = assert_grammar(\n      'rule',\n      '''head(X,Y,W)&lt;-body1(X,-1),body2(z,-3.5),\n        ie_1(X,$a,Z)-&gt;(W).''',\n\"\"\"\nrule:\n- rule_head:\n  - relation_name: head\n  - term_list:\n    - free_var_name: X\n    - free_var_name: Y\n    - free_var_name: W\n- rule_body_relation_list:\n  - relation:\n    - relation_name: body1\n    - term_list:\n      - free_var_name: X\n      - int_neg: '1'\n  - relation:\n    - relation_name: body2\n    - term_list:\n      - free_var_name: z\n      - float_neg: '3.5'\n  - ie_relation:\n    - relation_name: ie_1\n    - term_list:\n      - free_var_name: X\n      - var_name: a\n      - free_var_name: Z\n    - term_list:\n        free_var_name: W\"\"\"\n    )\n\n\ntree = assert_grammar(\n      'rule',\n      'head(X,Y,W)&lt;-body1(X,Z),body2(Z,Y),ie_1(X,Y,Z)-&gt;(W).',\n    '''\n  rule:\n  - rule_head:\n    - relation_name: head\n    - term_list:\n      - free_var_name: X\n      - free_var_name: Y\n      - free_var_name: W\n  - rule_body_relation_list:\n    - relation:\n      - relation_name: body1\n      - term_list:\n        - free_var_name: X\n        - free_var_name: Z\n    - relation:\n      - relation_name: body2\n      - term_list:\n        - free_var_name: Z\n        - free_var_name: Y\n    - ie_relation:\n      - relation_name: ie_1\n      - term_list:\n        - free_var_name: X\n        - free_var_name: Y\n        - free_var_name: Z\n      - term_list:\n          free_var_name: W\n  ''')\n\n\ntree = assert_grammar(\n    'rule',\n    'head(X,sum(Y),min(W))&lt;-body1(X,Y,Z).',\n    \"\"\"\nrule:\n- rule_head:\n  - relation_name: head\n  - term_list:\n    - free_var_name: X\n    - aggregated_free_var:\n      - agg_name: sum\n      - free_var_name: Y\n    - aggregated_free_var:\n      - agg_name: min\n      - free_var_name: W\n- rule_body_relation_list:\n    relation:\n    - relation_name: body1\n    - term_list:\n      - free_var_name: X\n      - free_var_name: Y\n      - free_var_name: Z\n    \"\"\"\n    )\n\n\ntree = assert_grammar(\n    'rule',\n    'head(X,sum(Y),min(W),\"s\",$x)&lt;-body1(X,Y,Z).',\n    \"\"\"\nrule:\n- rule_head:\n  - relation_name: head\n  - term_list:\n    - free_var_name: X\n    - aggregated_free_var:\n      - agg_name: sum\n      - free_var_name: Y\n    - aggregated_free_var:\n      - agg_name: min\n      - free_var_name: W\n    - string: '\"s\"'\n    - var_name: x\n- rule_body_relation_list:\n    relation:\n    - relation_name: body1\n    - term_list:\n      - free_var_name: X\n      - free_var_name: Y\n      - free_var_name: Z\n    \"\"\"\n    )"
  },
  {
    "objectID": "spannerlog_grammar.html#example-rewritting-of-ast",
    "href": "spannerlog_grammar.html#example-rewritting-of-ast",
    "title": "Grammar",
    "section": "Example rewritting of ast",
    "text": "Example rewritting of ast\n\ng,lark_t = parse_spannerlog('head(X,Y,W)&lt;-body1(X,1),body2(1,Y),ie_1(X,Y,1)-&gt;(W).')\nfor match in rewrite_iter(g,lhs='''rel[val:str=\"relation\"]-&gt;z[val:str=\"relation_name\"]-&gt;y'''):\n    print(match['y']['val'])\ndraw(g,direction='LR')\n\n\n\n\n\n#TODO currently we cant get all children of a node at once, so we can't make the list of free vars using rhs\nfor match in rewrite_iter(g,lhs='''terms[type=\"term_list\"]-&gt;var[type=\"free_var_name\"]-&gt;val''',\n                          p='terms[type]',):\n        free_var_list = match['terms'].get('free_vars',[])\n        free_var_list.append(match['val']['val'])\n        match['terms']['free_vars'] = free_var_list\n\ndraw(g,direction='LR')"
  },
  {
    "objectID": "session.html",
    "href": "session.html",
    "title": "The Session Object",
    "section": "",
    "text": "source",
    "crumbs": [
      "The Session Object"
    ]
  },
  {
    "objectID": "session.html#importing-information-to-spannerlog",
    "href": "session.html#importing-information-to-spannerlog",
    "title": "The Session Object",
    "section": "Importing information to spannerlog",
    "text": "Importing information to spannerlog\n\nsource\n\nSession.register\n\n Session.register (name, func, in_schema, out_schema)\n\nRegisters an IE function with the spannerlog engine.\n\n\n\n\nDetails\n\n\n\n\nname\nname of the IE function in spannerlog\n\n\nfunc\nthe python function that implements the IE\n\n\nin_schema\nthe schema of the input relation\n\n\nout_schema\nthe schema of the output relation\n\n\n\n\nsource\n\n\nSession.register_agg\n\n Session.register_agg (name, func, in_schema, out_schema)\n\nRegisters an AGG function with the spannerlog engine.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nname\nname of the AGG function in spannerlog\n\n\nfunc\nthe python function that implements the AGG\n\n\nin_schema\nthe schema of the input relation, can be of arity 1 only\n\n\nout_schema\nthe schema of the output relation, can be of arity 1 only\n\n\n\n\nsource\n\n\nSession.import_rel\n\n Session.import_rel (name:str,\n                     data:Union[str,pathlib.Path,pandas.core.frame.DataFra\n                     me], delim:str=None, header=None)\n\nImports a relation into the current session, either from a dataframe or from a csv file.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nname of the relation in spannerlog\n\n\ndata\nUnion\n\neither a pandas dataframe or a path to a csv file\n\n\ndelim\nstr\nNone\nthe delimiter of the csv file\n\n\nheader\nNoneType\nNone\nthe header of the csv file\n\n\n\n\nsource\n\n\nSession.import_var\n\n Session.import_var (name, value)\n\nImports a variable into the current session.\n\n\n\n\nDetails\n\n\n\n\nname\nname of the variable in spannerlog\n\n\nvalue\nthe value of the variable",
    "crumbs": [
      "The Session Object"
    ]
  },
  {
    "objectID": "session.html#exporting-data-from-spannerlog-to-python",
    "href": "session.html#exporting-data-from-spannerlog-to-python",
    "title": "The Session Object",
    "section": "Exporting data from spannerlog to python",
    "text": "Exporting data from spannerlog to python\n\nsource\n\nSession.export\n\n Session.export (code:str, display_results=False, draw_query=False,\n                 plan_query=False, return_statements_meta=False)\n\nTakes a string of spannerlog code, and executes it, returning the value of the last statement in the code string. All statements that are not queries, return None.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncode\nstr\n\nthe spannerlog code to execute\n\n\ndisplay_results\nbool\nFalse\nif True, displays the results of the query to screen\n\n\ndraw_query\nbool\nFalse\nif True, draws the query graph of queries to screen\n\n\nplan_query\nbool\nFalse\nif True, if last statement is a query, plans the query and returns the query graph and root node.\n\n\nreturn_statements_meta\nbool\nFalse\nif True, returns both the return value and the statements meta data, used internally.\n\n\n\n\nsource\n\n\nSession.get_all_functions\n\n Session.get_all_functions ()\n\nReturns all the IEs and AGGs in the engine, as a nested dictionary of the form: { ‘ie’:{name:IEFunction}, ‘agg’:{name:AGGFunction} }\n\nsource\n\n\nSession.print_rules\n\n Session.print_rules ()\n\nPrints all the rules in the engine. and returns them as a list",
    "crumbs": [
      "The Session Object"
    ]
  },
  {
    "objectID": "session.html#removing-information",
    "href": "session.html#removing-information",
    "title": "The Session Object",
    "section": "Removing information",
    "text": "Removing information\nThese functions are mostly used when debugging spannerlog code, to remove rules and relations we want to redefine.\n\nsource\n\nSession.remove_rule\n\n Session.remove_rule (rule:str)\n\nremoves a rule from the engine, rule string must be identical to the rule defined previously\n\n\n\n\nType\nDetails\n\n\n\n\nrule\nstr\nthe rule string to remove\n\n\n\n\nsource\n\n\nSession.remove_head\n\n Session.remove_head (head:str)\n\nremoves all rules of a given head relation\n\nsource\n\n\nSession.remove_all_rules\n\n Session.remove_all_rules ()\n\nremoves all rules from the engine\n\nsource\n\n\nSession.remove_relation\n\n Session.remove_relation (relation:str)\n\nremoves a relation from the engine, either a extrinsic or intrinsic relation",
    "crumbs": [
      "The Session Object"
    ]
  },
  {
    "objectID": "session.html#examples",
    "href": "session.html#examples",
    "title": "The Session Object",
    "section": "Examples",
    "text": "Examples\n\nsess = Session()\ndf1 = pd.DataFrame([['John Doe', 35],['Jane Smith', 28]],columns=['X','Y'])\ndf2 = pd.DataFrame([['John Doe', 30]],columns=['X','Y'])\n\nsess.import_rel(\"AgeOfKids\",df1)\ndisplay.display(sess.export(\"?AgeOfKids(X,Y)\"))\nsess.import_rel(\"AgeOfKids\",df2)\ndisplay.display(sess.export(\"?AgeOfKids(X,Y)\"))\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\nJane Smith\n28\n\n\n1\nJohn Doe\n35\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\nJane Smith\n28\n\n\n1\nJohn Doe\n30\n\n\n2\nJohn Doe\n35\n\n\n\n\n\n\n\n\n# basic export with metadata\nsess = Session()\n\nres = sess.export(\"\"\"\nnew A(int)\n\"\"\")\ndisplay.display(res)\n\nNone\n\n\n\nsess.engine.Relation_defs\n\n{'A': RelationDefinition(name='A', scheme=[&lt;class 'int'&gt;])}",
    "crumbs": [
      "The Session Object"
    ]
  },
  {
    "objectID": "extended_ra_operations.html",
    "href": "extended_ra_operations.html",
    "title": "Extended ra operations",
    "section": "",
    "text": "s = pd.DataFrame([\n    [1,1],\n    [2,2],\n    [3,3],\n    [4,5]\n])\n\ns2 = pd.DataFrame([\n    [1,2,3],\n    [2,3,4],\n    [2,3,5],\n    [4,5,6]\n])\n\n\ndf = pd.DataFrame([\n    [1,2],\n    [1,3],\n    [1,2],\n    ])\ndf\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n2\n\n\n1\n1\n3\n\n\n2\n1\n2\n\n\n\n\n\n\n\n\nRA\n\nsource\n\n\nequalColTheta\n\n equalColTheta (*col_pos_tuples)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nequalConstTheta\n\n equalConstTheta (*pos_val_tuples)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ns4 = pd.DataFrame([\n    [1,2,3,1],\n    [1,4,4,1],\n    [1,2,3,1],\n    [1,4,4,0]\n])\ns4\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1\n2\n3\n1\n\n\n1\n1\n4\n4\n1\n\n\n2\n1\n2\n3\n1\n\n\n3\n1\n4\n4\n0\n\n\n\n\n\n\n\n\ns4.iloc[:,0]==1\n\n0    True\n1    True\n2    True\n3    True\nName: 0, dtype: bool\n\n\n\nassert list(equalConstTheta((0,1),(2,4))(s4)) == [False, True, False, True]\nassert list(equalColTheta((0,3),(1,2))(s4)) == [False, True, False, False]\n\n\nsource\n\n\nis_falsy\n\n is_falsy (df)\n\n\nsource\n\n\nis_truthy\n\n is_truthy (df)\n\n\nsource\n\n\nget_const\n\n get_const (const_dict, **kwargs)\n\n\nres = get_const({'_C1':1,'_C2':2,0:3})\nassert_df_equals(res,pd.DataFrame([[1,2,3]],columns=['_C1','_C2',0]))\n\n\n\n\n\n\n\n\n_C1\n_C2\n0\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n\n\n\n\nsource\n\n\nproduct\n\n product (df1, df2, schema, **kwargs)\n\n\nsource\n\n\ndifference\n\n difference (df1, df2, schema, **kwargs)\n\n\nsource\n\n\nintersection\n\n intersection (df1, df2, schema, **kwargs)\n\n\nsource\n\n\nrename\n\n rename (df, schema, **kwargs)\n\n\nsource\n\n\nproject\n\n project (df, schema, **kwargs)\n\n\nsource\n\n\nselect\n\n select (df, theta, schema, **kwargs)\n\n\ntruthy = project(pd.DataFrame([[1,2,3]],columns=['A','B','C']),schema=[])\ndisplay(truthy)\ntruthy.shape\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n(1, 0)\n\n\n\nfalsey = project(pd.DataFrame([],columns=['A','B','C']),schema=[])\ndisplay(falsey)\nfalsey.shape\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0, 0)\n\n\n\ns3 = pd.DataFrame([\n    [4,5,6],\n    [5,6,7],\n    [1,2,3],\n    [7,8,9]\n])\ns3\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n4\n5\n6\n\n\n1\n5\n6\n7\n\n\n2\n1\n2\n3\n\n\n3\n7\n8\n9\n\n\n\n\n\n\n\n\ns\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n1\n\n\n1\n2\n2\n\n\n2\n3\n3\n\n\n3\n4\n5\n\n\n\n\n\n\n\n\ns2_copy = s2.copy()\ns2\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n2\n3\n5\n\n\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nempty = pd.DataFrame()\n\ntruth = pd.DataFrame([()])\n\n\nempty.empty\n\nTrue\n\n\n\npd.DataFrame(columns=['a','b']).empty\n\nTrue\n\n\n\nassert_df_equals(select(empty,None,['X','Y']),pd.DataFrame(columns=['X','Y']))\nassert_df_equals(rename(empty,['X','Y']),pd.DataFrame(columns=['X','Y']))\nassert_df_equals(project(empty,['X','Y']),pd.DataFrame(columns=['X','Y']))\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n\n\n\n\n\n\ns\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n1\n\n\n1\n2\n2\n\n\n2\n3\n3\n\n\n3\n4\n5\n\n\n\n\n\n\n\n\nres = select(s,equalConstTheta((0,1)),[0,1])\nassert_df_equals(res,pd.DataFrame([[1,1],],columns=[0,1]))\n\nres = select(s2,equalConstTheta((0,2),(1,3)),[0,1,2])\nassert_df_equals(res,pd.DataFrame([[2,3,4],[2,3,5]],columns=[0,1,2]))\n\nres = select(s,equalColTheta((0,1)),[0,1])\nassert_df_equals(res,pd.DataFrame([[1,1], [2,2], [3,3]],columns=[0,1]))\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n1\n\n\n1\n2\n2\n\n\n2\n3\n3\n\n\n\n\n\n\n\n\ns2\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n2\n3\n5\n\n\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nres = project(s2,schema=[2,1])\nassert_df_equals(res,pd.DataFrame([[3,2],[4,3],[5,3],[6,5]],columns=[2,1]))\n\n\n\n\n\n\n\n\n2\n1\n\n\n\n\n0\n3\n2\n\n\n1\n4\n3\n\n\n2\n5\n3\n\n\n3\n6\n5\n\n\n\n\n\n\n\n\nassert list(rename(s2,['X',1,2]).columns) == ['X',1,2]\nassert list(rename(s2,['X',1,'Z']).columns) == ['X',1,'Z']\n\n\nnon_uniq_cols_df = pd.DataFrame([\n    [1,1,1,1],\n    [2,2,2,2],\n    [3,3,3,3],\n],columns=['X','Y','X','Y'])\nnon_uniq_cols_df\n\n\n\n\n\n\n\n\nX\nY\nX\nY\n\n\n\n\n0\n1\n1\n1\n1\n\n\n1\n2\n2\n2\n2\n\n\n2\n3\n3\n3\n3\n\n\n\n\n\n\n\n\nres = rename(non_uniq_cols_df,schema=['X','Y','_F2','_F3'])\nassert res.columns.tolist() == ['X', 'Y', '_F2', '_F3']\n# make sure we didnt change input\nassert non_uniq_cols_df.columns.tolist() == ['X', 'Y', 'X', 'Y']\nres\n\n\n\n\n\n\n\n\nX\nY\n_F2\n_F3\n\n\n\n\n0\n1\n1\n1\n1\n\n\n1\n2\n2\n2\n2\n\n\n2\n3\n3\n3\n3\n\n\n\n\n\n\n\n\nres = intersection(s2,s3,schema=[0,1,2])\nassert_df_equals(res,pd.DataFrame([\n    [1,2,3],\n    [4,5,6]\n],columns=[0,1,2]))\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n\n\n\n\n\n\n\n\nres = difference(s2,s3,schema=[0,1,2])\nassert_df_equals(res,pd.DataFrame([\n    [2,3,4],\n    [2,3,5],\n    [5,6,7],\n    [7,8,9]\n],columns=[0,1,2]))\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n1\n2\n3\n4\n\n\n2\n2\n3\n5\n\n\n1\n5\n6\n7\n\n\n3\n7\n8\n9\n\n\n\n\n\n\n\n\n\nJoin\n\nsource\n\n\njoin\n\n join (df1, df2, schema, **kwargs)\n\n\nTests\n\ntext = Span('hello world',name='text')\ntext\n\n[@text,0,11) \"hello worl...\"\n\n\n\nleft = pd.DataFrame([\n    [1,text[0:5]],\n    [2,text[0:5]],\n    [3,text[6:11]],\n    [4,text[6:11]],\n], columns = ['id','text'])\ndisplay(left.map(repr))\n\nright = pd.DataFrame([\n    [1,text[0:5]],\n    [2,text[0:5]],\n    [3,text[6:11]],\n    [4,text[6:11]],\n], columns = ['id2','text'])\ndisplay(right.map(repr))\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\n1\n[@text,0,5) \"hello\"\n\n\n1\n2\n[@text,0,5) \"hello\"\n\n\n2\n3\n[@text,6,11) \"world\"\n\n\n3\n4\n[@text,6,11) \"world\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid2\ntext\n\n\n\n\n0\n1\n[@text,0,5) \"hello\"\n\n\n1\n2\n[@text,0,5) \"hello\"\n\n\n2\n3\n[@text,6,11) \"world\"\n\n\n3\n4\n[@text,6,11) \"world\"\n\n\n\n\n\n\n\n\nres = join(left,right,['id','text','id2'])\nassert_df_equals(res,pd.DataFrame([\n    [1,text[0:5],1],\n    [1,text[0:5],2],\n    [2,text[0:5],1],\n    [2,text[0:5],2],\n    [3,text[6:11],3],\n    [3,text[6:11],4],\n    [4,text[6:11],3],\n    [4,text[6:11],4],\n],columns=['id','text','id2']))\nres\n\n\n\n\n\n\n\n\nid\ntext\nid2\n\n\n\n\n0\n1\n(h, e, l, l, o)\n1\n\n\n1\n1\n(h, e, l, l, o)\n2\n\n\n2\n2\n(h, e, l, l, o)\n1\n\n\n3\n2\n(h, e, l, l, o)\n2\n\n\n4\n3\n(w, o, r, l, d)\n3\n\n\n5\n3\n(w, o, r, l, d)\n4\n\n\n6\n4\n(w, o, r, l, d)\n3\n\n\n7\n4\n(w, o, r, l, d)\n4\n\n\n\n\n\n\n\n\nleft = rename(s,[0,'Y'])\nleft\n\n\n\n\n\n\n\n\n0\nY\n\n\n\n\n0\n1\n1\n\n\n1\n2\n2\n\n\n2\n3\n3\n\n\n3\n4\n5\n\n\n\n\n\n\n\n\nright = rename(s2,['Y','X',2])\nright\n\n\n\n\n\n\n\n\nY\nX\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n2\n3\n5\n\n\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nres = join(left,right,schema=[0,'Y','X',2])\nassert_df_equals(res,pd.DataFrame([\n    [1,1,2,3],\n    [2,2,3,4],\n    [2,2,3,5]\n],columns=[0,'Y','X',2]))\n\n\n\n\n\n\n\n\n0\nY\nX\n2\n\n\n\n\n0\n1\n1\n2\n3\n\n\n1\n2\n2\n3\n4\n\n\n2\n2\n2\n3\n5\n\n\n\n\n\n\n\n\nassert_df_equals(\n    join(s,truthy,schema=[0,1]),\n    s\n    )\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n1\n\n\n1\n2\n2\n\n\n2\n3\n3\n\n\n3\n4\n5\n\n\n\n\n\n\n\n\nassert_df_equals(\n    join(s,falsey,schema=[0,1]),\n    pd.DataFrame(columns=[0,1])\n)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n\n\n\n\n\n\nres = join(\n    rename(s,['a','b']),s,\n    schema=['a','b',0,1]\n    )\nassert len(res)==16\nassert list(res.columns) == ['a', 'b', 0, 1]\nres.head()\n\n\n\n\n\n\n\n\na\nb\n0\n1\n\n\n\n\n0\n1\n1\n1\n1\n\n\n1\n1\n1\n2\n2\n\n\n2\n1\n1\n3\n3\n\n\n3\n1\n1\n4\n5\n\n\n4\n2\n2\n1\n1\n\n\n\n\n\n\n\n\n\nTests\n\nsource\n\n\n\nunion\n\n union (*dfs, schema, **kwargs)\n\n\nsource\n\n\nmerge_rows\n\n merge_rows (*dfs)\n\n\nTests\n\nres = union(s2,s3,schema=[0,1,2])\nassert_df_equals(res,pd.DataFrame([\n    [1,2,3],\n    [2,3,4],\n    [2,3,5],\n    [4,5,6],\n    [5,6,7],\n    [7,8,9]\n],columns=[0,1,2]))\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n5\n\n\n2\n5\n6\n7\n\n\n3\n2\n3\n4\n\n\n4\n4\n5\n6\n\n\n5\n7\n8\n9\n\n\n\n\n\n\n\n\n\n\nGroupby\n\nsource\n\n\ngroupby\n\n groupby (df, schema, agg, **kwargs)\n\n\nGrouby tests\n\ntext = Span('hello world',name='text')\n\ndf = pd.DataFrame([\n    [1,text[0:5]],\n    [2,text[0:5]],\n    [3,text[6:11]],\n    [4,text[6:11]],\n], columns = ['id','text'])\n\n\ndf\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\n1\n(h, e, l, l, o)\n\n\n1\n2\n(h, e, l, l, o)\n\n\n2\n3\n(w, o, r, l, d)\n\n\n3\n4\n(w, o, r, l, d)\n\n\n\n\n\n\n\n\ndf.groupby('text').agg({'id':'sum'})\n\n\n\n\n\n\n\n\nid\n\n\ntext\n\n\n\n\n\n(h, e, l, l, o)\n3\n\n\n(w, o, r, l, d)\n7\n\n\n\n\n\n\n\n\na = Span(text,5,10)\nb = Span(text,5,10)\nc = Span(text,6,10)\na==b,c==a\n\n(True, False)\n\n\n\na is b\n\nFalse\n\n\n\npd.DataFrame([\n    [1,a],\n    [2,b],\n    [3,c],\n],columns=['id','text']).groupby('text').agg({'id':'sum'})\n\n\n\n\n\n\n\n\nid\n\n\ntext\n\n\n\n\n\n( , w, o, r, l)\n3\n\n\n(w, o, r, l)\n3\n\n\n\n\n\n\n\n\ngroupby(df,schema=['id','text'],agg=['sum',None])\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\n3\n(h, e, l, l, o)\n\n\n1\n7\n(w, o, r, l, d)\n\n\n\n\n\n\n\n\ndef lexic_concat(strings):\n    strings = list(strings)\n    return \" \".join(sorted(strings))\n\n\ntexts = pd.DataFrame([\n    ['he'],\n    ['llo'],\n    ['ho'],\n],columns=['A'])\ntexts\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\nhe\n\n\n1\nllo\n\n\n2\nho\n\n\n\n\n\n\n\n\ntexts2 = pd.DataFrame([\n    ['a','he'],\n    ['b','llo'],\n    ['a','ho'],\n],columns=['A','B'])\ntexts2\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\na\nhe\n\n\n1\nb\nllo\n\n\n2\na\nho\n\n\n\n\n\n\n\n\nassert_df_equals(\n    groupby(texts,schema=['A'],agg=[lexic_concat]),\n    pd.DataFrame([\n        ['he ho llo']\n    ],columns=['A'])\n)\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\nhe ho llo\n\n\n\n\n\n\n\n\nassert_df_equals(\n    groupby(texts2,schema=['A','B'],agg=[lexic_concat,lexic_concat]),\n    pd.DataFrame([\n        ['a a b','he ho llo'],\n    ],columns=['A','B'])\n)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\na a b\nhe ho llo\n\n\n\n\n\n\n\n\nassert_df_equals(\n    groupby(texts2,schema=['A','B'],agg=[None,lexic_concat]),\n    pd.DataFrame([\n        ['a','he ho'],\n        ['b','llo'],\n    ],columns=['A','B'])\n)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\na\nhe ho\n\n\n1\nb\nllo\n\n\n\n\n\n\n\n\ns2\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n2\n3\n5\n\n\n3\n4\n5\n6\n\n\n\n\n\n\n\n\ns2_repeating_names = rename(s2,['A','B','A'])\ns2_repeating_names\n\n\n\n\n\n\n\n\nA\nB\nA\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n2\n3\n5\n\n\n3\n4\n5\n6\n\n\n\n\n\n\n\n\n# test that order remain the same even if groupby columns are not first\nassert_df_equals(\n    groupby(s2,schema=[0,1,2],agg=['sum',None,'min']),\n    pd.DataFrame([\n        [1,2,3],\n        [4,3,4],\n        [4,5,6]\n    ],columns=[0,1,2])\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n4\n3\n4\n\n\n2\n4\n5\n6\n\n\n\n\n\n\n\n\nassert_df_equals(\n    groupby(s2,schema=[0,1,2],agg=[None,'sum','min']),\n    pd.DataFrame([\n        [1,2,3],\n        [2,6,4],\n        [4,5,6]\n    ],columns=[0,1,2])\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n6\n4\n\n\n2\n4\n5\n6\n\n\n\n\n\n\n\n\nassert_df_equals(\n    groupby(s2_repeating_names,schema=['A','B','A'],agg=[None,'sum','min']),\n    pd.DataFrame([\n        [1,2,3],\n        [2,6,4],\n        [4,5,6]\n    ],columns=['A','B','A'])\n)\n\n\n\n\n\n\n\n\nA\nB\nA\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n6\n4\n\n\n2\n4\n5\n6\n\n\n\n\n\n\n\n\nassert_df_equals(\n    groupby(s2,schema=[0,1,2],agg=['max','sum','min']),\n    pd.DataFrame([[4,13,3]],columns=[0,1,2])\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n4\n13\n3\n\n\n\n\n\n\n\n\n\n\nCalc ie operators\nTo calculate ie functions we need * a map like operator to run the function on the input tuples to generate [input+output] tuples\n\nsource\n\n\nie_map\n\n ie_map (df, name, func, in_schema, out_schema, in_arity, out_arity,\n         **kwargs)\n\ngiven an indexed dataframe, apply an ie function to each row and return the output such that each output relation is indexed by the same index as the input relation that generated it\n\nsource\n\n\nmap_iter\n\n map_iter (df, name, func, in_schema, out_schema, in_arity, out_arity,\n           **kwargs)\n\nhelper function returns an iterator that applies a function to each row of a dataframe\n\nsource\n\n\nassert_iterable\n\n assert_iterable (name, func, input, output)\n\n\nsource\n\n\nassert_ie_schema\n\n assert_ie_schema (name, func, value, expected_schema, arity,\n                   input_or_output='input')\n\n\nsource\n\n\ncoerce_tuple_like\n\n coerce_tuple_like (name, func, input, output)\n\n\nTests\n\ns2\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n2\n3\n5\n\n\n3\n4\n5\n6\n\n\n\n\n\n\n\n\ndef func(x,y): return [(x+y,x-y)]\ndef func2(x,y,z): return [(x,y)]\nres = ie_map(s,'F',func,[int,int],[int,int],in_arity=2,out_arity=2)\nassert_df_equals(res,pd.DataFrame([\n    [1,1,2,0],\n    [2,2,4,0],\n    [3,3,6,0],\n    [4,5,9,-1]\n],columns=['col_0','col_1','col_2','col_3']))\n\n\n\n\n\n\n\n\ncol_0\ncol_1\ncol_2\ncol_3\n\n\n\n\n0\n1\n1\n2\n0\n\n\n1\n2\n2\n4\n0\n\n\n2\n3\n3\n6\n0\n\n\n3\n4\n5\n9\n-1\n\n\n\n\n\n\n\n\nres =ie_map(s2,'F',func2,[int,int,int],[int,int],in_arity=3,out_arity=2) \nassert_df_equals(res,pd.DataFrame([\n    [1,2,3,1,2],\n    [2,3,4,2,3],\n    [2,3,5,2,3],\n    [4,5,6,4,5]\n],columns=['col_0','col_1','col_2','col_3','col_4']))\n\n\n\n\n\n\n\n\ncol_0\ncol_1\ncol_2\ncol_3\ncol_4\n\n\n\n\n0\n1\n2\n3\n1\n2\n\n\n1\n2\n3\n4\n2\n3\n\n\n2\n2\n3\n5\n2\n3\n\n\n3\n4\n5\n6\n4\n5\n\n\n\n\n\n\n\n\nres =ie_map(None,'F',func,[int,int],[int,int],in_arity=2,out_arity=2)\nassert_df_equals(res,pd.DataFrame(columns=['col_0','col_1','col_2','col_3']))\n\n\n\n\n\n\n\n\ncol_0\ncol_1\ncol_2\ncol_3\n\n\n\n\n\n\n\n\n\n\n# test checking of schema\nwith pytest.raises(ValueError) as exc_info:\n    ie_map(s2,'F',func2,[int,int,int],[int,int,int],3,3)\nassert 'but expected' in str(exc_info.value)\nprint(exc_info.value)\n\nwith pytest.raises(ValueError) as exc_info:\n    ie_map(s2,'F',func2,[int,int,int],[int,str],3,2)\nassert 'but expected' in str(exc_info.value)\nprint(exc_info.value)\n\n# ie function that forgot to return a tuple\n# if its not iterable we dont know what to do\nnot_iter_func = lambda x,y:x+y\n\n# ie function that returns a value that a primitive, we cast it to an unary tuple\nnot_tuple_iter_func = lambda x,y:[x+y]\n\nwith pytest.raises(ValueError) as exc_info:\n    ie_map(s,'F',not_iter_func,[int,int],[int],2,1)\nassert 'that is not an iterable' in str(exc_info.value)\nprint(exc_info.value)\n\nres = ie_map(s,'F',not_tuple_iter_func,[int,int],[int],2,1)\nassert_df_equals(res,pd.DataFrame([\n    [1,1,2],\n    [2,2,4],\n    [3,3,6],\n    [4,5,9]\n],columns=['col_0','col_1','col_2']))\n\nIEFunction F with underlying function &lt;function func2&gt;\nreceived an output value [1, 2](schema=[int,int])\nbut expected [int,int,int]\nIEFunction F with underlying function &lt;function func2&gt;\nreceived an output value [1, 2](schema=[int,int])\nbut expected [int,str]\nIEFunction F with underlying function &lt;function &lt;lambda&gt;&gt;\nreturned a value that is not an iterable\nfor input [1, 1] -&gt; 2\n\n\n\n\n\n\n\n\n\ncol_0\ncol_1\ncol_2\n\n\n\n\n0\n1\n1\n2\n\n\n1\n2\n2\n4\n\n\n2\n3\n3\n6\n\n\n3\n4\n5\n9\n\n\n\n\n\n\n\n\nfrom spannerlib.ie_func.basic import rgx\n\n\n# Test that ies that return spans works with union\ntext = \"John Doe: 35 years old, Jane Smith: 28 years old\"\npattern = \"(\\\\w+\\\\s\\\\w+):\\\\s(\\\\d+)\"\nregex_in = pd.DataFrame(\n    [[pattern,text]]\n)\n\n\nres = ie_map(regex_in,'rgx',rgx,[str,(str,Span)],[Span,Span],2,2)\nres = project(res,['col_2','col_3'])\nres = union(*[res],schema=['col_0','col_1'])\nassert_df_equals(res,pd.DataFrame([\n    [Span(text,0,8),Span(text,10,12)],\n    [Span(text,24,34),Span(text,36,38)]],\n    columns=['col_0','col_1']\n))\nres.map(repr)\n\n[(0, 8), (10, 12)]\n[(24, 34), (36, 38)]\n\n\n\n\n\n\n\n\n\ncol_0\ncol_1\n\n\n\n\n0\n[@c1d7fe,0,8) \"John Doe\"\n[@c1d7fe,10,12) \"35\"\n\n\n1\n[@c1d7fe,24,34) \"Jane Smith\"\n[@c1d7fe,36,38) \"28\""
  },
  {
    "objectID": "inspecting_query_plans.html",
    "href": "inspecting_query_plans.html",
    "title": "View Query Plan",
    "section": "",
    "text": "from spannerlib import get_magic_session,Span\nfrom graph_rewrite import draw\n\n\nnew S(str,str)\nnew T(str,str)\n\nR(x,y,z)&lt;-S(x,y),T(x,y),rgx(x,y)-&gt;(z).\n\n\nsess = get_magic_session()\nsess.export('?R(x,y,z)')\n\n\n\n\n\n\n\n\nx\ny\nz\n\n\n\n\n\n\n\n\n\n\ngraph,root = sess.export('?R(x,y,z)',plan_query=True,draw_query=True)\n\n\n\n\n\nlen(graph.nodes())\n\n17\n\n\n\ndraw(graph,ret_mermaid=True)\n\n\nflowchart TB\nS[\"S\nrel=#quot;S#quot;, rule_id={0, #quot;fact#quot;}, schema=[#quot;col_0#quot;, #quot;col_1#quot;], op=#quot;get_rel#quot;, db=DB(S, T, R)\"]\nT[\"T\nrel=#quot;T#quot;, rule_id={0, #quot;fact#quot;}, schema=[#quot;col_0#quot;, #quot;col_1#quot;], op=#quot;get_rel#quot;, db=DB(S, T, R)\"]\nR[\"R\nrel=#quot;R#quot;, rule_id={0, #quot;fact#quot;}, op=#quot;union#quot;, schema=[#quot;col_0#quot;, #quot;col_1#quot;, #quot;col_2#quot;]\"]\n0[\"0\nop=#quot;rename#quot;, schema=[#quot;x#quot;, #quot;y#quot;], rule_id={0}\"]\n1[\"1\nop=#quot;project#quot;, schema=[#quot;x#quot;, #quot;y#quot;], rule_id={0}\"]\n2[\"2\nop=#quot;rename#quot;, schema=[#quot;x#quot;, #quot;y#quot;], rule_id={0}\"]\n3[\"3\nop=#quot;project#quot;, schema=[#quot;x#quot;, #quot;y#quot;], rule_id={0}\"]\n4[\"4\nop=#quot;project#quot;, schema=[#quot;x#quot;, #quot;y#quot;], rule_id={0}\"]\n5[\"5\nop=#quot;ie_map#quot;, func=#lt;function rgx at 0x126636ac0#gt;, in_arity=2, out_arity=1, schema=[#quot;col_0#quot;, #quot;col_1#quot;, #quot;col_2#quot;], rule_id={0}, name=#quot;rgx#quot;, in_schema=[#lt;class #quot;str#quot;#gt;, (#lt;class #quot;str#quot;#gt;, #lt;class #quot;spannerlib.span.Span#quot;#gt;)], out_schema=#lt;function span_arity at 0x126769940#gt;\"]\n6[\"6\nop=#quot;rename#quot;, schema=[#quot;x#quot;, #quot;y#quot;, #quot;z#quot;], rule_id={0}\"]\n7[\"7\nop=#quot;rename#quot;, schema=[#quot;x#quot;, #quot;y#quot;, #quot;z#quot;], rule_id={0}\"]\n8[\"8\nop=#quot;project#quot;, schema=[#quot;x#quot;, #quot;y#quot;, #quot;z#quot;], rule_id={0}\"]\n9[\"9\nop=#quot;join#quot;, schema=[#quot;x#quot;, #quot;y#quot;], rule_id={0}\"]\n10[\"10\nop=#quot;join#quot;, schema=[#quot;x#quot;, #quot;y#quot;, #quot;z#quot;], rule_id={0}\"]\n11[\"11\nop=#quot;project#quot;, schema=[#quot;x#quot;, #quot;y#quot;, #quot;z#quot;], rel=#quot;_R_0#quot;, rule_id={0}\"]\n12[\"12\nop=#quot;rename#quot;, schema=[#quot;x#quot;, #quot;y#quot;, #quot;z#quot;]\"]\n13[\"13\nop=#quot;project#quot;, schema=[#quot;x#quot;, #quot;y#quot;, #quot;z#quot;]\"]\nR --&gt; 11\n0 --&gt; S\n1 --&gt; 0\n2 --&gt; T\n3 --&gt; 2\n4 --&gt; 9\n5 --&gt; 4\n6 --&gt; 5\n7 --&gt; 6\n8 --&gt; 7\n9 --&gt; 1\n9 --&gt; 3\n10 --&gt; 9\n10 --&gt; 8\n11 --&gt; 10\n12 --&gt; R\n13 --&gt; 12",
    "crumbs": [
      "View Query Plan"
    ]
  },
  {
    "objectID": "session_tests/basic_tests.html",
    "href": "session_tests/basic_tests.html",
    "title": "Basic tests",
    "section": "",
    "text": "import numpy as np\nimport tempfile\nfrom pandas import DataFrame\nfrom pathlib import Path\nimport pandas as pd\n\nfrom spannerlib.span import Span\nfrom spannerlib.session import Session,test_session\n\n\npd.DataFrame([\n    [1,1]\n]).sort_values(by=0)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n1\n\n\n\n\n\n\n\n\n# test assignment\ntest_session(\n    \"\"\"\n        new Relation(int, int)\n        x = 1\n        y = 2\n        z = y\n        Relation($x, $y)\n        Relation($y, $x)\n        ?Relation(X, $x)\n    \"\"\",\n    pd.DataFrame([[2]],columns=['X']),\n)\n\n'?Relation(X,$x)'\n\n\n\n\n\n\n\nX\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test copy table rule\ntest_session(\n    \"\"\"\n        new B(int, int)\n        B(1, 1)\n        B(1, 2)\n        B(2, 3)\n        A(X, Y) &lt;- B(X, Y).\n        ?A(X, Y)\n    \"\"\",\n    pd.DataFrame(\n        [[1,1],[1,2],[2,3]],\n        columns=['X','Y']\n    )\n)\n\n'?A(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n2\n3\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test join 2 tables\ntest_session(\n    \"\"\"\n        new B(int, int)\n        new C(int, int)\n        B(1, 1)\n        B(1, 2)\n        B(2, 3)\n        C(2, 2)\n        C(1, 1)\n        D(X, Y, Z) &lt;- B(X, Y), C(Y, Z).\n        ?D(X, Y, Z)\n    \"\"\",\n    pd.DataFrame(\n        [[1,2,2],[1,1,1]],\n        columns=['X','Y','Z']\n    )\n)\n\n'?D(X,Y,Z)'\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\n1\n1\n1\n\n\n1\n2\n2\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# rel with same free var\ntest_session(\n    \"\"\"\n        new B(int, int)\n        B(1, 1)\n        B(1, 2)\n        B(2, 2)\n        A(X) &lt;- B(X, X).\n        ?A(X)\n    \"\"\",\n    pd.DataFrame(\n        [[1],[2]],\n        columns=['X']\n    )\n)\n\n'?A(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# union with same vars\ntest_session(\n    \"\"\"\n        new B(int, int)\n        new C(int, int)\n        B(1, 1)\n        B(1, 2)\n        B(2, 3)\n        C(2, 2)\n        C(1, 1)\n\n        A(X, Y) &lt;- B(X, Y).\n        A(X, Y) &lt;- C(X, Y).\n        ?A(X, Y)\n    \"\"\",\n    pd.DataFrame(\n        [[1,1],[1,2],[2,2],[2,3]],\n        columns=['X','Y']\n    )\n)\n\n'?A(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n2\n2\n\n\n2\n3\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# union with different vars\ntest_session(\n    \"\"\"\n        new B(int, int)\n        new C(int, int)\n        B(1, 1)\n        B(1, 2)\n        B(2, 3)\n        C(2, 2)\n        C(1, 1)\n\n        A(X, Y) &lt;- B(X, Y).\n        A(Z, W) &lt;- C(Z, W).\n        ?A(X, Y)\n    \"\"\",\n    pd.DataFrame(\n        [[1,1],[1,2],[2,2],[2,3]],\n        columns=['X','Y']\n    )\n)\n\n'?A(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n2\n2\n\n\n2\n3\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test project\ntest_session(\n    \"\"\"\n            new B(int, int)\n            B(1, 1)\n            B(1, 2)\n\n            A(X) &lt;- B(X, Y).\n            ?A(X)\n    \"\"\",\n    pd.DataFrame(\n        [[1]],\n        columns=['X']\n    )\n)\n\n'?A(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# add fact after rule\ntest_session(\n    \"\"\"\n            new B(int, int)\n            B(1, 1)\n            A(X, Y) &lt;- B(X, Y).\n            B(1, 2)\n            ?A(Z, W)\n    \"\"\",\n    pd.DataFrame(\n        [[1,1],[1,2]],\n        columns=['Z','W']\n    )\n)\n\n'?A(Z,W)'\n\n\n\n\n\n\n\nZ\nW\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test data types\ntest_session(\n    \"\"\"\n            new B(int, str, float,bool)\n            B(1, \"2\", -3.5,False)\n            ?B(X, Y, Z,W)\n    \"\"\",\n    pd.DataFrame(\n        [[1,\"2\",-3.5,False]],\n        columns=['X','Y','Z','W']\n    )\n)\n\n'?B(X,Y,Z,W)'\n\n\n\n\n\n\n\nX\nY\nZ\nW\n\n\n\n\n1\n2\n-3.500000\nFalse\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# join same relation\ntest_session(\n    \"\"\"\n            new Parent(str, str)\n            Parent(\"Sam\", \"Noah\")\n            Parent(\"Noah\", \"Austin\")\n            Parent(\"Austin\", \"Stephen\")\n\n\n            GrandParent(G, C) &lt;- Parent(G, M), Parent(M, C).\n            ?GrandParent(X, \"Austin\")\n    \"\"\",\n    pd.DataFrame(\n        [['Sam']],\n        columns=['X']\n    )\n)\n\n'?GrandParent(X,\"Austin\")'\n\n\n\n\n\n\n\nX\n\n\n\n\nSam\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test_rule_with_constant\ntest_session(\n\"\"\"\n    new B(int, int)\n    new C(int, int)\n    B(1, 1)\n    B(1, 2)\n    B(2, 3)\n    C(2, 2)\n    C(1, 1)\n    A(X) &lt;- B(1, X).\n    ?A(X)\n\"\"\",\npd.DataFrame([[1],[2]],columns=['X'])\n)\n\n'?A(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test_rule_with_true_value\ntest_session(\n[\n    \"\"\"\n    new B(int, int)\n    new C(int, int)\n    B(1, 1)\n    B(1, 2)\n    B(2, 3)\n    C(2, 2)\n    C(1, 1)\n\n    A(X, Y) &lt;- B(X, Y), C(1, 1).\n    \"\"\",\n    \"\"\"\n    ?A(X, Y)\n    \"\"\"\n],\n[\n    None,   \n    pd.DataFrame([[1,1],[1,2],[2,3]],columns=['X','Y'])\n],debug=True\n)\n\n\n\n\n'?A(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n2\n3\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test_rule_with_false_value\ntest_session(\n\"\"\"\n    new B(int, int)\n    new C(int, int)\n    B(1, 1)\n    B(1, 2)\n    B(2, 3)\n    C(2, 2)\n    C(1, 1)\n\n    A(X, Y) &lt;- B(X, Y), C(0, 0).\n    ?A(X, Y)\n\"\"\",\npd.DataFrame([],columns=['X','Y'])\n)\n\n'?A(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test_query_with_same_var\ntest_session(\n[\n    \"\"\"\n    new B(int, int)\n    B(1, 1)\n    B(1, 2)\n    B(2, 3)\n\n    A(X, Y) &lt;- B(X, Y).\n    ?A(X,Y)\n    \"\"\",\n    \"\"\"\n    ?A(X, X)\n\"\"\"\n],[\n    pd.DataFrame([[1,1],[1,2],[2,3]],columns=['X','Y']),\n    pd.DataFrame([[1]],columns=['X']),\n],\n)\n\n'?A(X,Y)'\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n2\n3\n\n\n\n\n\n\n\n\n\n'?A(X,X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test_rule_with_constant\ntest_session(\n[\n    \"\"\"\n    new B(int, int)\n    new C(int, int)\n    B(1, 1)\n    B(1, 2)\n    B(2, 3)\n    C(2, 2)\n    C(1, 1)\n\n    A(X) &lt;- B(1, X).\n    ?A(X)\n\"\"\"\n],[\n    pd.DataFrame([[1],[2]],columns=['X']),\n],\n)\n\n'?A(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test_select_and_join\ntest_session(\n[\n    \"\"\"\n            new B(int)\n            new C(int, int)\n            B(2)\n            C(1, 4)\n            C(2, 5)\n            A(X) &lt;- B(X), C(X, 5).\n            ?A(X)\n\"\"\"\n],[\n    pd.DataFrame([[2]],columns=['X']),\n],\n)\n\n'?A(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test Boolean values\ntest_session(\n[\n    \"\"\"\n            new A(int)\n            A(1)\n            ?A(1)\n\"\"\",\n\"\"\"?A(2)\"\"\"\n],[\n    True,\n    False\n],\n)\n\n'?A(1)'\n\n\nTrue\n\n\n'?A(2)'\n\n\nFalse\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test no code doesnt break\ntest_session(\n[\n    \"\"\"\"\"\",\n],[\n    None,\n],\n)\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# test_add and remove fact\ntest_session(\n[\n    \"\"\"\n    new rel(int)\n    +rel(8)\n    rel(16)\n    -rel(16)\n    rel(32)\n    rel(16)\n    -rel(32)\n    ?rel(X)\n\"\"\"\n],[\n    pd.DataFrame([[8],[16]],columns=['X']),\n],\n)\n\n'?rel(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n8\n\n\n16\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# combine constants in head with regular heads\ntest_session(\n[\n    \"\"\"\n    new BadPeople(str,str)\n    BadPeople(\"John\",\"bad\")\n    \n    new GoodPeople(str)\n    GoodPeople(\"Bob\")\n    \n    PeopleAffinity(Name,Aff)&lt;-BadPeople(Name,Aff).\n    PeopleAffinity(Name,\"good\")&lt;-GoodPeople(Name).\n    ?PeopleAffinity(Name,Aff)\n\"\"\"\n],[\n    pd.DataFrame([\n        ['John','bad'],\n        ['Bob','good']\n    ],columns=['Name', 'Aff']),\n],\n)\n\n'?PeopleAffinity(Name,Aff)'\n\n\n\n\n\n\n\nName\nAff\n\n\n\n\nBob\ngood\n\n\nJohn\nbad\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\n# combine constants in head with regular heads\ntest_session(\n[\n\"\"\"\ndarkness_sentence = \"hello darkness my old friend\"\nsunshine_sentence = \"Im walking on sunshine\"\n\nnew SentenceSentiment(str,bool)\nSentenceSentiment($darkness_sentence, False)\nSentenceSentiment($sunshine_sentence, True)\n\nword_pattern = \"(\\w+)\"\nGoodSentences(Sentence) &lt;- SentenceSentiment(Sentence, True).\nBadWords(Word) &lt;- SentenceSentiment(Sentence, False), rgx($word_pattern, Sentence)-&gt;(WordSpan),as_str(WordSpan)-&gt;(Word).\nGoodWords(Word) &lt;- SentenceSentiment(Sentence, True), rgx($word_pattern, Sentence)-&gt;(WordSpan),as_str(WordSpan)-&gt;(Word).\n\n\"\"\",\n\"\"\"\n?GoodSentences(Sentence)\n\"\"\",\n\"\"\"\n?BadWords(Word)\n\"\"\",\n\"\"\"\n?GoodWords(Word)\n\"\"\"\n],[None,\n    pd.DataFrame([\n        ['Im walking on sunshine']\n    ],columns=['Sentence']),\n    pd.DataFrame([\n        ['hello'],\n        ['darkness'],\n        ['my'],\n        ['old'],\n        ['friend']\n    ],columns=['Word']),\n    pd.DataFrame([\n        ['Im'],\n        ['walking'],\n        ['on'],\n        ['sunshine']\n    ],columns=['Word']),\n],\n# debug=True\n)\n\n'?GoodSentences(Sentence)'\n\n\n\n\n\n\n\nSentence\n\n\n\n\nIm walking on sunshine\n\n\n\n\n\n\n\n\n\n'?BadWords(Word)'\n\n\n\n\n\n\n\nWord\n\n\n\n\ndarkness\n\n\nfriend\n\n\nhello\n\n\nmy\n\n\nold\n\n\n\n\n\n\n\n\n\n'?GoodWords(Word)'\n\n\n\n\n\n\n\nWord\n\n\n\n\nIm\n\n\non\n\n\nsunshine\n\n\nwalking\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;"
  },
  {
    "objectID": "session_tests/test_ie_functions.html",
    "href": "session_tests/test_ie_functions.html",
    "title": "Test IE functions",
    "section": "",
    "text": "import numpy as np\nimport tempfile\nfrom pandas import DataFrame\nfrom pathlib import Path\nimport pandas as pd\n\nfrom spannerlib.span import Span\nfrom spannerlib.session import Session,test_session\nfrom spannerlib.utils import assert_df_equals\n\n\ndef yield_range(num: int):\n        for i in range(num):\n            yield i,\n\ndef yield_range_span(text: str,name:str):\n    for i in range(len(text)):\n        yield Span(text,0,i+1,name=name)\n    \ndef yield_range_str(num: int):\n    for i in range(num):\n        yield \"string\" + str(i),\n\n\nsess = test_session(\n    [\n        \"\"\"\n        test_range(X) &lt;- yield_range(5) -&gt; (X).\n        ?test_range(X)\n        \"\"\",\n\n        \"\"\"\n        test_range_str(X) &lt;- yield_range_str(5) -&gt; (X).\n        ?test_range_str(X)\n        \"\"\",\n        \"\"\"\n        test_range_raw(X) &lt;- yield_range_span(\"hello\",\"doc\") -&gt; (X).\n        test_range_span(S) &lt;- yield_range_span(\"hello\",\"doc\") -&gt; (X), as_str(X)-&gt;(S).\n        ?test_range_span(S)\n        \"\"\",\n    ],\n    [\n        pd.DataFrame({'X':range(5)}),\n        pd.DataFrame({'X':[\"string0\",\"string1\",\"string2\",\"string3\",\"string4\"]}),\n        pd.DataFrame({'S':[\"h\",\"he\",\"hel\",\"hell\",\"hello\"]})\n    ],\n    ie_funcs=[\n        ['yield_range',yield_range,[int],[int]],\n        ['yield_range_span',yield_range_span,[str,str],[Span]],\n        ['yield_range_str',yield_range_str,[int],[str]]\n    ],\n)\n\n'?test_range(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\n\n\n\n'?test_range_str(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\nstring0\n\n\nstring1\n\n\nstring2\n\n\nstring3\n\n\nstring4\n\n\n\n\n\n\n\n\n\n'?test_range_span(S)'\n\n\n\n\n\n\n\nS\n\n\n\n\nh\n\n\nhe\n\n\nhel\n\n\nhell\n\n\nhello\n\n\n\n\n\n\n\n\n\n\nsess.export('?test_range_raw(S)',display_results=True)\n\n'?test_range_raw(S)'\n\n\n\n\n\n\n\nS\n\n\n\n\n[@doc,0,1) \"h\"\n\n\n[@doc,0,2) \"he\"\n\n\n[@doc,0,3) \"hel\"\n\n\n[@doc,0,4) \"hell\"\n\n\n[@doc,0,5) \"hello\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS\n\n\n\n\n0\n(h)\n\n\n1\n(h, e)\n\n\n2\n(h, e, l)\n\n\n3\n(h, e, l, l)\n\n\n4\n(h, e, l, l, o)\n\n\n\n\n\n\n\n\nfrom spannerlib.ie_func.basic import rgx\n\n\nlist(rgx('.*',\"aa\")) == ['aa',\"\"]\n\nTrue\n\n\n\n{(Span(\"aa\",0,2),)} != {('aa',)}\n\nTrue\n\n\n\nsess = test_session(\n[\n    \"\"\"\n    py_span_rel(X) &lt;- rgx(\".+\",\"aa\") -&gt; (X).\n    py_string_rel(X) &lt;- rgx(\".+\",\"aa\") -&gt; (S), as_str(S)-&gt;(X).\n    ?py_span_rel(X)\n    \"\"\",\n    \"\"\"\n    ?py_string_rel(X)\n    \"\"\"\n],\n[\n    pd.DataFrame([\n        [\"aa\"],\n    ],columns=['X']),\n    pd.DataFrame([\n        [\"aa\"],\n    ],columns=['X']),\n],\n)\n# _ = sess.export('?py_span_rel(X)',display_results=True)\n\n'?py_span_rel(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\n[@e0c903,0,2) \"aa\"\n\n\n\n\n\n\n\n\n\n'?py_string_rel(X)'\n\n\n\n\n\n\n\nX\n\n\n\n\naa\n\n\n\n\n\n\n\n\n\n\nsess = test_session(\n[\n    \"\"\"\n    py_span_rel(X,DEBUG) &lt;- rgx(\".+\",\"aa\") -&gt; (X), print(\"X={0}, const={1}\",X,\"aa\")-&gt;(DEBUG).\n    ?py_span_rel(X,DEBUG)\n    \"\"\",\n\n],\n[\n\n    pd.DataFrame([\n        [\"aa\",\"X=aa, const=aa\"],\n    ],columns=['X','DEBUG']),\n],\n)\n# _ = sess.export('?py_span_rel(X)',display_results=True)\n\nX=aa, const=aa\n\n\n'?py_span_rel(X,DEBUG)'\n\n\n\n\n\n\n\nX\nDEBUG\n\n\n\n\n[@e0c903,0,2) \"aa\"\nX=aa, const=aa\n\n\n\n\n\n\n\n\n\n\n# TODO add tests of extracting spans from spans once we have a way to treat spans as strings in the type system\n\n\n# uncomment these once we decide how to support rust regexs\n\n\n# test_session(\n# [\n#     \"\"\"\n#     string_rel(X) &lt;- rgx_string(\"aa\",\".+\") -&gt; (X).\n#     span_rel(X) &lt;- rgx_span(\"aa\",\".+\") -&gt; (X).\n#     ?string_rel(X)\n#     \"\"\"\n#     \"\"\"\n#     ?span_rel(X)\n#     \"\"\"\n# ],[\n#     pd.DataFrame([\n#         ['a'],\n#         ['aa'],\n#     ],columns=['X']),\n#     pd.DataFrame([\n#         [Span(0,1)],\n#         [Span(0,2)],\n#         [Span(1,2)]\n#     ],columns=['X']),\n# ],\n# )\n\n\n# with tempfile.TemporaryDirectory() as temp_dir:\n#         rgx_text_file = Path(temp_dir) / \"temp\"\n#         with open(rgx_text_file, \"w\") as f:\n#             f.write(\"aa\")\n\n# test_session(\n# [\n#     \"\"\"\n#     string_rel(X) &lt;- rgx_string_from_file(\"tests/rgx_text_file\",\".+\") -&gt; (X).\n#     span_rel(X) &lt;- rgx_span_from_file(\"tests/rgx_text_file\",\".+\") -&gt; (X).\n#     ?string_rel(X)\n#     \"\"\"\n#     \"\"\"\n#     ?span_rel(X)\n#     \"\"\"\n# ],[\n#     pd.DataFrame([\n#         ['a'],\n#         ['aa'],\n#     ],columns=['X']),\n#     pd.DataFrame([\n#         [Span(0,1)],\n#         [Span(0,2)],\n#         [Span(1,2)]\n#     ],columns=['X']),\n# ],\n# )\n\n\n# text = \"aab\"\n# pattern = \"(?P&lt;group_all&gt;(?P&lt;group_a&gt;a+)(?P&lt;group_b&gt;b+))\"\n# test_session(\n# [\n#     f\"\"\"\n#     group_string_rel(X,Y,Z) &lt;- rgx_string(\"{text}\",\"{pattern}\") -&gt; (X,Y,Z).\n#     ?group_string_rel(X, Y, Z)\n#     \"\"\"\n# ],[\n#     pd.DataFrame([\n#         ['aa','b','aab'],\n#         ['a','b','ab'],\n#     ],columns=['X','Y','Z']),\n# ],\n# )\n\n\n# TODO add tests for ie functions from stdlib, do this only for the ones we leave in the final demo version"
  },
  {
    "objectID": "session_tests/test_agg_functions.html",
    "href": "session_tests/test_agg_functions.html",
    "title": "Test aggregation functions",
    "section": "",
    "text": "import numpy as np\nimport tempfile\nfrom pandas import DataFrame\nfrom pathlib import Path\nimport pandas as pd\n\nfrom spannerlib.span import Span\nfrom spannerlib.session import Session,test_session\n\n\ndef lexic_concat(strings):\n    strings = list(strings)\n    return \" \".join(sorted(strings))\n\ntest_session(\n    [\n        \"\"\"\n        new Text(str)\n        Text(\"he\")\n        Text(\"llo\")\n        Text(\"ho\")\n\n        Prompt(concat(P))&lt;-Text(P).\n        \"\"\",\n    \n        \"\"\"\n        ?Prompt(P)\n        \"\"\"\n    ],\n    [\n        None,\n        DataFrame({'P': ['he ho llo']})\n    ],\n    agg_funcs=[\n        ['concat',lexic_concat,[str],[str]],\n    ],\n)\n\n'?Prompt(P)'\n\n\n\n\n\n\n\nP\n\n\n\n\nhe ho llo\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ndef lexic_concat(strings):\n    strings = list(strings)\n    return \" \".join(sorted(strings))\n\ntest_session(\n    [\n        \"\"\"\n        new Text(str)\n        Text(\"he\")\n        Text(\"llo\")\n        Text(\"ho\")\n\n        Prompt(concat(P))&lt;-Text(P).\n        \"\"\",\n    \n        \"\"\"\n        ?Prompt(P)\n        \"\"\"\n    ],\n    [\n        None,\n        DataFrame({'P': ['he ho llo']})\n    ],\n    agg_funcs=[\n        ['concat',lexic_concat,[str],[str]],\n    ],\n)\n\n'?Prompt(P)'\n\n\n\n\n\n\n\nP\n\n\n\n\nhe ho llo\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ntest_session(\n    [\n        \"\"\"\n        new nums(int,int,int)\n        nums(1,2,3)\n        nums(1,5,6)\n        nums(2,3,4)\n        nums(2,8,9)\n        \n        aggregations(X,sum(Y),min(Z)) &lt;- nums(X,Y,Z).\n        ?aggregations(A,B,C)\n        \"\"\",\n    \n    ],\n    [\n            DataFrame([\n                [1, 7, 3],\n                [2, 11, 4]]\n                , columns=['A','B','C'])\n    ],\n    agg_funcs=[\n        ['concat',lexic_concat,[str],[str]],\n    ],\n)\n\n'?aggregations(A,B,C)'\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\n7\n3\n\n\n2\n11\n4\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ntest_session(\n    [\n        \"\"\"\n        new nums(int,int,int)\n        nums(1,2,3)\n        nums(1,2,6)\n        nums(2,3,4)\n        nums(2,3,9)\n        \n        aggregations(sum(X),Y,min(Z)) &lt;- nums(X,Y,Z).\n        \"\"\",\n        \"\"\"\n        ?aggregations(A,B,C)\n        \"\"\",\n    \n    ],\n    [ \n    None,\n    DataFrame([\n                [2, 2, 3],\n                [4, 3, 4]]\n                , columns=['A','B','C'])\n    ],\n    agg_funcs=[\n        ['concat',lexic_concat,[str],[str]],\n    ],\n    debug=True\n)\n\n\n\n\n'?aggregations(A,B,C)'\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n2\n2\n3\n\n\n4\n3\n4\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;\n\n\n\ntest_session(\n    [\n        \"\"\"\n        new nums(int,int,int)\n        nums(1,2,3)\n        nums(1,5,6)\n        nums(2,3,4)\n        nums(2,8,9)\n        \n        aggregations(X,sum(Y),min(X)) &lt;- nums(X,Y,Z).\n        ?aggregations(A,B,C)\n        \"\"\",\n    \n    ],\n    [\n            DataFrame([\n                [1, 7, 1],\n                [2, 11, 2]]\n                , columns=['A','B','C'])\n    ],\n    agg_funcs=[\n        ['concat',lexic_concat,[str],[str]],\n    ],\n)\n\n'?aggregations(A,B,C)'\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\n7\n1\n\n\n2\n11\n2\n\n\n\n\n\n\n\n\n\n&lt;spannerlib.session.Session&gt;"
  }
]