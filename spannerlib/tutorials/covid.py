# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/covid-nlp/000_covid_pipeline.ipynb.

# %% auto 0
__all__ = ['nlp', 'sess', 'slog_file', 'input_dir', 'data_dir', 'lemma_list', 'lemmatizer', 'file_paths', 'raw_docs',
           'lemma_concepts', 'pos_concept_docs', 'target_rule_docs', 'section_tags', 'section_delimeter_pattern',
           'post_process_pattern_rules', 'postprocess_attribute_rules', 'next_sentence_postproccessing_rules',
           'doc_tags', 'paths', 'classification', 'split_sentence', 'LemmaFromList', 'PosFromList', 'rewrite',
           'rewrite_docs', 'agg_mention', 'AggDocumentTags']

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 13
import re
import csv
import pandas as pd
from pandas import DataFrame
from pathlib import Path

import spacy
nlp = spacy.load("en_core_web_sm")

from .. import get_magic_session,Session,Span
sess = get_magic_session()

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 14
slog_file = 'covid_logic.slog'
input_dir = Path('sample_inputs')
data_dir = Path('data')

#TODO from here - Done
# organize code into IE/Agg functions, 
# regular python functions
# and the pipeline itself

# after that make a new section about the original pipeline

# TODO make syntax and semantic hide their traceback
# TODO make ie function show traceback only in the function itself
# TODO explain the export tags
# TODO move input samples into their own subdir
# TODO if someone tries to add a dataframe to a rule head, raise an error

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 16
def split_sentence(text):
    """
    This function reads a text file, processes its content using spaCy's English language model,
    tokenizing it into sentences and returns each individual sentence in the processed text using a generator.
    
    Parameters:
        text_path (str): The path to the text file to be annotated.

    Returns:
        str: Individual sentences extracted from the input text.
    """

    doc = nlp(str(text))

    start = 0
    for sentence in doc.sents:
        end = start+len(sentence.text)
        yield Span(text,start,end)
        start = end + 1



# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 19
class LemmaFromList():
    def __init__(self,lemma_list):
        self.lemma_list = lemma_list

    def __call__(self,text):
        nlp = spacy.load("en_core_web_sm")
        doc = nlp(str(text))
        for word in doc:
            start = word.idx
            end = start + len(word.text)
            if word.lemma_ in self.lemma_list:
                yield (Span(text,start,end),word.lemma_)
            elif word.like_num:
                yield (Span(text,start,end),'like_num')
            else:
                pass

lemma_list = (data_dir/'lemma_words.txt').read_text().split()
lemmatizer = LemmaFromList(lemma_list)




# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 21
class PosFromList():
    def __init__(self,pos_list):
        self.pos_list = pos_list
    def __call__(self,text):
        nlp = spacy.load("en_core_web_sm")
        doc = nlp(str(text))
        for word in doc:
            start = word.idx
            end = start + len(word.text)
            if word.pos_ in self.pos_list:
                yield (Span(text,start,end),word.pos_)


# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 24
def rewrite(text,span_label_pairs):
    """rewrites a string given a dataframe with spans and the string to rewrite them to
    assumes that the spans belong to the text


    Args:
        text (str like): string to rewrite
        span_label_pairs (pd.Dataframe) dataframe with two columns, first is spans in the doc to rewrite
            second is what to rewrite to
    Returns:
        The rewritten string
    """    
    if isinstance(text,Span):
        text = text.as_str()
    span_label_pairs = sorted(list(span_label_pairs.itertuples(index=False,name=None)), key=lambda x: x[0].start)

    rewritten_text = ''
    current_pos = 0
    for span,label in span_label_pairs:
        rewritten_text += text[current_pos:span.start] + label 
        current_pos = span.end

    rewritten_text += text[current_pos:]

    return rewritten_text


# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 26
def rewrite_docs(docs,span_label,new_version):
    new_tuples =[]
    span_label.columns = ['P','D','W','L']
    for path,doc,_ in docs.itertuples(index=False,name=None):
        span_label_per_doc = span_label[span_label['P'] == path][['W','L']]
        new_text = rewrite(doc,span_label_per_doc)
        new_tuples.append((path,new_text,new_version))
    return pd.DataFrame(new_tuples,columns=['P','D','V'])
    

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 31
sess.register('split_sentence',split_sentence,[(str,Span)],[Span])
sess.register('pos',pos_annotator,[(Span,str)],[Span,str])
sess.register('lemma',lemmatizer,[(Span,str)],[Span,str])

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 32
from glob import glob
file_paths = pd.DataFrame(glob(str(input_dir/'*.txt')))
sess.import_rel('Files',file_paths)

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 36
raw_docs  = sess.export('?RawDocs(P,D,V)')
#
sess.import_rel('Docs',raw_docs)
raw_docs

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 42
sess.import_rel("ConceptTagRules",data_dir/"concept_tags_rules.csv" , delim=",")

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 47
lemma_concepts = rewrite_docs(lemma_docs,lemma_concept_matches,'lemma_concept')
sess.import_rel('Docs',lemma_concepts)
lemma_concepts

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 54
pos_concept_docs = rewrite_docs(lemma_concepts,pos_concept_matches,'pos_concept')
sess.import_rel('Docs',pos_concept_docs)
sess.export('?Docs("sample_inputs/sample8.txt",D,V)')

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 57
sess.import_rel("TargetTagRules",data_dir/"target_rules.csv",delim=",")

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 61
target_rule_docs = rewrite_docs(pos_concept_docs,target_matches,'target_concept')
sess.import_rel('Docs',target_rule_docs)

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 64
section_tags = pd.read_csv(data_dir/'section_tags.csv',names=['literal','tag'])
section_tags.head()

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 65
section_delimeter_pattern = section_tags['literal'].str.cat(sep='|')
sess.import_var('section_delimeter_pattern',section_delimeter_pattern)
section_delimeter_pattern

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 66
sess.import_rel("SectionTags",data_dir/"section_tags.csv",delim=",")
sess.import_rel("PositiveSectionTags",data_dir/"positive_section_tags.csv",delim=",")


# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 85
sess.import_rel("SentenceContextRules",context_rules)

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 89
post_process_pattern_rules = pd.read_csv(data_dir/'postprocess_pattern_rules.csv',delimiter='#',header=None,names=['pattern','tag'])
post_process_pattern_rules

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 90
sess.import_rel("PostprocessPatternRules",post_process_pattern_rules)

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 93
postprocess_attribute_rules = pd.read_csv(data_dir/'postprocess_attributes_rules.csv',delimiter='#',header=None,names=['pattern','old_tag','new_tag'])
postprocess_attribute_rules

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 94
sess.import_rel("PostprocessRulesWithAttributes",postprocess_attribute_rules)

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 97
next_sentence_postproccessing_rules = pd.read_csv(data_dir/'postprocess_pattern_next_sentence_rules.csv',header=None,names=['pattern','tag'])
sess.import_rel("NextSentencePostprocessPatternRules",next_sentence_postproccessing_rules)
next_sentence_postproccessing_rules


# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 100
def agg_mention(group):
    """
    aggregates attribute groups of covid spans

    Returns:
        str: Filtered "CovidSpan" attribute determined by the following rules:
            - If 'IGNORE' is present, returns 'IGNORE'.
            - If 'negated' is present (and 'no_negated' is not present), returns 'negated'.
            - If 'future' is present (and 'no_future' is not present), returns 'negated'.
            - If 'other experiencer' or 'not relevant' is present, returns 'negated'.
            - If 'positive' is present (and 'uncertain' and 'no_positive' are not present), returns 'positive'.
            - Otherwise, returns 'uncertain'.
    """
    if 'IGNORE' in group.values:
        return 'IGNORE'
    elif 'negated' in group.values and not 'no_negated' in group.values:
        return 'negated'
    elif 'future' in group.values and not 'no_future' in group.values:
        return 'negated'
    elif 'other experiencer' in group.values or 'not relevant' in group.values:
        return 'negated'
    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:
        return 'positive'
    else:
        return 'uncertain'

sess.register_agg('agg_mention',agg_mention,[str],[str])

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 102
def AggDocumentTags(group):
    """
Classifies a document as 'POS', 'UNK', or 'NEG' based on COVID-19 attributes.

Parameters:
    group (pandas.Series): A pandas Series representing COVID-19 attributes for each document within a DataFrame.
    
Returns:
    str: Document classification determined as follows:
         - 'POS': If at least one COVID-19 attribute with "positive" is present in the group.
         - 'UNK': If at least one COVID-19 attribute with "uncertain" is present in the group and no "positive" attributes,
                  or there's at least one COVID-19 attribute with 'IGNORE' and no other COVID-19 attributes exist.
         - 'NEG': Otherwise.
"""
    if 'positive' in group.values:
        return 'POS'
    elif 'uncertain' in group.values:
        return 'UNK'
    elif 'negated' in group.values:
        return 'NEG'
    else:
        return 'UNK'

sess.register_agg('agg_doc_tags',AggDocumentTags,[str],[str])

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 104
doc_tags = sess.export('?DocumentTags(P,T)')
doc_tags

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 105
paths = sess.export('?Files(P)')
paths

# %% ../../nbs/covid-nlp/000_covid_pipeline.ipynb 107
classification = paths.merge(doc_tags,on='P',how='outer')
classification['T']=classification['T'].fillna('UNK')
classification
