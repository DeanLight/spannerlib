{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid-19 NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Introduction:\n",
    "In this tutorial, we will guide you through the process of re-writing an existing data pipeline into the rgxlog framework, allowing you to witness a real-world example of the framework's benefits. And to offer further tutorials on advanced applications of the rgxlog framework.\n",
    "We've chosen to adapt a pipeline from the field of NLP, specifically the Covid-19 NLP pipeline, which was a part of a published [paper](https://aclanthology.org/2020.nlpcovid19-acl.10.pdf) in 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding The Covid-19 NLP Pipeline:\n",
    "The pipline repository [link](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/).\n",
    "\n",
    "\n",
    "The primary objective of the NLP pipeline is to identify individuals who have been positively diagnosed with COVID-19 by extracting pertinent information from unstructured free-text narratives found within the Electronic Health Record (EHR) of the Department of Veterans Affairs (VA). By automating this process, the pipeline streamlines the screening of a substantial volume of clinical text, significantly reducing the time and effort required for identification.\n",
    "The pipeline is built on medSpacy framework, and defines a new UI to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipline Stages:\n",
    "![image-6.png](covid_pipeline_files/image-6.png)\n",
    "- preprocessor: Modifies the preprocessed text\n",
    "- concept tagger: Assigns a semantic tag in a custom attribute to each Token, which helps with concept extraction and normalization.\n",
    "- target matcher: Extracts spans using rules, based on the concept tagger.\n",
    "- sectionizer: Identifies note section headers in the text and assigns section titles to entities and tokens contained in that section.\n",
    "- context: Identifies semantic modifiers of entities and asserts attributes such as positive status, negation, and other experiencier.\n",
    "- postprocessor: Modifies or removes the entity based on business logic. This handles special cases or complex logic using the results of earlier stages.\n",
    "- document classifier: Assigns a label of \"POS\", \"UNK\", or \"NEG\" to each file, A document will be classified as positive if it has at least one positive, non-excluded entity\n",
    "We will explain about each stage in more details later on.\n",
    "\n",
    "At each stage, we encountered unique challenges. Each stage involved working with different classes and medSpacy framework attributes. Our approach was to first understand the original implementation, seek opportunities to simplify it, and then rewrite it in rgxlog. What was possible we wrote it declaratively using rules, facts, and queries. Additionally, if any essential functionalities were missing in the library, we built IE functions.\n",
    "As we mentioned the pipline uses medSpacy framework so first we need to import some libraries and install some requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Of The Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up The Environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install some requirements to work with [medspacy](https://github.com/medspacy/medspacy) framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /miniconda/lib/python3.8/site-packages (3.7.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /miniconda/lib/python3.8/site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /miniconda/lib/python3.8/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /miniconda/lib/python3.8/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: jinja2 in /miniconda/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /miniconda/lib/python3.8/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /miniconda/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /miniconda/lib/python3.8/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /miniconda/lib/python3.8/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /miniconda/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /miniconda/lib/python3.8/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /miniconda/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /miniconda/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /miniconda/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /miniconda/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /miniconda/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /miniconda/lib/python3.8/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /miniconda/lib/python3.8/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /miniconda/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /miniconda/lib/python3.8/site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /miniconda/lib/python3.8/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /miniconda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /miniconda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /miniconda/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /miniconda/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /miniconda/lib/python3.8/site-packages (from jinja2->spacy) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /miniconda/lib/python3.8/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: setuptools in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.6.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: jinja2 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (22.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.6.2)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /miniconda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /miniconda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /miniconda/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /miniconda/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /miniconda/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the rgxlog framework and import what we need from the rgxlog framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "current_python=f\"{sys.executable}\"\n",
    "package_path=Path(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /spanner_workbench\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nbdev in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (2.3.13)\n",
      "Requirement already satisfied: pandas in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (2.0.1)\n",
      "Requirement already satisfied: notebook in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (6.5.4)\n",
      "Requirement already satisfied: pyDatalog in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.17.4)\n",
      "Requirement already satisfied: tabulate in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.9.0)\n",
      "Requirement already satisfied: lark-parser>=0.9.0 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.12.0)\n",
      "Requirement already satisfied: ipython>=7.18.1 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (8.12.2)\n",
      "Requirement already satisfied: setuptools>=50.2.0 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (65.6.3)\n",
      "Requirement already satisfied: networkx>=2.5 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (3.1)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.6.2)\n",
      "Requirement already satisfied: jsonpath-ng in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (1.5.3)\n",
      "Requirement already satisfied: psutil in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (5.9.5)\n",
      "Requirement already satisfied: install-jdk in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (1.0.4)\n",
      "Requirement already satisfied: parse in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (1.19.0)\n",
      "Requirement already satisfied: spanner-nlp in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.0.6)\n",
      "Requirement already satisfied: pytest in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (7.3.1)\n",
      "Requirement already satisfied: Jinja2 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (3.1.2)\n",
      "Requirement already satisfied: pycodestyle in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (2.10.0)\n",
      "Requirement already satisfied: mypy in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: matplotlib-inline in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.18.2)\n",
      "Requirement already satisfied: traitlets>=5 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (5.9.0)\n",
      "Requirement already satisfied: pickleshare in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.7.5)\n",
      "Requirement already satisfied: typing-extensions in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (4.6.2)\n",
      "Requirement already satisfied: decorator in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (3.0.38)\n",
      "Requirement already satisfied: stack-data in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.6.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (2.15.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (4.8.0)\n",
      "Requirement already satisfied: backcall in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /miniconda/lib/python3.8/site-packages (from Jinja2->rgxlog==0.0.1) (2.1.2)\n",
      "Requirement already satisfied: ply in /miniconda/lib/python3.8/site-packages (from jsonpath-ng->rgxlog==0.0.1) (3.11)\n",
      "Requirement already satisfied: six in /miniconda/lib/python3.8/site-packages (from jsonpath-ng->rgxlog==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /miniconda/lib/python3.8/site-packages (from mypy->rgxlog==0.0.1) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /miniconda/lib/python3.8/site-packages (from mypy->rgxlog==0.0.1) (2.0.1)\n",
      "Requirement already satisfied: watchdog in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (3.0.0)\n",
      "Requirement already satisfied: PyYAML in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (6.0)\n",
      "Requirement already satisfied: execnb>=0.1.4 in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (0.1.5)\n",
      "Requirement already satisfied: ipywidgets<=8.0.4 in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (8.0.4)\n",
      "Requirement already satisfied: asttokens in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (2.2.1)\n",
      "Requirement already satisfied: ghapi>=1.0.3 in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (1.0.4)\n",
      "Requirement already satisfied: fastcore>=1.5.27 in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (1.5.29)\n",
      "Requirement already satisfied: astunparse in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (1.6.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (0.17.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (1.8.2)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (8.2.0)\n",
      "Requirement already satisfied: argon2-cffi in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (21.3.0)\n",
      "Requirement already satisfied: ipykernel in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (6.23.1)\n",
      "Requirement already satisfied: prometheus-client in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (0.17.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (6.3.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (5.3.0)\n",
      "Requirement already satisfied: nbformat in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (5.8.0)\n",
      "Requirement already satisfied: nbconvert>=5 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (7.4.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (1.5.6)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (1.0.0)\n",
      "Requirement already satisfied: ipython-genutils in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (0.2.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /miniconda/lib/python3.8/site-packages (from pandas->rgxlog==0.0.1) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /miniconda/lib/python3.8/site-packages (from pandas->rgxlog==0.0.1) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /miniconda/lib/python3.8/site-packages (from pandas->rgxlog==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /miniconda/lib/python3.8/site-packages (from pandas->rgxlog==0.0.1) (2023.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /miniconda/lib/python3.8/site-packages (from pytest->rgxlog==0.0.1) (1.1.1)\n",
      "Requirement already satisfied: iniconfig in /miniconda/lib/python3.8/site-packages (from pytest->rgxlog==0.0.1) (2.0.0)\n",
      "Requirement already satisfied: packaging in /miniconda/lib/python3.8/site-packages (from pytest->rgxlog==0.0.1) (22.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /miniconda/lib/python3.8/site-packages (from pytest->rgxlog==0.0.1) (1.0.0)\n",
      "Requirement already satisfied: requests in /miniconda/lib/python3.8/site-packages (from spanner-nlp->rgxlog==0.0.1) (2.28.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /miniconda/lib/python3.8/site-packages (from fastcore>=1.5.27->nbdev->rgxlog==0.0.1) (22.3.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /miniconda/lib/python3.8/site-packages (from ipywidgets<=8.0.4->nbdev->rgxlog==0.0.1) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /miniconda/lib/python3.8/site-packages (from ipywidgets<=8.0.4->nbdev->rgxlog==0.0.1) (3.0.9)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /miniconda/lib/python3.8/site-packages (from ipykernel->notebook->rgxlog==0.0.1) (1.6.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in /miniconda/lib/python3.8/site-packages (from ipykernel->notebook->rgxlog==0.0.1) (0.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /miniconda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.18.1->rgxlog==0.0.1) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /miniconda/lib/python3.8/site-packages (from jupyter-client>=5.3.4->notebook->rgxlog==0.0.1) (6.6.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /miniconda/lib/python3.8/site-packages (from jupyter-core>=4.6.1->notebook->rgxlog==0.0.1) (3.5.1)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /miniconda/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.2.3)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /miniconda/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (2.6.0)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (2.0.5)\n",
      "Requirement already satisfied: defusedxml in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (0.7.1)\n",
      "Requirement already satisfied: tinycss2 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (1.2.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (0.8.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (0.2.2)\n",
      "Requirement already satisfied: bleach in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (6.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (4.12.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /miniconda/lib/python3.8/site-packages (from nbformat->notebook->rgxlog==0.0.1) (4.17.3)\n",
      "Requirement already satisfied: fastjsonschema in /miniconda/lib/python3.8/site-packages (from nbformat->notebook->rgxlog==0.0.1) (2.17.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /miniconda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.18.1->rgxlog==0.0.1) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /miniconda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.18.1->rgxlog==0.0.1) (0.2.6)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /miniconda/lib/python3.8/site-packages (from argon2-cffi->notebook->rgxlog==0.0.1) (21.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /miniconda/lib/python3.8/site-packages (from astunparse->nbdev->rgxlog==0.0.1) (0.37.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp->rgxlog==0.0.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp->rgxlog==0.0.1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp->rgxlog==0.0.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp->rgxlog==0.0.1) (1.25.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in /miniconda/lib/python3.8/site-packages (from stack-data->ipython>=7.18.1->rgxlog==0.0.1) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in /miniconda/lib/python3.8/site-packages (from stack-data->ipython>=7.18.1->rgxlog==0.0.1) (0.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /miniconda/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=5.3.4->notebook->rgxlog==0.0.1) (3.15.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (5.12.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.3.10)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (0.19.3)\n",
      "Requirement already satisfied: jupyter-server-terminals in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.4.4)\n",
      "Requirement already satisfied: overrides in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (7.3.1)\n",
      "Requirement already satisfied: websocket-client in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (1.5.2)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.6.3)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (3.6.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /miniconda/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->rgxlog==0.0.1) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /miniconda/lib/python3.8/site-packages (from beautifulsoup4->nbconvert>=5->notebook->rgxlog==0.0.1) (2.4.1)\n",
      "Requirement already satisfied: webencodings in /miniconda/lib/python3.8/site-packages (from bleach->nbconvert>=5->notebook->rgxlog==0.0.1) (0.5.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /miniconda/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /miniconda/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->rgxlog==0.0.1) (2.21)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /miniconda/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.1.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /miniconda/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /miniconda/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.1.4)\n",
      "Requirement already satisfied: isoduration in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (2.1)\n",
      "Requirement already satisfied: uri-template in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.2.0)\n",
      "Requirement already satisfied: fqdn in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.5.1)\n",
      "Requirement already satisfied: webcolors>=1.11 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arrow>=0.15.0 in /miniconda/lib/python3.8/site-packages (from isoduration->jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.2.3)\n",
      "Building wheels for collected packages: rgxlog\n",
      "  Building wheel for rgxlog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rgxlog: filename=rgxlog-0.0.1-py3-none-any.whl size=89487 sha256=9ead8b269021048b9b4cdb511024e427700155f882646d78334badf344607fea\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n0deoiky/wheels/62/da/4d/48e3d3b55711eef8154983104097b04f07d8a68aa8ee1aa6ed\n",
      "Successfully built rgxlog\n",
      "Installing collected packages: rgxlog\n",
      "  Attempting uninstall: rgxlog\n",
      "    Found existing installation: rgxlog 0.0.1\n",
      "    Uninstalling rgxlog-0.0.1:\n",
      "      Successfully uninstalled rgxlog-0.0.1\n",
      "Successfully installed rgxlog-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "! {current_python} -m pip install {package_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import rgxlog\n",
    "from rgxlog.session import Session\n",
    "from rgxlog.primitive_types import Span, DataTypes\n",
    "from rgxlog import magic_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some generic ie functions that will be used in every stage of the pipline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(text_path):\n",
    "    \"\"\"\n",
    "    Reads from file and return it's content.\n",
    "\n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to read from.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file.\n",
    "    \"\"\"\n",
    "    with open(f\"{text_path}\", 'r') as file:\n",
    "        content = file.read()\n",
    "    yield content\n",
    "\n",
    "magic_session.register(ie_function=read_from_file,\n",
    "                       ie_function_name = \"read_from_file\",\n",
    "                       in_rel=[DataTypes.string],\n",
    "                       out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "for value in read_from_file(\"sample1.txt\"):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Print the contents of a CSV file in a human-readable format.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', newline='') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            print(','.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Age,Occupation\n",
      "John,25,Engineer\n",
      "Jane,30,Doctor\n",
      "Bob,28,Teacher\n",
      "Alice,22,Student\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "\n",
    "data_to_write = [\n",
    "    ['Name', 'Age', 'Occupation'],\n",
    "    ['John', '25', 'Engineer'],\n",
    "    ['Jane', '30', 'Doctor'],\n",
    "    ['Bob', '28', 'Teacher'],\n",
    "    ['Alice', '22', 'Student']\n",
    "]\n",
    "\n",
    "with open('example.csv', 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerows(data_to_write)\n",
    "\n",
    "print_csv_file('example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_containing_span(spans):\n",
    "    \"\"\"\n",
    "    This function takes a list of spans, where each span is represented\n",
    "    as a list containing a label and a span (interval). It resolves overlaps\n",
    "    by selecting the containing span, favoring the larger span in case of conflicts.\n",
    "\n",
    "    Parameters:\n",
    "    spans (list of lists): A list of spans, where each span is represented\n",
    "        as a list [label, span].\n",
    "\n",
    "    Returns:\n",
    "    list of lists: A list of resolved spans, where each span is a list\n",
    "        [label, span], with conflicts resolved by selecting the containing span.\n",
    "    \"\"\"\n",
    "    # Sort the replacements by the size of the spans in descending order\n",
    "    spans.sort(key=lambda x: x[1].span_end - x[1].span_start, reverse=True)\n",
    "\n",
    "    # Initialize a list to keep track of intervals that have been replaced\n",
    "    resolved_spans = []\n",
    "    \n",
    "    for label, span in spans:\n",
    "        conflict = False\n",
    "\n",
    "        for _, existing_span in resolved_spans:\n",
    "            existing_start = existing_span.span_start\n",
    "            existing_end = existing_span.span_end\n",
    "\n",
    "            if not (span.span_end <= existing_start or span.span_start >= existing_end):\n",
    "                conflict = True\n",
    "                break\n",
    "\n",
    "        if not conflict:\n",
    "            resolved_spans.append([label, span])\n",
    "\n",
    "    return resolved_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Label1, Span: 2-8\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "spans = [ \n",
    "     ['Label1', Span(2, 8)],\n",
    "     ['Label2', Span(5, 8)]\n",
    " ]\n",
    "resolved_spans = select_containing_span(spans)\n",
    "for label, span in resolved_spans:\n",
    "    print(f\"Label: {label}, Span: {span.span_start}-{span.span_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_spans(spans_table, paths_table, session):\n",
    "    \"\"\"\n",
    "    This function takes tables a spans tables and path table for the files paths,\n",
    "    it generate queries for the tables, executes the queries using the given session, processes the results, \n",
    "    and replaces specific spans in a text with the corresponding labels, it first\n",
    "    resolve spans overlapping conflicts for each giving path.\n",
    "\n",
    "    Parameters:\n",
    "    spans_table (str): A string representing the spans table to process, table columns are formated as (Label, Span, Path).\n",
    "    paths_table (str): A string representing the paths table to process, table columns are formated as (Path)\n",
    "    session: the session in the rgxlog to run the queries at.\n",
    "\n",
    "    Returns:\n",
    "    str: The adjusted text string with the new labels.\n",
    "    \"\"\"\n",
    "    # Get a list of all the paths\n",
    "    paths = session.run_commands(f\"?{paths_table}(Path)\", print_results=False, format_results=True)\n",
    "    paths = paths[0].values.tolist()\n",
    "    for path_list in paths:\n",
    "        path = path_list[0]\n",
    "\n",
    "        # Generate a spans query for each path, the query will be formates as (Label, Span, Path)\n",
    "        results = magic_session.run_commands(f'?{spans_table}(Label, Span, \"{path}\")', print_results=True, format_results=True)\n",
    "        if len(results[0]) == 0:\n",
    "            continue\n",
    "        # replacments is list of lists where each list is a [Label, Span]\n",
    "        replacements = results[0].values.tolist()\n",
    "        \n",
    "        with open(f\"{path}\", 'r') as file:\n",
    "            adjusted_string = file.read()\n",
    "    \n",
    "        # Resolve spans conflicts\n",
    "        resolved_replacements = select_containing_span(replacements)\n",
    "    \n",
    "        # Sort the resolved replacements by the starting index of each span in descending order\n",
    "        resolved_replacements.sort(key=lambda x: x[1].span_start, reverse=True)\n",
    "    \n",
    "        # iterate over the resolved query results and replace the space with the corresponding label\n",
    "        for i in range(len(resolved_replacements)):\n",
    "            replace_string, span = resolved_replacements[i]\n",
    "            replace_length = len(replace_string)\n",
    "            adjusted_string = adjusted_string[:span.span_start] + replace_string + adjusted_string[span.span_end:]\n",
    "    \n",
    "        with open(f\"{path}\", 'w') as file:\n",
    "            file.writelines(adjusted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'sampleMatches(Label, Span, \"example.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " Covid-19 | [12, 29)\n",
      "\n",
      "The boy has Covid-19\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "with open('example.txt', 'w') as file:\n",
    "    file.write('The boy has novel coronavirus')\n",
    "%rgxlog new samplePaths(str)\n",
    "%rgxlog samplePaths(\"example.txt\")\n",
    "%rgxlog new sampleMatches(str, span, str)\n",
    "%rgxlog sampleMatches(\"Covid-19\", [12,29), \"example.txt\")\n",
    "replace_spans('sampleMatches', 'samplePaths', magic_session)\n",
    "for value in read_from_file(\"example.txt\"):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_span_contained(span1, span2):\n",
    "    \"\"\"\n",
    "    Checks if one span is contained within the other span and returns the smaller span if yes.\n",
    "\n",
    "    Parameters:\n",
    "        span1 (span)\n",
    "        span2 (span)\n",
    "\n",
    "    Returns:\n",
    "        span: span1 if contained within span2 or vice versa, or None if not contained.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1.span_start, span1.span_end\n",
    "    start2, end2 = span2.span_start, span2.span_end\n",
    "    \n",
    "    if start2 <= start1 and end1 <= end2:\n",
    "        yield span1\n",
    "        \n",
    "    elif start1 <= start2 and end2 <= end1:\n",
    "        yield span2\n",
    "\n",
    "magic_session.register(is_span_contained, \"is_span_contained\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Span: 8-9\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "span1 = Span(2, 12)\n",
    "span2 = Span(8, 9)\n",
    "for span in is_span_contained(span1, span2):\n",
    "    print(f\" Span: {span.span_start}-{span.span_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_span(span1, span2):\n",
    "    \"\"\"\n",
    "    Computes the relative position of the conatined span within the other span.\n",
    "\n",
    "    Parameters:\n",
    "        span1 (Span): The first span object.\n",
    "        span2 (Span): The second span object.\n",
    "\n",
    "    Returns:\n",
    "        Span: The new relative span of the contained one.\n",
    "        None: If there's no span contained within the other.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1.span_start, span1.span_end\n",
    "    start2, end2 = span2.span_start, span2.span_end\n",
    "    \n",
    "    if start2 <= start1 and end1 <= end2:\n",
    "        yield Span(span1.span_start - span2.span_start, span1.span_end - span2.span_start)\n",
    "        \n",
    "    elif start1 <= start2 and end2 <= end1:\n",
    "        yield Span(span2.span_start - span1.span_start, span2.span_end - span1.span_start)\n",
    "\n",
    "magic_session.register(get_relative_span, \"get_relative_span\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Span: 0-3\n"
     ]
    }
   ],
   "source": [
    "# usage example \n",
    "span1 = Span(2, 12)\n",
    "span2 = Span(2, 5)\n",
    "for span in get_relative_span(span1, span2):\n",
    "    print(f\" Span: {span.span_start}-{span.span_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenization(text_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, processes its content using spaCy's English language model,\n",
    "    tokenizing it into sentences and returns each individual sentence in the processed text using a generator.\n",
    "    \n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be annotated.\n",
    "\n",
    "    Returns:\n",
    "        str: Individual sentences extracted from the input text.\n",
    "    \"\"\"\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        yield sentence.text\n",
    "\n",
    "magic_session.register(ie_function=sent_tokenization, ie_function_name = \"sent_tokenization\", in_rel=[DataTypes.string], out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient presents to be tested for COVID-19.\n",
      "His wife recently tested positive for novel coronavirus.\n",
      "SARS-COV-2 results came back positive.\n"
     ]
    }
   ],
   "source": [
    "# usage example \n",
    "for sentence in sent_tokenization(\"sample1.txt\"):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Input:\n",
    "The paths of the text files to be classified should be written in \"files_paths.csv\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1.txt\n",
      "sample2.txt\n",
      "sample3.txt\n",
      "sample4.txt\n",
      "sample5.txt\n",
      "sample6.txt\n",
      "sample7.txt\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('files_paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_relation_from_csv(\"files_paths.csv\", relation_name=\"FilesPaths\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial files contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                   Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | Patient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n",
      " sample2.txt |                                         The patient was tested for COVID-19. Results are positive.\n",
      " sample3.txt |                                            Problem List: 1. Pneumonia 2. Novel Coronavirus 2019\n",
      " sample4.txt |                                                           neg COVID-19 education.\n",
      " sample5.txt |                                                        positive COVID-19 precaution.\n",
      " sample6.txt |                                            The patient have reported positive COVID-19 exposure.\n",
      " sample7.txt |                              Elevated cholesterol levels require further assessment and lifestyle adjustments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "FilesContent(Path, Content) <- FilesPaths(Path), read_from_file(Path) -> (Content)\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue we have to do some pre processing to help ease the next stages, we will certain words in each list to it's lemma forms, here a list of the words that we want to lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "man\n",
      "woman\n",
      "have\n",
      "do\n",
      "emergency\n",
      "epidemic\n",
      "outbreak\n",
      "crisis\n",
      "breakout\n",
      "pandemic\n",
      "spread\n",
      "confirm\n",
      "person\n",
      "patient\n",
      "veteran\n",
      "limit\n",
      "reduce\n",
      "factor\n",
      "contact\n",
      "case\n",
      "lower\n",
      "minimize\n",
      "risk\n",
      "chance\n",
      "possibility\n",
      "care\n",
      "clean\n",
      "desire\n",
      "flight\n",
      "trip\n",
      "plan\n",
      "reschedule\n",
      "postpone\n",
      "barrier\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('lemma_words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a helper method to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text_path, lemma_words_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, lemmatizes its content using spaCy's English language model,\n",
    "    and replaces certain words with their lemmas the rest will remain the same. The updated text is then written back to the same file.\n",
    "\n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be lemmatized.\n",
    "        lemma_words_path(str): The path that contains the list of words to be lemmatized\n",
    "\n",
    "    Returns:\n",
    "        str: The lemmatized text.\n",
    "    \"\"\"\n",
    "    # Define a list of words to be lemmatized\n",
    "    lemma_words = [line.strip() for line in open(f\"{lemma_words_path}\") if line.strip()]\n",
    "\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    lemmatized_text = \"\"\n",
    "    for token in doc:\n",
    "        if token.lemma_ in lemma_words:\n",
    "            lemmatized_text += token.lemma_\n",
    "        elif token.like_num:\n",
    "            lemmatized_text += \"like_num\"\n",
    "        else:\n",
    "            lemmatized_text += token.text\n",
    "        lemmatized_text += \" \"\n",
    "\n",
    "    # Write the lemmatized text back to the same file\n",
    "    with open(text_path, 'w') as file:\n",
    "        file.writelines(lemmatized_text)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy be sick \n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "with open('example.txt', 'w') as file:\n",
    "    file.write('The boy was sick')\n",
    "lemmatized_text = lemmatize_text('example.txt', 'lemma_words.txt')\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the texts to lemmatize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files_paths.csv', 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        path = row[0]\n",
    "        lemmatize_text(path, 'lemma_words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see for example in sample2.txt, was has changed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                     Content\n",
      "-------------+--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for novel coronavirus . SARS - COV-2 results came back positive .\n",
      " sample2.txt |                                            The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                                    Problem List : like_num . Pneumonia like_num . Novel Coronavirus like_num\n",
      " sample4.txt |                                                             neg COVID-19 education .\n",
      " sample5.txt |                                                          positive COVID-19 precaution .\n",
      " sample6.txt |                                              The patient have reported positive COVID-19 exposure .\n",
      " sample7.txt |                                Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Concept Tagger](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/concept_tag_rules.py):\n",
    "Concept tag rules, also known as pattern-based rules or custom rules, are a way to specify and define patterns that an NLP (Natural Language Processing) system should recognize within text data. These rules are used to identify specific concepts or entities within text documents. In the context of MedSpaCy and medical NLP, concept tag rules are often used to identify medical entities and concepts accurately.\n",
    "\n",
    "In the orginal project they used the TargetRule class which defines a rule for identifying a specific concept or entity in text.\n",
    "each concept Target Rule looks like this:\n",
    "\n",
    "TargetRule(\n",
    "            literal=\"coronavirus\",\n",
    "            category=\"COVID-19\",\n",
    "            pattern=[{\"LOWER\": {\"REGEX\": \"coronavirus|hcov|ncov$\"}}],\n",
    "          )\n",
    "\n",
    "**Literal** : This specifies the literal text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the category or label associated with the identified entity.\n",
    "\n",
    "**Pattern** : This defines the pattern or conditions under which the entity should be recognized. It's a list of dictionaries specifying conditions for token matching. These rules some times used lemma attribute or POS of each token. A documentation can be found at : https://spacy.io/usage/rule-based-matching.\n",
    "\n",
    "Instead what we did is to define regex patterns, we have added these pattern in concept_target_rules.csv file, there are two types of these patterns lemma and pos, that we will implement each later on.\n",
    "Each rule in the csv file is like this : regexPattern, label, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:hcov|covid(?:(?:-)?(?:\\s)?19|10)?|2019-cov|cov2|ncov-19|covd 19|no-cov|sars cov),COVID-19,lemma\n",
      "(?i)(?:coivid|(?:novel )?corona(?:virus)?(?: (?:20)?19)?|sars(?:\\s)?(?:-)?(?:\\s)?cov(?:id)?(?:-)?(?:2|19)),COVID-19,lemma\n",
      "(?i)(?:\\+(?: ve)?|\\(\\+\\)|positive|\\bpos\\b|active|confirmed),positive,lemma\n",
      "(?i)(?:pneum(?:onia)?|pna|hypoxia|septic shoc|ards\\(?(?:(?:[12])/2)\\)?|(?:hypoxemic|acute|severe)? resp(?:iratory)? failure(?:\\(?(?:[12]/2)\\)?)?)\",associated_diagnosis,lemma\n",
      "(?i)(?:(?:diagnos(?:is|ed)|dx(?:\\.)?)(?:of|with)?),diagnosis,lemma\n",
      "(?i)(?:^screen),screening,lemma\n",
      "(?i)(?:in contact with|any one|co-worker|at work|(?:the|a)(?:wo)?man|(?:another|a) (?:pt|patient|pt\\.)),other_experiencer,lemma\n",
      "(?i)(?:patient|pt(?:\\.)?|vt|veteran),patient,lemma\n",
      "(?i)(?:like_num (?:days|day|weeks|week|months|month) (?:ago|prior)),timesx,lemma\n",
      "(?i)(?:(?:antibody|antibodies|ab) test),antibody test,lemma\n",
      "(?i)(?:(?:coronavirus|hcovs?|ncovs?|covs?)(?:\\s)?(?:-)?(?:\\s)?(?: infection)?(?: strain)?(?:\\s)?(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63) (?:coronavirus|hcovs?|ncovs?|covs?)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:non(?:\\s)?(?:-)?(?:\\s)?(?:novel|covid|ncovid|covid-19)(?: coronavirus)?|other coronavirus),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:wife|husband|spouse|family|member|girlfriend|boyfriend|mother|father|nephew|niece|grandparent|grandparents|granddaughter|relative|relatives|caregiver),family,pos\n",
      "(?i)(?:grandchild|grandson|cousin|grandmother|grandfather|parent|son|daughter|mom|dad|brother|sister|aunt|uncle|child|children|sibling|siblings),family,pos\n",
      "(?i)(?:someone|somebody|person|anyone|anybody|people|individual|individuals|teacher|anybody|employees|employer|customer|client|residents),other_experiencer,pos\n",
      "(?i)(?:resident|pts|patients|coworker|coworkers|workers|colleague|captain|captains|pilot|pilots|sailor|sailors|meeting),other_experiencer,pos\n",
      "(?i)(?:boyfriend|persons|person|church|convention|guest|party|attendee|conference|roommate|friend|friends|coach|player|neighbor|manager|boss),other_experiencer,pos\n",
      "(?i)(?:cashier|landlord|worked|works|^mate|nobody|mates|housemate|housemates|hotel|soldier|airport|tsa|lady|ladies|lobby|staffer|staffers),other_experiencer,pos\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('concept_tags_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_relation_from_csv(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma Rules:\n",
    "Lemma rules are rules that used the attribute _lemma of each token in the NLP, we already lemmatized the texts, so now we can create a regex patterns for that.\n",
    "\n",
    "Example for a lemma rule from the original NLP:\n",
    "\n",
    "        TargetRule(\n",
    "            \"results positive\",\n",
    "            \"positive\",\n",
    "            pattern=[\n",
    "                {\"LOWER\": \"results\"},\n",
    "                {\"LEMMA\": \"be\", \"OP\": \"?\"},\n",
    "                {\"LOWER\": {\"IN\": [\"pos\", \"positive\"]}},\n",
    "            ],\n",
    "        ),\n",
    "We used the py_rgx_span to capture the patterns, and will use the spans later on in replace_spans that will replace each span with the correct label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%rgxlog\n",
    "LemmaMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"lemma\"), py_rgx_span(Content, Pattern) -> (Span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                     Content\n",
      "-------------+--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for novel coronavirus . SARS - COV-2 results came back positive .\n",
      " sample2.txt |                                            The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                                    Problem List : like_num . Pneumonia like_num . Novel Coronavirus like_num\n",
      " sample4.txt |                                                             neg COVID-19 education .\n",
      " sample5.txt |                                                          positive COVID-19 precaution .\n",
      " sample6.txt |                                              The patient have reported positive COVID-19 exposure .\n",
      " sample7.txt |                                Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'LemmaMatches(Label, Span, \"sample1.txt\")':\n",
      "  Label   |    Span\n",
      "----------+------------\n",
      " positive | [134, 142)\n",
      " positive |  [70, 78)\n",
      " COVID-19 | [103, 115)\n",
      " COVID-19 | [83, 100)\n",
      " COVID-19 |  [34, 42)\n",
      " patient  |   [0, 7)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample2.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " positive | [48, 56)\n",
      " COVID-19 | [26, 34)\n",
      " patient  | [4, 11)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample3.txt\")':\n",
      "        Label         |   Span\n",
      "----------------------+----------\n",
      "       COVID-19       | [47, 64)\n",
      " associated_diagnosis | [26, 35)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample4.txt\")':\n",
      "  Label   |  Span\n",
      "----------+---------\n",
      " COVID-19 | [4, 12)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample5.txt\")':\n",
      "  Label   |  Span\n",
      "----------+---------\n",
      " positive | [0, 8)\n",
      " COVID-19 | [9, 17)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample6.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " positive | [26, 34)\n",
      " COVID-19 | [35, 43)\n",
      " patient  | [4, 11)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace the matches with the correct label\n",
    "replace_spans(\"LemmaMatches\", \"FilesPaths\", magic_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After:\n",
    "As we can see for example in the sample1.txt, every other covid-19 name was changed to COVID-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                               Content\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                     The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                             Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                      neg COVID-19 education .\n",
      " sample5.txt |                                                   positive COVID-19 precaution .\n",
      " sample6.txt |                                       The patient have reported positive COVID-19 exposure .\n",
      " sample7.txt |                         Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Rules:\n",
    "As we mentioned above these rules used the POS attribute of each token, there were a small number of rules so we only used this to the tokens we needed.\n",
    "Example of the a rule from the original NLP:\n",
    "\n",
    "        TargetRule(\n",
    "            \"other experiencer\",\n",
    "            category=\"other_experiencer\",\n",
    "            pattern=[\n",
    "                {\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]},\n",
    "                    \"LOWER\": {\n",
    "                        \"IN\": [\n",
    "                            \"someone\",\n",
    "                            \"somebody\",\n",
    "                            \"person\",\n",
    "                            \"anyone\",\n",
    "                            \"anybody\",\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "\n",
    "The patterns we've defined will match words listed under \"IN\", We specifically capture words if their Part-of-Speech (POS) falls into one of the categories: [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]. To accomplish this, two functions are employed: the first function determines the POS of each token, and the second one, py_rgx_span, captures the predefined patterns. After matching words, We confirm the accurate POS tags of the matched words using spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text_with_pos(text_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, processes its content using spaCy's English language model,\n",
    "    and returns a tuple of (POS, Span) for each token if it's one of NOUN|PROPN|PRON|ADJ\n",
    "    otherwise an empty tuple will be returned\n",
    "    \n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be annotated.\n",
    "\n",
    "    Returns:\n",
    "        tuple(str, Span): The POS of the token and it's span\n",
    "    \"\"\"\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]:\n",
    "            yield token.pos_, Span(token.idx, token.idx + len(token.text))\n",
    "        else:\n",
    "            yield tuple()\n",
    "magic_session.register(ie_function=annotate_text_with_pos, ie_function_name = \"annotate_text_with_pos\", in_rel=[DataTypes.string], out_rel=[DataTypes.string, DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ, (0,4)\n",
      "NOUN, (5,8)\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "with open('example.txt', 'w') as file:\n",
    "    file.write('sick boy')\n",
    "\n",
    "for POS, span in annotate_text_with_pos('example.txt'):\n",
    "    print(f\"{POS}, ({span.span_start},{span.span_end})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'POSTable(POS, Span, Path)':\n",
      "  POS  |    Span    |    Path\n",
      "-------+------------+-------------\n",
      "  ADJ  |   [0, 7)   | sample1.txt\n",
      "  ADJ  | [121, 129) | sample1.txt\n",
      "  ADJ  |  [70, 78)  | sample1.txt\n",
      " NOUN  | [103, 110) | sample1.txt\n",
      " NOUN  |  [49, 53)  | sample1.txt\n",
      " NOUN  |  [8, 16)   | sample1.txt\n",
      " PRON  |  [45, 48)  | sample1.txt\n",
      " PROPN |  [34, 42)  | sample1.txt\n",
      " PROPN |  [83, 91)  | sample1.txt\n",
      " PROPN | [94, 102)  | sample1.txt\n",
      "  ADJ  |  [48, 56)  | sample2.txt\n",
      " NOUN  |  [37, 44)  | sample2.txt\n",
      " NOUN  |  [4, 11)   | sample2.txt\n",
      " PROPN |  [26, 34)  | sample2.txt\n",
      " PROPN |   [0, 7)   | sample3.txt\n",
      " PROPN |  [15, 23)  | sample3.txt\n",
      " PROPN |  [47, 55)  | sample3.txt\n",
      " PROPN |  [58, 66)  | sample3.txt\n",
      " PROPN |  [67, 75)  | sample3.txt\n",
      " PROPN |  [8, 12)   | sample3.txt\n",
      " NOUN  |  [13, 22)  | sample4.txt\n",
      " PROPN |   [0, 3)   | sample4.txt\n",
      " PROPN |  [4, 12)   | sample4.txt\n",
      "  ADJ  |   [0, 8)   | sample5.txt\n",
      " NOUN  |  [18, 28)  | sample5.txt\n",
      " PROPN |  [9, 17)   | sample5.txt\n",
      "  ADJ  |  [26, 34)  | sample6.txt\n",
      " NOUN  |  [4, 11)   | sample6.txt\n",
      " NOUN  |  [44, 52)  | sample6.txt\n",
      " PROPN |  [35, 43)  | sample6.txt\n",
      "  ADJ  |   [0, 8)   | sample7.txt\n",
      "  ADJ  |  [36, 43)  | sample7.txt\n",
      " NOUN  |  [21, 27)  | sample7.txt\n",
      " NOUN  |  [44, 54)  | sample7.txt\n",
      " NOUN  |  [59, 68)  | sample7.txt\n",
      " NOUN  |  [69, 80)  | sample7.txt\n",
      " NOUN  |  [9, 20)   | sample7.txt\n",
      "\n",
      "printing results for query 'POSMatches(Label, Span, Path)':\n",
      "  Label  |   Span   |    Path\n",
      "---------+----------+-------------\n",
      " family  | [49, 53) | sample1.txt\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, Path)':\n",
      "  Label  |   Span   |    Path\n",
      "---------+----------+-------------\n",
      " family  | [49, 53) | sample1.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "POSTable(POS, Span, Path) <- FilesContent(Path, Content), annotate_text_with_pos(Path) -> (POS, Span)\n",
    "?POSTable(POS, Span, Path)\n",
    "\n",
    "POSMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"pos\"), py_rgx_span(Content, Pattern) -> (Span)\n",
    "?POSMatches(Label, Span, Path)\n",
    "\n",
    "POSRuleMatches(Label, Span, Path) <- POSTable(POS, Span, Path), POSMatches(Label, Span, Path)\n",
    "?POSRuleMatches(Label, Span, Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                               Content\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                     The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                             Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                      neg COVID-19 education .\n",
      " sample5.txt |                                                   positive COVID-19 precaution .\n",
      " sample6.txt |                                       The patient have reported positive COVID-19 exposure .\n",
      " sample7.txt |                         Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'POSRuleMatches(Label, Span, \"sample1.txt\")':\n",
      "  Label  |   Span\n",
      "---------+----------\n",
      " family  | [49, 53)\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample3.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample6.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace the matches with the correct label\n",
    "replace_spans(\"POSRuleMatches\", \"FilesPaths\", magic_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After:\n",
    "As we can see for example in sample1.txt, wife has changed to family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                        The patient have reported positive COVID-19 exposure .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Target Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/target_rules.py):\n",
    "These rules used the label that was assigned through the concept tagger, to capture some more complex patterns and assign a label for them inorder to decremnt the cases of false positive.\n",
    "Each rule look like this:\n",
    "\n",
    "        TargetRule(\n",
    "            literal=\"coronavirus screening\",\n",
    "            category=\"IGNORE\",\n",
    "            pattern=[\n",
    "                {\"_\": {\"concept_tag\": \"COVID-19\"}},\n",
    "                {\"LOWER\": {\"IN\": [\"screen\", \"screening\", \"screenings\"]}},\n",
    "            ],\n",
    "        ),\n",
    "Since we replaced the spans we found with the corresponding label we didn't need the concept_tag attribute of the token/span.\n",
    "\n",
    "**Literal** : This specifies the literal text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the category or label associated with the identified entity.\n",
    "\n",
    "**Pattern** : This defines the pattern or conditions under which the entity should be recognized. It's a list of dictionaries specifying conditions for token matching. These rules some times used lemma attribute or POS of each token. A documentation can be found at : https://spacy.io/usage/rule-based-matching.\n",
    "\n",
    "Similar to the concept tag apporach we defined regex patterns, we have added these pattern in target_rules.csv file\n",
    "Each rule in the csv file is like this : regexPattern, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:COVID-19 positive (?:unit|floor)|positive COVID-19 (?:unit|floor|exposure)),COVID-19\n",
      "(?i)(?:known(?: positive)? COVID-19(?: positive)? (?:exposure|contact)),COVID-19\n",
      "(?i)(?:COVID-19 positive screening|positive COVID-19 screening|screening COVID-19 positive|screening positive COVID-19),positive coronavirus screening\n",
      "(?i)(?:diagnosis : COVID-19 (?:test|screening)),COVID-19\n",
      "(?i)(?:COVID-19 screening),coronavirus screening\n",
      "(?i)(?:active COVID-19 precaution|droplet isolation precaution|positive for (?:flu|influenza)|(?:the|a) positive case|results are confirm),1 2 3\n",
      "(?i)(?:exposed to positive|[ ] COVID-19|age like_num(?: )?\\+|(?:return|back) to work|COVID-19 infection rate),1 2 3\n",
      "(?i)(?:COVID-19 (?:restriction|emergency|epidemic|outbreak|crisis|breakout|pandemic|spread|screening)|droplet precaution),1 2\n",
      "(?i)(?:contact precautions|positive (?:flu|influenza)|positive (?:patient|person)|confirm (?:with|w/(?:/)?|w)|(?:the|positive) case),1 2\n",
      "(?i)(?:results confirm|(?:neg|pos)\\S+ pressure|positive (?:attitude|feedback|serology)|COVID-19 (guidelines|rate)),1 2\n",
      "(?i)(?:has the patient been diagnosed (?:with|w/(?:/)?|w)),1 2 3 4 5 6\n",
      "(?i)(?:has patient been diagnosed (?:with|w/(?:/)?|w)),1 2 3 4 5\n",
      "(?i)((?:person|patient) with confirm COVID-19),1 2 3 4\n",
      "(?i)(?:COVID-19 positive (?:tested )?other_experiencer),COVID-19\n",
      "(?i)(?:in order to decrease the spread of the COVID-19 infection),1 2 3 4 5 6 7 8 9 10\n",
      "(?i)(?:COVID-19 positive (?:patient|person|people|veteran)),OTHER_PERSON\n",
      "(?i)(?:positive COVID-19 (?:tested )?other_experiencer),COVID-19\n",
      "(?i)(?:(?:(?:contact|exposure) (?:with|to)? )?positive COVID-19 (?:patient|person|veteran)),OTHER_PERSON\n",
      "(?i)(?:(?:patient|person) (?:who|that) test (?:positive|confirm) for COVID-19),OTHER_PERSON\n",
      "(?i)(ref : not detected|history of present illness|does not know|but|therefore|flu|metapneumovirus|;),<IGNORE>\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('target_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_relation_from_csv(\"target_rules.csv\", relation_name=\"TargetTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%rgxlog\n",
    "TargetTagMatches(Label, Span, Path) <- FilesContent(Path, Content), TargetTagRules(Pattern, Label), py_rgx_span(Content, Pattern) -> (Span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                        The patient have reported positive COVID-19 exposure .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'TargetTagMatches(Label, Span, \"sample1.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample3.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample6.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " COVID-19 | [26, 52)\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_spans(\"TargetTagMatches\", \"FilesPaths\", magic_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After:\n",
    "As we can see in sample6.txt, the covid positive exposure has changed to covid, in order to not give false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/section_rules.py):\n",
    "Here, we'll add a section detection component that defines rules for detecting sections titles, which usually appear before a semicolon.\n",
    "Section rules are utilized to identify specific section names, enabling the separation of text into different parts. Entities occurring in certain sections are considered positive.\n",
    "\n",
    "In the original project, the SectionRule class was used to define rules for identifying specific section text. Each SectionRule has the following structure\n",
    "\n",
    "      SectionRule(category=\"problem_list\", literal=\"Active Problem List:\"),\n",
    "      SectionRule(category=\"problem_list\", literal=\"Current Problems:\"),\n",
    "    \n",
    "    \n",
    "**Literal** : This specifies the literal section text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the section category associated with the identified section.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Similar to the approach used in the concept tagger stage, regex patterns were derived from these literals, and these patterns are stored in the 'section_target_rules.csv' file and are used to match section texts and replace them with their appropriate category.\n",
    "\n",
    "Each rule in the CSV file follows this format: regexPattern, sectionLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:Lab results :),labs :\n",
      "(?i)(?:Addendum :),addendum :\n",
      "(?i)(?:(?:ALLERGIC REACTIONS|ALLERGIES) :),allergies :\n",
      "(?i)(?:(?:CC|Chief Complaint) :),chief_complaint :\n",
      "(?i)(?:COMMENTS :),comments :\n",
      "(?i)(?:(?:(?:ADMISSION )?DIAGNOSES|Diagnosis|Primary Diagnosis|Primary|Secondary(?: (?:Diagnoses|Diagnosis))) :),diagnoses :\n",
      "(?i)(?:(?:Brief Hospital Course|CONCISE SUMMARY OF HOSPITAL COURSE BY ISSUE/SYSTEM|HOSPITAL COURSE|SUMMARY OF HOSPITAL COURSE) :),hospital_course :\n",
      "(?i)(?:(?:Imaging|MRI|INTERPRETATION|Radiology) :),imaging :\n",
      "(?i)(?:(?:ADMISSION LABS|Discharge Labs|ECHO|Findings|INDICATION|Labs|Micro|Microbiology|Studies|Pertinent Results) :),labs_and_studies :\n",
      "(?i)(?:(?:ACTIVE MEDICATIONS(?: LIST)|ADMISSION MEDICATIONS|CURRENT MEDICATIONS|DISCHARGE MEDICATIONS|HOME MEDICATIONS|MEDICATIONS) :),medications :\n",
      "(?i)(?:(?:MEDICATIONS AT HOME|MEDICATIONS LIST|MEDICATIONS ON ADMISSION|MEDICATIONS ON DISCHARGE|MEDICATIONS ON TRANSFER|MEDICATIONS PRIOR TO ADMISSION) :),medications :\n",
      "(?i)(?:Neuro :),neurological :\n",
      "(?i)(?:(?:A/P|MEDICATIONS LIST|ASSESSMENT/PLAN|ASSESSMENT|Clinical Impression|DISCHARGE DIAGNOSES|DISCHARGE DIAGNOSIS) :),observation_and_plan :\n",
      "(?i)(?:(?:Discharge Condition|Discharge Disposition|FINAL DIAGNOSES|FINAL DIAGNOSIS|IMPRESSION|Impression and Plan|Impression and Recommendation) :),observation_and_plan :\n",
      "(?i)(?:(?:Facility|Service) :),other :\n",
      "(?i)(?:(?:Current Medical Problems|History of Chronic Illness|MHx|PAST HISTORY|PAST MEDICAL Hx|PAST SURGICAL HISTORY|PMH|PMHx|PAST MEDICAL HISTORY|UNDERLYING MEDICAL CONDITION) :),past_medical_history :\n",
      "(?i)(?:(?:Education|Patient Education|DISCHARGE INSTRUCTIONS/FOLLOWUP|DISCHARGE INSTRUCTIONS|Followup Instructions) :),patient_education :\n",
      "(?i)(?:(?:PE|PHYSICAL EXAM|PHYSICAL EXAMINATION) :),physical_exam :\n",
      "(?i)(?:(?:Active Problem List|Current Problems|Medical Problems|PROBLEM LIST) :),problem_list :\n",
      "(?i)(?:REASON FOR THIS EXAMINATION :),reason_for_examination :\n",
      "(?i)(?:(?:Electronic Signature|Signed electronically by) :),signature :\n",
      "(?i)(?:(?:PMHSx|PSH|SH|Sexual History:|Social History) :),social_history :\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('section_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_relation_from_csv(\"section_rules.csv\", relation_name=\"SectionRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionRulesMatches(Label, Span, Path)':\n",
      "     Label      |  Span   |    Path\n",
      "----------------+---------+-------------\n",
      " problem_list : | [0, 14) | sample3.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "SectionRulesMatches(Label, Span, Path) <- FilesContent(Path, Content), SectionRules(Pattern, Label), py_rgx_span(Content, Pattern) -> (Span)\n",
    "?SectionRulesMatches(Label, Span, Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample1.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample3.txt\")':\n",
      "     Label      |  Span\n",
      "----------------+---------\n",
      " problem_list : | [0, 14)\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample6.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_spans(\"SectionRulesMatches\", \"FilesPaths\", magic_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After:\n",
    "As we can see in sample 3, current problems has changed to problems_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Assertion:\n",
    "\n",
    " Next, we will explore how to assert attributes indicating whether a mention of COVID-19 is positive or not. In our project, we have created a table     named 'CovidAttributes' that contains all attributes for each COVID-19 mention. This table will be used for classifying documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionMatches(Path, Span, CovidAttribute)':\n",
      "    Path     |  Span   |  CovidAttribute\n",
      "-------------+---------+------------------\n",
      " sample3.txt | [0, 76) |     positive\n",
      "\n",
      "printing results for query 'CovidMatches(Path, Span)':\n",
      "    Path     |   Span\n",
      "-------------+-----------\n",
      " sample1.txt | [34, 42)\n",
      " sample1.txt | [85, 93)\n",
      " sample1.txt | [96, 104)\n",
      " sample2.txt | [26, 34)\n",
      " sample3.txt | [58, 66)\n",
      " sample4.txt |  [4, 12)\n",
      " sample5.txt |  [9, 17)\n",
      " sample6.txt | [26, 34)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "#Here, we employ a pattern to identify entities present in specific sections and mark them as positive,\n",
    "#and adding them to the 'CovidAttributes' table.\n",
    "\n",
    "pattern = \"(?i)(?:diagnoses :|observation_and_plan :|past_medical_history :|problem_list :)(?:(?!labs :|addendum :|allergies :|chief_complaint :|comments :|family_history :|hospital_course :|imaging :|labs_and_studies :|medications :|neurological :|other :|patient_education :|physical_exam :|reason_for_examination :|signature :|social_history :).)*\"\n",
    "new SectionRulesAttribute(str, str)\n",
    "SectionRulesAttribute(pattern, \"positive\")\n",
    "\n",
    "SectionMatches(Path, Span, CovidAttribute) <- FilesContent(Path, Content), SectionRulesAttribute(Pattern, CovidAttribute), py_rgx_span(Content, Pattern) -> (Span)\n",
    "?SectionMatches(Path, Span, CovidAttribute)\n",
    "\n",
    "CovidMatches(Path, Span) <- FilesContent(Path, Content), py_rgx_span(Content, \"COVID-19\") -> (Span)\n",
    "?CovidMatches(Path, Span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionCovidAttributes(Path, CovidSpan, CovidAttribute)':\n",
      "    Path     |  CovidSpan  |  CovidAttribute\n",
      "-------------+-------------+------------------\n",
      " sample3.txt |  [58, 66)   |     positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "SectionCovidAttributes(Path, CovidSpan, CovidAttribute) <- SectionMatches(Path, Span1, CovidAttribute), CovidMatches(Path, Span2), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?SectionCovidAttributes(Path, CovidSpan, CovidAttribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text into Sentences:\n",
    "\n",
    "In the subsequent stages, where attributes are assigned to COVID-19 mentions, a departure from the previous stages occurs. Here, patterns are no longer applied to the entire text, instead, they are applied at the sentence level, since the attributes of COVID-19 mentions are typically determined by the context of the sentence in which they appear. This means the text is processed and tokenized into sentences using spaCy's English language model. This process is accomplished through the use of  ie functions and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'Sents(Path, Sent)':\n",
      "    Path     |                                        Sent\n",
      "-------------+------------------------------------------------------------------------------------\n",
      " sample1.txt |                       COVID-19 results came back positive .\n",
      " sample1.txt |                 His family recently tested positive for COVID-19 .\n",
      " sample1.txt |                    patient presents to be tested for COVID-19 .\n",
      " sample2.txt |                               Results be positive .\n",
      " sample2.txt |                        The patient be tested for COVID-19 .\n",
      " sample3.txt |                                 COVID-19 like_num\n",
      " sample3.txt |                                associated_diagnosis\n",
      " sample3.txt |                                     like_num .\n",
      " sample3.txt |                             problem_list : like_num .\n",
      " sample4.txt |                              neg COVID-19 education .\n",
      " sample5.txt |                           positive COVID-19 precaution .\n",
      " sample6.txt |                        The patient have reported COVID-19 .\n",
      " sample7.txt | Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n",
      "printing results for query 'SentSpans(Path, Sent, SentSpan)':\n",
      "    Path     |                                        Sent                                        |  SentSpan\n",
      "-------------+------------------------------------------------------------------------------------+------------\n",
      " sample1.txt |                       COVID-19 results came back positive .                        | [96, 133)\n",
      " sample1.txt |                 His family recently tested positive for COVID-19 .                 |  [45, 95)\n",
      " sample1.txt |                    patient presents to be tested for COVID-19 .                    |  [0, 44)\n",
      " sample2.txt |                               Results be positive .                                |  [37, 58)\n",
      " sample2.txt |                        The patient be tested for COVID-19 .                        |  [0, 36)\n",
      " sample3.txt |                                 COVID-19 like_num                                  |  [58, 75)\n",
      " sample3.txt |                                associated_diagnosis                                |  [26, 46)\n",
      " sample3.txt |                                     like_num .                                     |  [15, 25)\n",
      " sample3.txt |                                     like_num .                                     |  [47, 57)\n",
      " sample3.txt |                             problem_list : like_num .                              |  [0, 25)\n",
      " sample4.txt |                              neg COVID-19 education .                              |  [0, 24)\n",
      " sample5.txt |                           positive COVID-19 precaution .                           |  [0, 30)\n",
      " sample6.txt |                        The patient have reported COVID-19 .                        |  [0, 36)\n",
      " sample7.txt | Elevated cholesterol levels require further assessment and lifestyle adjustments . |  [0, 82)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "#Sentences of the text\n",
    "Sents(Path, Sent) <- FilesPaths(Path), sent_tokenization(Path) -> (Sent)\n",
    "?Sents(Path, Sent)\n",
    "\n",
    "#SentSpan is the span of the sentence in the text\n",
    "SentSpans(Path, Sent, SentSpan) <- FilesContent(Path, Content), Sents(Path, Sent), py_rgx_span(Content, Sent) -> (SentSpan)\n",
    "?SentSpans(Path, Sent, SentSpan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |  CovidAttribute  |       Sent\n",
      "-------------+-------------+------------------+-------------------\n",
      " sample3.txt |   [0, 8)    |     positive     | COVID-19 like_num\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- SectionCovidAttributes(Path, AbsCovidSpan, CovidAttribute),\\\n",
    "SentSpans(Path, Sent, SentSpan) ,get_relative_span(AbsCovidSpan, SentSpan) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Context Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/context_rules.py):\n",
    "These rules assign an attribute for each COVID-19 label based on the context, these attributes will be used later to classify each text.\n",
    "\n",
    "Example for this rule is: \n",
    "\n",
    "    ConTextRule(\n",
    "        literal=\"Not Detected\",\n",
    "        category=\"NEGATED_EXISTENCE\",\n",
    "        direction=\"BACKWARD\",\n",
    "        pattern=[\n",
    "            {\"LOWER\": {\"IN\": [\"not\", \"non\"]}},\n",
    "            {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "            {\"TEXT\": \"-\", \"OP\": \"?\"},\n",
    "            {\"LOWER\": {\"REGEX\": \"detecte?d\"}},\n",
    "        ],\n",
    "        allowed_types={\"COVID-19\"},\n",
    "    ),\n",
    "   **direction** specify if the allowed_types should be before or after the pattern,\n",
    "   **allowed_types** specify on what labels should this rule be applied on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:positive COVID-19|COVID-19 (?:\\([^)]*\\)) (?:positive|detected)|COVID-19(?: positive)? associated_diagnosis)#positive\n",
      "(?i)(?:COVID-19 status : positive)#positive\n",
      "(?i)(?:associated_diagnosis COVID-19|associated_diagnosis (?:with|w|w//|from) (?:associated_diagnosis )?COVID-19)#positive\n",
      "(?i)(?:COVID-19 positive(?: patient| precaution)?|associated_diagnosis (?:due|secondary) to COVID-19)#positive\n",
      "(?i)(?:(?:current|recent) COVID-19 diagnosis)#positive\n",
      "(?i)(?:COVID-19 (?:- )?related (?:admission|associated_diagnosis)|admitted (?:due to|(?:with|w|w/)) COVID-19)#positive\n",
      "(?i)(?:COVID-19 infection|b34(?:\\.)?2|b97.29|u07.1)#positive\n",
      "(?i)(?:COVID-19 eval(?:uation)?|(?:positive )? COVID-19 symptoms|rule out COVID-19)#uncertain\n",
      "(?i)(?:patient (?:do )?have COVID-19)#positive\n",
      "(?i)(?:diagnosis : COVID-19(?: (?:test|screen)(?:ing|ed|s)? positive)?(?: positive)?)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:not|non) (?:- )?detecte?d)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} negative screening|negative screening(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} : negative)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:not be|none) detected)#negated\n",
      "(?i)(?:free from(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? not (?:be )?tested)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} not indicated)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? NEGATIVE NEG)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} negative test)#negated\n",
      "(?i)(?:negative test(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:without any(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:denie(?:s|d)(?: any| travel)?(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#negated\n",
      "(?i)(?:no (?:evidence(?: of)?|(?:hx|-hx|history) of|diagnosis (?:of)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:no(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:no (?:positive|one|residents|confirm case|contact(?: w/?(?:ith)?$))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? no confirm case)#negated\n",
      "(?i)(?:(?:no|n't) (?:be )? confirm(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:(?:no known|not have)(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:answer(?:ed|s|ing)? (?:no|negative|neg)|negative))#negated\n",
      "(?i)(?:(?:answer(?:ed|s|ing)? (?:no|negative|neg)|(?:neg|negative)(?: for)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:not positive(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? not positive)#negated\n",
      "(?i)(?:excluded(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} excluded)#negated\n",
      "(?i)(?:no risk factor for(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#uncertain\n",
      "(?i)(?:negative screening(?: for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? screening (?:negative|neg))#negated\n",
      "(?i)(?:(?:screening (?:negative|neg) for|do (?:not|n't) have (?:any )?(?:signs|symptoms|ss|s/s))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? do not screening positive)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:be negative|not test positive))#negated\n",
      "(?i)(?:(?:be negative|not test positive|not? screening(?: for)|no signs of|no (?:sign|symptom|indication(?:of|for)?)|not? test(?:\\S+)? for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:no exposure|(?:without|w/o) (?:signs|symptoms)(?:or (?:signs|symptoms))|do)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} not have)#negated\n",
      "(?i)(?:(?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria|no concern (?:for|of)|not? (?:at )risk)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria))#negated\n",
      "(?i)(?:(?:no suspicion(?: for)|not suspect|ruled out for|no(?: recent) travel|not be in|clear(?:ed|s|ing) (?:of|for|from))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:not(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:be ruled out|be not likely|not have contact with))#negated\n",
      "(?i)(?:(?:no (?:hx|history) (?:of )travel|not have contact with|no symptoms of|no risk factors|no (?:confirm case|report))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:no (?:exposure|contact) (?:to|with)|not test(?:\\S+)? positive))#negated\n",
      "(?i)(?:(?:no (?:exposure|contact) (?:to|with)|do (?:not|n't) meet(?: screening)(?: criteria)(?: for)|not test(?:\\S+)? positive(?: for)|not tested(?: or diagnosis))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:no|any)(?: known) contact(?: with)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} : no)#negated\n",
      "(?i)(?:(?:(?:not|never) diagnosis with|not been tested (?:for )?or diagnosis with)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} confirm)#positive\n",
      "(?i)(?:(?:confirm|known)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#positive\n",
      "(?i)(?:(?:(?:test(?:\\S+)?)?positive(?: for)?|notif(?:y|ied) of positive (?:results?|test(?:\\S+)?|status))(?: (?!<IGNORE>)\\S+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positiv(?:e|ity)|test(?:\\S+)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positiv(?:e|ity)|test(?:\\S+)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} (?:positive status|results be positive))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} results positive)#positive\n",
      "(?i)(?:results positive(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#positive\n",
      "(?i)(?:notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status)(?: (?!<IGNORE>)\\S+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status))#positive\n",
      "(?i)(?:likely secondary to(?: (?!<IGNORE>)\\S+){0,0} COVID-19)#positive\n",
      "(?i)(?:(?:problem(?: list)? (?:of|:)|(?:active|current|acute) problems :|admi(?:t|ssion) diagnosis(?: :)?)(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#positive\n",
      "(?i)(?:(?:reason for admission :|treatment of|(?:admitting )diagnosis(?: :)?)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} diagnosis like_num)#positive\n",
      "(?i)(?:(?:Reason for admission :|inpatient with|discharged from|in m?icu (?:for|with))(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#admission\n",
      "(?i)(?:(?:admit(?:ted|s|ting) (?:like_num|with|for)|admitted (?:to|on)|Reason for ICU :|admission for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#admission\n",
      "(?i)(?:Reason for ED visit or Hospital Admission :(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#admission\n",
      "(?i)(?:(?:(?:in|to) (?:the )(?:hospital|icu|micu) (?:for|due to)|hospitali(?:zed)?(?: timesx)? (?:for|due to))(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#admission\n",
      "(?i)(?:(?:diagnosis with|found to be positive for)(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,5} found to be positive)#positive\n",
      "(?i)(?:(?:positive test|presum(?:e|ed|es|ing) positive|not(?: yet)? recover(?:s|ing|ed)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positive test|presum(?:e|ed|es|ing) positive))#positive\n",
      "(?i)(?:(?:management of|ards(?: (?:from|with|secondary to))?|acute respiratory distress|post - extubation)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#positive\n",
      "(?i)(?:(?:in(?: the)? setting of|in the s / o|found to have|present(?:s|ed|ing)? with)(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#positive\n",
      "(?i)(?:resp(?:iratory) failure(?:(?: (?:with|due to))?|like_num|\\( like_num \\))(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#positive\n",
      "(?i)(?:(?:active(?: for)|recovering from)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} recovering from)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} (?:detected|value : detected|POSITIVEH))#positive\n",
      "(?i)(?:(?:\\d+(?: )?-|like_num )year(?:(?: )?-(?: )?old| old) (?:(?:aa|white|black|hispanic|caucasian) )?(?:\\b(?!family\\b|other_experiencer\\b)\\S+\\b )?(?:with|w|w/|admitted)(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#patient_experiencer\n",
      "(?i)(?:(?:like_num (?:y[or]|y / o)|[\\d]+yo) (?:\\b(?!family\\b|other_experiencer\\b)\\S+\\b )?(?:patient |veteran )?(?:with|w|w/)(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#patient_experiencer\n",
      "(?i)(?:the (?:veteran|vet|patient) have(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#patient_experiencer\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} precaution)#future\n",
      "(?i)(?:(?:(?:precaution|protection|protect) (?:for|against)|concern about|reports of|vaccine|protect yourself|prevent(?:ed|ion|s|ing)|avoid)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:prevent(?:ed|ion|s|ing)|vaccine|educat\\S*|instruction))#future\n",
      "(?i)(?:(?:questions (?:about|regarding|re|concerning|on|for)|(?:anxiety|ask(?:ing|ed|es|ed)?) about|educat(?:ion|ed|ing|ed)?|instruction)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:(?:information(?: )?(?:on|about|regarding|re)?|protocols?)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} protocols?)#future\n",
      "(?i)(?:(?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?|(?:anxious|worr(?:ied|ies|y|ying)) (?:about|re|regarding))(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?))#future\n",
      "(?i)(?:if(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#future\n",
      "(?i)(?:(?:advisor(?:y|ies)|travel screen(?: :)?|Travel History Questionnaire|prescreen|front gate)(?: (?!<IGNORE>)\\S+)*? COVID-19)#screening\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} (?:questionnaire :|questionn?aire|question\\S*|prescreen|front gate))#screening\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,9} screen\\S*)#screening\n",
      "(?i)(?:screen\\S*(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#screening\n",
      "(?i)(?:have you(?: (?!<IGNORE>)\\S+)*? COVID-19)#not relevant\n",
      "(?i)(?:(?:mers)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:This patient was screened for the following suspected travel related illness(?:es)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? This patient was screened for the following suspected travel related illness(?:es)?)#future\n",
      "(?i)(?:(?:will(?: be) travel|travel plans|if you need|plan to travel)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:(?:(?:limit|reduce|lower|minimize)(?: the)? (?:risk|chance|possibility) of|if you)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:(?:(?:(?:-)?hx|history|) of)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:(?:^(?:check|test|retest|eval)(?: for)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#test\n",
      "(?i)(?:(?:work(?:-|\\s)up)(?: (?!<IGNORE>)\\S+)*? COVID-19)#test\n",
      "(?i)(?:(?:evaluation)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#test\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} (?:evaluation))#test\n",
      "(?i)(?:(?:swab|PCR|specimen sent)(?: (?!<IGNORE>)\\S+)*? COVID-19)#test\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:swab|PCR|specimen sent))#test\n",
      "(?i)(?:(?:awaiting results|at risk for|risk for|currently being ruled out or has tested positive for|to exclude)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:awaiting results|currently being ruled out or has tested positive for|(?:patient|person) of interest))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,0} (?:risk))#uncertain\n",
      "(?i)(?:(?:investigation of)(?: (?!<IGNORE>)\\S+){0,0} COVID-19)#uncertain\n",
      "(?i)(?:(?:question of|differential diagnosis :|ddx :)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:awaiting|questionnaire|r(?:/)?o(?:\\.)?)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} (?:awaiting|questionnaire|r(?:/)?o(?:\\.)?))#uncertain\n",
      "(?i)(?:(?:under investigation|(?:may|might) be positive(?: for)?|flew|tarvel(?:ed)?|travelled)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:under investigation|(?:may|might) be positive))#uncertain\n",
      "(?i)(?:(?:facility (?:with|has)(?: a)?|known to have|(?:same )?room|patients with)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:area|county|community|city) (?:with|of)|in the building|(?:several|multiple|one)(?:of )?(?:the )? other_experiencer)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:in the building))#negated\n",
      "(?i)(?:(?:(?:he|she) thinks (?:he|she) (?:have|had|has)|\\S+ would like)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|unlikely to be)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|possible positive))#uncertain\n",
      "(?i)(?:(?:(?:possible|potential)? exposure|possibly|possible positive)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:(?:risk of|likely|probable|probably)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:suspicion(?: for)?|^suspect|differential diagnosis|ddx(?: :)?|doubt)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:suspicion|^suspect|differential diagnosis|ddx(?: :)?|may have been exposed))#uncertain\n",
      "(?i)(?:(?:(?:positive )?(?:sign|symptom) of)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:sx|s/s|rule (?:- )out|be ruled out(?: for)?|^(?:vs\\.?|versus)$)(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} (?:sx|s/s|rule (?:- )out|^(?:vs\\.?|versus)$))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:(?:possible|potential)? exposure|may have been exposed))#uncertain\n",
      "(?i)(?:(?:concern(:?s)?(?: for| of)?|if (?:negative|positive)|c/f|assess(?:ed)? for|concerning for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:(?:unlikely(?: to be positive)?|low (?:suspicion|probability|risk (?:for|in|of)))(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:unlikely(?: to be positive)?|low (?:suspicion|probability)|is unlikely))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} (?:extremely low))#uncertain\n",
      "(?i)(?:(?:low risk of)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#uncertain\n",
      "(?i)(?:(?:(?:other_experiencer|family) ^test positive(?: for)?|any one|contact with(?: known))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:other_experiencer|family)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:other_experiencer|any one|contact with(?: known)))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,0} (?:(?:a|an|another) \\S+ tested positive))#negated\n",
      "(?i)(?:(?:had contact|same (?:building|floor)|care for|clean)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:had contact|same (?:building|floor)|care for|clean))#negated\n",
      "(?i)(?:(?:concern(?:ed)? about)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#negated\n",
      "(?i)(?:(?:patient concern (?:for|of)|desire|(?:concerned|prepare) (?:for|about))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:seen in|a (?:positive|confirmed) case of|cases|epidemic|pandemic)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} (?:cases|epidemic|pandemic|national emergency|crisis|situation|mandate|\\?))#negated\n",
      "(?i)(?:(?:national emergency|crisis|situation|mandate)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:(?:seen in(?: the)? setting of)(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#negated\n",
      "(?i)(?:(?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off|goals :)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:in the (?:area|community)|outbreak))#negated\n",
      "(?i)(?:(?:in the (?:area|community)|outbreak)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation))#negated\n",
      "(?i)(?:(?:^read about|deploy|(?:come|been) in close contact(?: with)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:^read about|deploy|(?:come|been) in close contact(?: with)?|error))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:have you had close contact|web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases))#negated\n",
      "(?i)(?:(?:have you had close contact|the group|session|(?:nurse(?:s)?|rn) notes)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases|error)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:person|patients) with(?: confirmed)?(?: or)?(?: suspected)?|cases of)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#negated\n",
      "(?i)(?:elective(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} elective)#negated\n",
      "(?i)(?:(?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?)))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:(?:someone|person) who (?:has|have) tested positive|contact with))#negated\n",
      "(?i)(?:(?:(?:someone|person) who (?:has|have) tested positive|contact with)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,0} (?:\\(resolved\\)))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:social worker|initially negative|likely recovered|not aware|positive (?:case|symptom|sign)|client history|emergency contact|several positive|special instructions :))#IGNORE\n",
      "(?i)(?:(?:social worker|initially negative|likely recovered|not aware|positive (?:case|symptom|sign)|client history|emergency contact|several positive|special instructions :)(?: (?!<IGNORE>)\\S+)*? COVID-19)#IGNORE\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('context_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_relation_from_csv(\"context_rules.csv\", relation_name=\"ContextRules\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'ContextMatches(CovidAttribute, Span, Path, Sent)':\n",
      "   CovidAttribute    |   Span   |    Path     |                        Sent\n",
      "---------------------+----------+-------------+----------------------------------------------------\n",
      "      positive       | [0, 17)  | sample5.txt |           positive COVID-19 precaution .\n",
      "       negated       | [0, 12)  | sample4.txt |              neg COVID-19 education .\n",
      "      positive       | [27, 48) | sample1.txt | His family recently tested positive for COVID-19 .\n",
      "      positive       | [0, 35)  | sample1.txt |       COVID-19 results came back positive .\n",
      " patient_experiencer | [0, 34)  | sample6.txt |        The patient have reported COVID-19 .\n",
      "       future        | [9, 28)  | sample5.txt |           positive COVID-19 precaution .\n",
      "       future        | [4, 22)  | sample4.txt |              neg COVID-19 education .\n",
      "       negated       | [4, 48)  | sample1.txt | His family recently tested positive for COVID-19 .\n",
      "\n",
      "printing results for query 'CovidSpans(Path, Span, Sent)':\n",
      "    Path     |   Span   |                        Sent\n",
      "-------------+----------+----------------------------------------------------\n",
      " sample1.txt |  [0, 8)  |       COVID-19 results came back positive .\n",
      " sample1.txt | [40, 48) | His family recently tested positive for COVID-19 .\n",
      " sample1.txt | [34, 42) |    patient presents to be tested for COVID-19 .\n",
      " sample2.txt | [26, 34) |        The patient be tested for COVID-19 .\n",
      " sample3.txt |  [0, 8)  |                 COVID-19 like_num\n",
      " sample4.txt | [4, 12)  |              neg COVID-19 education .\n",
      " sample5.txt | [9, 17)  |           positive COVID-19 precaution .\n",
      " sample6.txt | [26, 34) |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "#covid_attributes: negated, other_experiencer, is_future, not_relevant, uncertain, positive\n",
    "ContextMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), ContextRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "?ContextMatches(CovidAttribute, Span, Path, Sent)\n",
    "\n",
    "CovidSpans(Path, Span, Sent) <- Sents(Path, Sent), py_rgx_span(Sent, \"COVID-19\") -> (Span)\n",
    "?CovidSpans(Path, Span, Sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- ContextMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Postprocessor](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/postprocess_rules.py):\n",
    "The postprocessor is designed to apply extra adjustments to the processed text using custom logic or specific requirements not addressed by the spaCy pipeline. These rules modify, remove, or change attributes for each mention of COVID-19 based on either their existing attributes, the context of the sentences in which they appear, or a combination of both. This flexibility allows us to address data issues and implement targeted improvements. For instance, it proves useful in identifying and rectifying incorrectly labeled positive cases, thereby enhancing the accuracy of our classification.\n",
    "\n",
    "**How we implemented it:**  \n",
    "As mentioned earlier, postprocess rules are responsible for modifying, removing, or changing attributes for each mention of COVID-19. In the original project, these attributes are represented as boolean variables stored in an object class for each COVID-19 mention. The rules simply switch the corresponding boolean variable to assign or remove the attribute. However, in rgxlog, we don't have the luxury of creating classes. In our project, when we want to remove a specific attribute, we introduce an additional attribute that acts as its negation. For instance, for the attribute 'positive,' we add 'no_positive,' causing the document classifier to behave as if there is no positive attribute.\n",
    "\n",
    "Additionally, in some cases, the entire COVID-19 mention is removed by eliminating its object. In our project, we introduce an 'IGNORE' attribute, which results in the exclusion of the mention from consideration in the document classifier stage.\n",
    "<br>\n",
    "\n",
    "**In the subsequent cells, we will explore three types of postprocess rules:**\n",
    "1) Rules based on patterns\n",
    "2) Rules utilizing existing attributes and patterns\n",
    "3) Rules applied to the next sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Postprocess rules based on patterns:\n",
    "\n",
    "Example rule in the original project:\n",
    "\n",
    "```\n",
    "PostprocessingRule(\n",
    "        patterns=[\n",
    "            PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.sentence_contains,\n",
    "                condition_args=({\"deny\", \"denies\", \"denied\"},),\n",
    "            ),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.sentence_contains,\n",
    "                condition_args=({\"contact\", \"contacts\", \"confirmed\"},),\n",
    "            ),\n",
    "        \\],\n",
    "        action=postprocessing_functions.remove_ent,\n",
    "        description=\"Remove a coronavirus entity if 'denies' and 'contact' are in. This will help get rid of false positives from screening.\",\n",
    "    ),    \n",
    "```\n",
    "This rule iterates through each entity and checks a series of conditions which are the \"PostprocessingPattern\". If all conditions evaluate as True, then some action is taken on the entity, which is 'remove' action in this example.\n",
    "\n",
    "\n",
    "In our case, we assign \"IGNORE\" attribute to the COVID-19 mention causing it to be excluded from consideration during the document classification process.\n",
    "\n",
    "Each rule in the CSV file follows this format: regexPattern, Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*education.*#IGNORE\n",
      ".* \\?#IGNORE\n",
      "(?=.*\\b(?:deny|denies|denied)\\b)(?=.*\\b(?:contact|confirm)\\b).*#IGNORE\n",
      "(?=.*\\b(?:setting of|s/o)\\b)(?!.*\\b(?:COVID-19 infection|COVID-19 ards)\\b).*#no_positive\n",
      "(?i)(.*benign.*)#uncertain\n",
      "admitted to COVID-19 unit#positive\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('postprocess_pattern_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_relation_from_csv(\"postprocess_pattern_rules.csv\", relation_name=\"PostprocessRules\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PostprocessMatches(CovidAttribute, Span, Path, Sent)':\n",
      "  CovidAttribute  |  Span   |    Path     |           Sent\n",
      "------------------+---------+-------------+--------------------------\n",
      "      IGNORE      | [0, 24) | sample4.txt | neg COVID-19 education .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "PostprocessMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostprocessRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "?PostprocessMatches(CovidAttribute, Span, Path, Sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- PostprocessMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Postprocess rules utilizing existing attributes and patterns:\n",
    "```\n",
    "PostprocessingRule(\n",
    "        patterns=[\n",
    "        \n",
    "            PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.is_modified_by_category,\n",
    "                condition_args=(\"DEFINITE_POSITIVE_EXISTENCE\",),\n",
    "            ),\n",
    "            # PostprocessingPattern(postprocessing_functions.is_modified_by_category, condition_args=(\"TEST\",)),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.sentence_contains,\n",
    "                condition_args=(\n",
    "                    {\n",
    "                        \"should\",\n",
    "                        \"unless\",\n",
    "                        \"either\",\n",
    "                        \"if comes back\",\n",
    "                        \"if returns\",\n",
    "                        \"if s?he tests positive\",\n",
    "                    },\n",
    "                    True,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        action=set_is_uncertain,\n",
    "        action_args=(True,),\n",
    "        description=\"Subjunctive of test returning positive. 'Will contact patient should his covid-19 test return positive.'\",\n",
    "    ),\n",
    "```\n",
    "This rule examines whether a COVID-19 mention possesses a positive attribute and if the sentence containing it includes any of the words specified in 'condition_args' If these conditions are met, the uncertain attribute is set to true.\n",
    "\n",
    "\n",
    "In our case, we check for each COVID-19 mention in the 'CovidAttributes' table if it's labeled as 'positive', also, we check if any of the specified words in 'condition_args' are present in the same sentence using a regex search. If the conditions are met, then we simply assign it an 'uncertain' attribute.\n",
    "\n",
    "Each rule in the CSV file follows this format: regexPattern, ExistingAttribute, NewAttribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*pending.*#negated#no_negated\n",
      ".*(?:should|unless|either|if comes back|if returns|if s?he tests positive).*#positive#uncertain\n",
      ".*precaution.*#positive#no_future\n",
      ".*(?:re[ -]?test|second test|repeat).*#negated#no_negated\n",
      ".*(?:sign|symptom|s/s).*#positive#uncertain\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('postprocess_attributes_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_relation_from_csv(\"postprocess_attributes_rules.csv\", relation_name=\"PostprocessRulesWithAttributes\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)':\n",
      "  CovidAttribute  |  NewAttribute  |  Span   |    Path     |              Sent\n",
      "------------------+----------------+---------+-------------+--------------------------------\n",
      "     positive     |   no_future    | [0, 30) | sample5.txt | positive COVID-19 precaution .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostprocessRulesWithAttributes(Pattern, CovidAttribute, NewAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "?PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, NewAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |    NewAttribute     |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      no_future      |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "CovidAttributes(Path, CovidSpan, NewAttribute, Sent) <- CovidAttributes(Path, CovidSpan, CovidAttribute, Sent), PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)\n",
    "?CovidAttributes(Path, CovidSpan, NewAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Postprocess rules applied to the next sentence:\n",
    "There's a rule that checks if the following sentence contains positive mentions. If it does, the COVID-19 mentions in the current sentence are also\n",
    "marked as positive. To Implement this rule in our project, we defined a new relation that pairs each sentence with its subsequent sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_sent(text_path):\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = list(doc.sents)\n",
    "    for i in range(len(sentences) - 1):  # Iterate until the second-to-last sentence\n",
    "        yield(sentences[i].text, sentences[i + 1].text)\n",
    "\n",
    "magic_session.register(ie_function=next_sent, ie_function_name = \"next_sent\", in_rel=[DataTypes.string], out_rel=[DataTypes.string,DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: patient presents to be tested for COVID-19 . next sentence: His family recently tested positive for COVID-19 .\n",
      "sentence: His family recently tested positive for COVID-19 . next sentence: COVID-19 results came back positive .\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "for first_sent, second_sent in next_sent(\"sample1.txt\"):\n",
    "    print(f\"sentence: {first_sent}\", f\"next sentence: {second_sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'NextSent(Path, Sent1, Sent2)':\n",
      "    Path     |                       Sent1                        |                       Sent2\n",
      "-------------+----------------------------------------------------+----------------------------------------------------\n",
      " sample1.txt | His family recently tested positive for COVID-19 . |       COVID-19 results came back positive .\n",
      " sample1.txt |    patient presents to be tested for COVID-19 .    | His family recently tested positive for COVID-19 .\n",
      " sample2.txt |        The patient be tested for COVID-19 .        |               Results be positive .\n",
      " sample3.txt |                associated_diagnosis                |                     like_num .\n",
      " sample3.txt |                     like_num .                     |                 COVID-19 like_num\n",
      " sample3.txt |             problem_list : like_num .              |                associated_diagnosis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "NextSent(Path, Sent1, Sent2) <- FilesPaths(Path), next_sent(Path) -> (Sent1, Sent2)\n",
    "?NextSent(Path, Sent1, Sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample2.txt |  [26, 34)   |      positive       |        The patient be tested for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      no_future      |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "new PostProcessWithNextSentenceRules(str, str)\n",
    "PostProcessWithNextSentenceRules(\"(?i)(?:^(?:positive|detected)|results?(?: be)? positive)\", \"positive\")\n",
    "PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostProcessWithNextSentenceRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent1) <- CovidSpans(Path, CovidSpan, Sent1), NextSent(Path, Sent1, Sent2), PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent2)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Document Classifier](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/document_classifier.py):\n",
    "Now we have the basic pieces in place to make our document classification. Each document is classified as either 'POS', 'UNK', or 'NEG' determined by the attributes of its COVID-19 mentions. The Results are stored in a DataFrame.\n",
    "\n",
    "Document Classifier stage has 2 parts:\n",
    " 1) **Attribute filtering**: Our pipeline assigns various attributes to each COVID-19 mention. However, during this stage, each COVID-19 case is refined to possess only one attribute. This filtering process operates based on specific conditions outlined in the 'attribute_filter' function.\n",
    " 2) **Document classification**: Documents are classified based on distinct conditions, as detailed in the 'classify_doc_helper' function. This step ensures the accurate categorization of each document according to the specified criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_filter(group):\n",
    "    \"\"\"\n",
    "    Filters attributes within each \"CovidSpan\" of a DataFrame table based on specific conditions.\n",
    "\n",
    "    Parameters:\n",
    "        group (pandas.Series): A pandas Series representing attributes for each \"CovidSpan\" within a DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        str: Filtered \"CovidSpan\" attribute determined by the following rules:\n",
    "            - If 'IGNORE' is present, returns 'IGNORE'.\n",
    "            - If 'negated' is present (and 'no_negated' is not present), returns 'negated'.\n",
    "            - If 'future' is present (and 'no_future' is not present), returns 'negated'.\n",
    "            - If 'other experiencer' or 'not relevant' is present, returns 'negated'.\n",
    "            - If 'positive' is present (and 'uncertain' and 'no_positive' are not present), returns 'positive'.\n",
    "            - Otherwise, returns 'uncertain'.\n",
    "    \"\"\"\n",
    "    if 'IGNORE' in group.values:\n",
    "        return 'IGNORE'\n",
    "    elif 'negated' in group.values and not 'no_negated' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'future' in group.values and not 'no_future' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'other experiencer' in group.values or 'not relevant' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'uncertain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "          Path Attribute\n",
      "0  sample1.txt    IGNORE\n",
      "1  sample1.txt   negated\n",
      "2  sample1.txt  positive\n",
      "3  sample2.txt  positive\n",
      "\n",
      "After:\n",
      "          Path Attribute\n",
      "0  sample1.txt    IGNORE\n",
      "1  sample2.txt  positive\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "data = {'Path': [\"sample1.txt\", \"sample1.txt\", \"sample1.txt\", \"sample2.txt\"],\n",
    "        'Attribute': ['IGNORE', 'negated', 'positive', 'positive']}\n",
    "df_example = pd.DataFrame(data)\n",
    "print(\"Before:\")\n",
    "print(df_example)\n",
    "\n",
    "df_example['Attribute'] = df_example.groupby(['Path'])['Attribute'].transform(attribute_filter)\n",
    "df_example = df_example.drop_duplicates().reset_index(drop=True)\n",
    "print(\"\\nAfter:\")\n",
    "print(df_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>CovidSpan</th>\n",
       "      <th>CovidAttribute</th>\n",
       "      <th>Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>[0, 8)</td>\n",
       "      <td>positive</td>\n",
       "      <td>COVID-19 results came back positive .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>[40, 48)</td>\n",
       "      <td>negated</td>\n",
       "      <td>His family recently tested positive for COVID-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>[26, 34)</td>\n",
       "      <td>positive</td>\n",
       "      <td>The patient be tested for COVID-19 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample3.txt</td>\n",
       "      <td>[0, 8)</td>\n",
       "      <td>positive</td>\n",
       "      <td>COVID-19 like_num</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample4.txt</td>\n",
       "      <td>[4, 12)</td>\n",
       "      <td>IGNORE</td>\n",
       "      <td>neg COVID-19 education .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sample5.txt</td>\n",
       "      <td>[9, 17)</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive COVID-19 precaution .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sample6.txt</td>\n",
       "      <td>[26, 34)</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>The patient have reported COVID-19 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path CovidSpan CovidAttribute   \n",
       "0  sample1.txt    [0, 8)       positive  \\\n",
       "1  sample1.txt  [40, 48)        negated   \n",
       "2  sample2.txt  [26, 34)       positive   \n",
       "3  sample3.txt    [0, 8)       positive   \n",
       "4  sample4.txt   [4, 12)         IGNORE   \n",
       "5  sample5.txt   [9, 17)       positive   \n",
       "6  sample6.txt  [26, 34)      uncertain   \n",
       "\n",
       "                                                Sent  \n",
       "0              COVID-19 results came back positive .  \n",
       "1  His family recently tested positive for COVID-...  \n",
       "2               The patient be tested for COVID-19 .  \n",
       "3                                  COVID-19 like_num  \n",
       "4                           neg COVID-19 education .  \n",
       "5                     positive COVID-19 precaution .  \n",
       "6               The patient have reported COVID-19 .  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (magic_session.run_commands(\"?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\", print_results=False, format_results=True))[0]\n",
    "if len(df) == 0:\n",
    "    df = DataFrame(columns=[\"Path\",\"CovidSpan\",\"CovidAttribute\"])\n",
    "df['CovidAttribute'] = df.groupby(['CovidSpan', 'Sent'])['CovidAttribute'].transform(attribute_filter)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_doc_helper(group):\n",
    "    \"\"\"\n",
    "Classifies a document as 'POS', 'UNK', or 'NEG' based on COVID-19 attributes.\n",
    "\n",
    "Parameters:\n",
    "    group (pandas.Series): A pandas Series representing COVID-19 attributes for each document within a DataFrame.\n",
    "    \n",
    "Returns:\n",
    "    str: Document classification determined as follows:\n",
    "         - 'POS': If at least one COVID-19 attribute with \"positive\" is present in the group.\n",
    "         - 'UNK': If at least one COVID-19 attribute with \"uncertain\" is present in the group and no \"positive\" attributes,\n",
    "                  or there's at least one COVID-19 attribute with 'IGNORE' and no other COVID-19 attributes exist.\n",
    "         - 'NEG': Otherwise.\n",
    "\"\"\"\n",
    "    if 'positive' in group.values:\n",
    "        return 'POS'\n",
    "    elif 'uncertain' in group.values:\n",
    "        return 'UNK'\n",
    "    elif 'negated' in group.values:\n",
    "        return 'NEG'\n",
    "    else:\n",
    "        return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "          Path  Attribute\n",
      "0  sample1.txt  uncertain\n",
      "1  sample1.txt    negated\n",
      "2  sample1.txt   positive\n",
      "3  sample2.txt   positive\n",
      "\n",
      "After:\n",
      "          Path DocResult\n",
      "0  sample1.txt       POS\n",
      "1  sample2.txt       POS\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "data = {'Path': [\"sample1.txt\", \"sample1.txt\", \"sample1.txt\", \"sample2.txt\"],\n",
    "        'Attribute': ['uncertain', 'negated', 'positive', 'positive']}\n",
    "df_example = pd.DataFrame(data)\n",
    "print(\"Before:\")\n",
    "print(df_example)\n",
    "\n",
    "df_example['DocResult'] = df_example.groupby(['Path'])['Attribute'].transform(classify_doc_helper)\n",
    "df_example = df_example[['Path', 'DocResult']]\n",
    "df_example = df_example.drop_duplicates().reset_index(drop=True)\n",
    "print(\"\\nAfter:\")\n",
    "print(df_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>DocResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample3.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample4.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample5.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sample6.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path DocResult\n",
       "0  sample1.txt       POS\n",
       "1  sample2.txt       POS\n",
       "2  sample3.txt       POS\n",
       "3  sample4.txt       UNK\n",
       "4  sample5.txt       POS\n",
       "5  sample6.txt       UNK"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DocResult'] = df.groupby('Path')['CovidAttribute'].transform(classify_doc_helper)\n",
    "df = df[['Path', 'DocResult']]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling unmentioned paths:\n",
    "At this step, we assign a classification result 'UNK' to paths not identified in the previous DataFrame result. This occurs when our pipeline doesn't detect any mention of COVID-19 or its synonyms in the text of those paths. As a result, these paths are excluded from all types of relations, consistent with our primary focus on COVID-19 entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>DocResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample3.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample4.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample5.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sample6.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sample7.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path DocResult\n",
       "0  sample1.txt       POS\n",
       "1  sample2.txt       POS\n",
       "2  sample3.txt       POS\n",
       "3  sample4.txt       UNK\n",
       "4  sample5.txt       POS\n",
       "5  sample6.txt       UNK\n",
       "6  sample7.txt       UNK"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = (magic_session.run_commands(\"?FilesPaths(Path)\", print_results=False, format_results=True))[0]\n",
    "df = (pd.merge(df, df_path, on='Path', how='outer'))\n",
    "df['DocResult'] = df['DocResult'].fillna(\"UNK\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing It All Together\n",
    "\n",
    "In this section, we will directly compare the original Python Spacy pipeline project with its rgxlog counterpart. Our emphasis is on showcasing the overall brevity of the rgxlog implementation in contrast to the Python Spacy pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Metrics\n",
    "\n",
    "Let's commence by providing an estimated count of total lines in each implementation:\n",
    "\n",
    "- **Total Number of Lines in the original Python implementation:** **4435**\n",
    "- **Total Number of Lines in our rgxlog implementation:** **596** (7 times smaller!)<br>\n",
    "\n",
    "And here's a detailed comparison:\n",
    "\n",
    "![comparison.png](covid_pipeline_files/comparison.png)\n",
    "\n",
    "Now, we will present the combined rgxlog and python code (excluding \"generic ie\" functions and excluding queries) to visually illustrate the compactness of the implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept tagger:\n",
    "```python\n",
    "def lemmatize_text(text_path, lemma_words_path):\n",
    "    # Define a list of words to be lemmatized\n",
    "    lemma_words = [line.strip() for line in open(f\"{lemma_words_path}\") if line.strip()]\n",
    "\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    lemmatized_text = \"\"\n",
    "    for token in doc:\n",
    "        if token.lemma_ in lemma_words:\n",
    "            lemmatized_text += token.lemma_\n",
    "        elif token.like_num:\n",
    "            lemmatized_text += \"like_num\"\n",
    "        else:\n",
    "            lemmatized_text += token.text\n",
    "        lemmatized_text += \" \"\n",
    "\n",
    "    # Write the lemmatized text back to the same file\n",
    "    with open(text_path, 'w') as file:\n",
    "        file.writelines(lemmatized_text)\n",
    "\n",
    "    yield lemmatized_text\n",
    "\n",
    "def annotate_text_with_pos(text_path):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]:\n",
    "            yield token.pos_, Span(token.idx, token.idx + len(token.text))\n",
    "        else:\n",
    "            yield tuple()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "session.import_relation_from_csv(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")\n",
    "\n",
    "%%rgxlog\n",
    "LemmaMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"lemma\"), py_rgx_span(Content, Pattern) -> (Span)\n",
    "replace_spans(\"LemmaMatches\", \"FilesPaths\")\n",
    "POSTable(POS, Span, Path) <- FilesContent(Path, Content), annotate_text_with_pos(Path) -> (POS, Span)\n",
    "POSMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"pos\"), py_rgx_span(Content, Pattern) -> (Span)\n",
    "POSRuleMatches(Label, Span, Path) <- POSTable(POS, Span, Path), POSMatches(Label, Span, Path)\n",
    "replace_spans(\"POSRuleMatches\", \"FilesPaths\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target matcher:\n",
    "```python\n",
    "magic_session.import_relation_from_csv(\"target_rules.csv\", relation_name=\"TargetTagRules\", delimiter=\",\")\n",
    "\n",
    "%%rgxlog\n",
    "TargetTagMatches(Label, Span, Path) <- FilesContent(Path, Content), TargetTagRules(Pattern, Label), py_rgx_span(Content,Pattern) -> (Span)\n",
    "replace_spans(\"TargetTagMatches\", \"FilesPaths\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sectionizer:\n",
    "```python\n",
    "\n",
    "magic_session.import_relation_from_csv(\"section_rules.csv\", relation_name=\"SectionRules\", delimiter=\",\")\n",
    "\n",
    "%%rgxlog\n",
    "SectionRulesMatches(Label, Span, Path) <- FilesContent(Path, Content), SectionRules(Pattern, Label), py_rgx_span(Content,Pattern) -> (Span)\n",
    "replace_spans(\"SectionRulesMatches\", \"FilesPaths\")\n",
    "\n",
    "pattern = \"(?i)(?:diagnoses :|observation_and_plan :|past_medical_history :|problem_list :)(?:(?!labs :|addendum :|allergies :|chief_complaint :|comments :|family_history :|hospital_course :|imaging :|labs_and_studies :|medications :|neurological :|other :|patient_education :|physical_exam :|reason_for_examination :|signature :|social_history :).)*\"\n",
    "\n",
    "new SectionRulesAttribute(str, str)\n",
    "SectionRulesAttribute(pattern, \"positive\")\n",
    "SectionMatches(Path, Span, CovidAttribute) <- FilesContent(Path, Content), SectionRulesAttribute(Pattern, CovidAttribute), py_rgx_span(Content, Pattern) -> (Span)\n",
    "CovidMatches(Path, Span) <- FilesContent(Path, Content), py_rgx_span(Content, \"COVID-19\") -> (Span)\n",
    "SectionCovidAttributes(Path, CovidSpan, CovidAttribute) <- SectionMatches(Path, Span1, CovidAttribute), CovidMatches(Path, Span2), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "\n",
    "Sents(Path, Sent) <- FilesPaths(Path), sent_tokenization(Path) -> (Sent)\n",
    "SentSpans(Path, Sent, SentSpan) <- FilesContent(Path, Content), Sents(Path, Sent), py_rgx_span(Content, Sent) -> (SentSpan)\n",
    "\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- SectionCovidAttributes(Path, AbsCovidSpan, CovidAttribute),\\\n",
    "SentSpans(Path, Sent, SentSpan) ,get_relative_span(AbsCovidSpan, SentSpan) -> (CovidSpan)\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context matcher:\n",
    "```python\n",
    "magic_session.import_relation_from_csv(\"context_rules.csv\", relation_name=\"ContextRules\", delimiter=\"#\")\n",
    "\n",
    "%%rgxlog\n",
    "ContextMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), ContextRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "CovidSpans(Path, Span, Sent) <- Sents(Path, Sent), py_rgx_span(Sent, \"COVID-19\") -> (Span)\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- ContextMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessor:\n",
    "```python\n",
    "def next_sent(text_path):\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = list(doc.sents)\n",
    "    for i in range(len(sentences) - 1):  # Iterate until the second-to-last sentence\n",
    "        yield(sentences[i].text, sentences[i + 1].text)\n",
    "\n",
    "magic_session.register(ie_function=next_sent, ie_function_name = \"next_sent\", in_rel=[DataTypes.string], out_rel=[DataTypes.string,DataTypes.string])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "magic_session.import_relation_from_csv(\"postprocess_pattern_rules.csv\", relation_name=\"PostprocessRules\", delimiter=\"#\")\n",
    "\n",
    "%%rgxlog\n",
    "PostprocessMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostprocessRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- PostprocessMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "\n",
    "NextSent(Path, Sent1, Sent2) <- FilesPaths(Path), next_sent(Path) -> (Sent1, Sent2)\n",
    "new PostProcessWithNextSentenceRules(str, str)\n",
    "PostProcessWithNextSentenceRules(\"(?i)(?:^(?:positive|detected)|results?(?: be)? positive)\", \"positive\")\n",
    "PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostProcessWithNextSentenceRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent1) <- CovidSpans(Path, CovidSpan, Sent1), NextSent(Path, Sent1, Sent2), PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Classifier:\n",
    "```python\n",
    "def attribute_filter(group):\n",
    "    if 'IGNORE' in group.values:\n",
    "        return 'IGNORE'\n",
    "    elif 'negated' in group.values and not 'no_negated' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'future' in group.values and not 'no_future' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'other experiencer' in group.values or 'not relevant' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'uncertain'\n",
    "\n",
    "df = (magic_session.run_commands(\"?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\", print_results=False, format_results=True))[0]\n",
    "if len(df) == 0:\n",
    "    df = DataFrame(columns=[\"Path\",\"CovidSpan\",\"CovidAttribute\"])\n",
    "df['CovidAttribute'] = df.groupby(['CovidSpan', 'Sent'])['CovidAttribute'].transform(attribute_filter)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def classify_doc_helper(group):\n",
    "    if 'positive' in group.values:\n",
    "        return 'POS'\n",
    "    elif 'uncertain' in group.values:\n",
    "        return 'UNK'\n",
    "    elif 'negated' in group.values:\n",
    "        return 'NEG'\n",
    "    else:\n",
    "        return 'UNK'\n",
    "        \n",
    "df['DocResult'] = df.groupby('Path')['CovidAttribute'].transform(classify_doc_helper)\n",
    "df = df[['Path', 'DocResult']]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df_path = (magic_session.run_commands(\"?FilesPaths(Path)\", print_results=False, format_results=True))[0]\n",
    "df = (pd.merge(df, df_path, on='Path', how='outer'))\n",
    "df['DocResult'] = df['DocResult'].fillna(\"UNK\")\n",
    "df\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
