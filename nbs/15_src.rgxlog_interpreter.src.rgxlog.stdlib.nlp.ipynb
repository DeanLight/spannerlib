{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nlp\n",
    "\n",
    "this module supports nlp methods. for documentation:\n",
    "https://stanfordnlp.github.io/CoreNLP/index.html\n",
    "we are aware that starting the engine inside each method affects efficiency.\n",
    "still, don't set `core_nlp_engine` as a global variable,\n",
    "because that way, the java processes will not be killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp src.rgxlog_interpreter.src.rgxlog.stdlib.nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spanner-nlp in /miniconda/lib/python3.8/site-packages (0.0.6)\n",
      "Requirement already satisfied: requests in /miniconda/lib/python3.8/site-packages (from spanner-nlp) (2.28.1)\n",
      "Requirement already satisfied: psutil in /miniconda/lib/python3.8/site-packages (from spanner-nlp) (5.9.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp) (3.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "!pip install spanner-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from os import popen\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "import jdk\n",
    "from spanner_nlp.StanfordCoreNLP import StanfordCoreNLP\n",
    "\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.datatypes.primitive_types import DataTypes\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.stdlib.utils import download_file_from_google_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "JAVA_MIN_VERSION = 1.8\n",
    "\n",
    "NLP_URL = \"https://drive.google.com/u/0/uc?export=download&id=1QixGiHD2mHKuJtB69GHDQA0wTyXtHzjl\"\n",
    "NLP_DIR_NAME = 'stanford-corenlp-4.1.0'\n",
    "# jupyter issue solve\n",
    "# CURR_DIR = Path(__file__).parent\n",
    "CURR_DIR = os.getcwd()\n",
    "NLP_DIR_PATH = str(Path(CURR_DIR) / NLP_DIR_NAME)\n",
    "\n",
    "JAVA_DOWNLOADER = \"install-jdk\"\n",
    "_USER_DIR = Path.home()\n",
    "INSTALLATION_PATH = _USER_DIR / \".jre\"\n",
    "\n",
    "STANFORD_ZIP_GOOGLE_DRIVE_ID = \"1QixGiHD2mHKuJtB69GHDQA0wTyXtHzjl\"\n",
    "STANFORD_ZIP_NAME = \"stanford-corenlp-4.1.0.zip\"\n",
    "STANFORD_ZIP_PATH = Path(CURR_DIR) / STANFORD_ZIP_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _is_installed_nlp() -> bool:\n",
    "    return Path(NLP_DIR_PATH).is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _install_nlp() -> None:\n",
    "    logger.info(f\"Installing {NLP_DIR_NAME} into {CURR_DIR}.\")\n",
    "\n",
    "    if not STANFORD_ZIP_PATH.is_file():\n",
    "        logger.info(f\"downloading {STANFORD_ZIP_NAME}...\")\n",
    "        download_file_from_google_drive(STANFORD_ZIP_GOOGLE_DRIVE_ID, STANFORD_ZIP_PATH)\n",
    "    print(STANFORD_ZIP_PATH)\n",
    "    with open(STANFORD_ZIP_PATH, \"rb\") as zipresp:\n",
    "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            logging.info(f\"Extracting files from the zip folder...\")\n",
    "            zfile.extractall(CURR_DIR)\n",
    "\n",
    "    logging.info(\"installation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _is_installed_java() -> bool:\n",
    "    version = popen(\n",
    "        \"java -version 2>&1 | grep 'version' 2>&1 | awk -F\\\\\\\" '{ split($2,a,\\\".\\\"); print a[1]\\\".\\\"a[2]}'\").read()\n",
    "\n",
    "    if len(version) != 0 and float(version) >= JAVA_MIN_VERSION:\n",
    "        return True\n",
    "\n",
    "    return Path(INSTALLATION_PATH).is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _run_installation() -> None:\n",
    "    if not _is_installed_nlp():\n",
    "        print(\"HERE\")\n",
    "        _install_nlp()\n",
    "        assert _is_installed_nlp()\n",
    "    if not _is_installed_java():\n",
    "        print(\"THERE\")\n",
    "        logging.info(f\"Installing JRE into {INSTALLATION_PATH}.\")\n",
    "        jdk.install('8', jre=True)\n",
    "        if _is_installed_java():\n",
    "            logging.info(\"installation completed.\")\n",
    "        else:\n",
    "            raise IOError(\"installation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "/spanner_workbench/nbs/stanford-corenlp-4.1.0.zip\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43m_run_installation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m CoreNLPEngine \u001b[38;5;241m=\u001b[39m StanfordCoreNLP(NLP_DIR_PATH)\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36m_run_installation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_installed_nlp():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHERE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43m_install_nlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _is_installed_nlp()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_installed_java():\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36m_install_nlp\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(STANFORD_ZIP_PATH)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(STANFORD_ZIP_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zipresp:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zfile:\n\u001b[1;32m     11\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting files from the zip folder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m         zfile\u001b[38;5;241m.\u001b[39mextractall(CURR_DIR)\n",
      "File \u001b[0;32m/miniconda/lib/python3.8/zipfile.py:1269\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1269\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/miniconda/lib/python3.8/zipfile.py:1336\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[0;32m-> 1336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "_run_installation()\n",
    "CoreNLPEngine = StanfordCoreNLP(NLP_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_wrapper(sentence: str) -> Iterator:\n",
    "    for token in CoreNLPEngine.tokenize(sentence):\n",
    "        yield token[\"token\"], token[\"span\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "Tokenize = dict(ie_function=tokenize_wrapper,\n",
    "                ie_function_name='Tokenize',\n",
    "                in_rel=[DataTypes.string],\n",
    "                out_rel=[DataTypes.string, DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ssplit_wrapper(sentence: str) -> Iterator:\n",
    "    for s in CoreNLPEngine.ssplit(sentence):\n",
    "        yield s,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "SSplit = dict(ie_function=ssplit_wrapper,\n",
    "              ie_function_name='SSplit',\n",
    "              in_rel=[DataTypes.string],\n",
    "              out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pos_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.pos(sentence):\n",
    "        yield res[\"token\"], res[\"pos\"], res[\"span\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "POS = dict(ie_function=pos_wrapper,\n",
    "           ie_function_name='POS',\n",
    "           in_rel=[DataTypes.string],\n",
    "           out_rel=[DataTypes.string, DataTypes.string, DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lemma_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.lemma(sentence):\n",
    "        yield res[\"token\"], res[\"lemma\"], res[\"span\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "Lemma = dict(ie_function=lemma_wrapper,\n",
    "             ie_function_name='Lemma',\n",
    "             in_rel=[DataTypes.string],\n",
    "             out_rel=[DataTypes.string, DataTypes.string, DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ner_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.ner(sentence):\n",
    "        if res[\"ner\"] != 'O':\n",
    "            yield res[\"token\"], res[\"ner\"], res[\"span\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "NER = dict(ie_function=ner_wrapper,\n",
    "           ie_function_name='NER',\n",
    "           in_rel=[DataTypes.string],\n",
    "           out_rel=[DataTypes.string, DataTypes.string, DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def entitymentions_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.entitymentions(sentence):\n",
    "        confidence = json.dumps(res[\"nerConfidences\"]).replace(\"\\\"\", \"'\")\n",
    "        yield (res[\"docTokenBegin\"], res[\"docTokenEnd\"], res[\"tokenBegin\"], res[\"tokenEnd\"], res[\"text\"],\n",
    "               res[\"characterOffsetBegin\"], res[\"characterOffsetEnd\"], res[\"ner\"], confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "EntityMentions = dict(ie_function=entitymentions_wrapper,\n",
    "                      ie_function_name='EntityMentions',\n",
    "                      in_rel=[DataTypes.string],\n",
    "                      out_rel=[DataTypes.integer, DataTypes.integer, DataTypes.integer, DataTypes.integer,\n",
    "                               DataTypes.string, DataTypes.integer, DataTypes.integer, DataTypes.string,\n",
    "                               DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def regexner_wrapper(sentence: str, pattern: str) -> Iterator:\n",
    "    # for res in CoreNLPEngine.regexner(sentence, pattern):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "RGXNer = dict(ie_function=regexner_wrapper,\n",
    "              ie_function_name='RGXNer',\n",
    "              in_rel=[DataTypes.string, DataTypes.string],\n",
    "              out_rel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokensregex_wrapper(sentence: str, pattern: str) -> Iterator:\n",
    "    # for res in CoreNLPEngine.tokensregex(sentence, pattern):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "TokensRegex = dict(ie_function=tokensregex_wrapper,\n",
    "                   ie_function_name='TokensRegex',\n",
    "                   in_rel=[DataTypes.string, DataTypes.string],\n",
    "                   out_rel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cleanxml_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.cleanxml(sentence)[\"tokens\"]:\n",
    "        yield res['index'], res['word'], res['originalText'], res['characterOffsetBegin'], res['characterOffsetEnd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "CleanXML = dict(ie_function=cleanxml_wrapper,\n",
    "                ie_function_name='CleanXML',\n",
    "                in_rel=[DataTypes.string],\n",
    "                out_rel=[DataTypes.integer, DataTypes.string, DataTypes.string, DataTypes.integer, DataTypes.integer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.parse(sentence):\n",
    "        # note #1: this yields a tuple\n",
    "        # note #2: we replace the newlines with `<nl> because it is difficult to tell the results apart otherwise\n",
    "        yield res.replace(\"\\n\", \"<nl>\").replace(\"\\r\", \"\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "Parse = dict(ie_function=parse_wrapper,\n",
    "             ie_function_name='Parse',\n",
    "             in_rel=[DataTypes.string],\n",
    "             out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dependency_parse_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.dependency_parse(sentence):\n",
    "        yield res['dep'], res['governor'], res['governorGloss'], res['dependent'], res['dependentGloss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DepParse = dict(ie_function=dependency_parse_wrapper,\n",
    "                ie_function_name='DepParse',\n",
    "                in_rel=[DataTypes.string],\n",
    "                out_rel=[DataTypes.string, DataTypes.integer, DataTypes.string, DataTypes.integer, DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def coref_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.coref(sentence):\n",
    "        yield (res['id'], res['text'], res['type'], res['number'], res['gender'], res['animacy'], res['startIndex'],\n",
    "               res['endIndex'], res['headIndex'], res['sentNum'],\n",
    "               tuple(res['position']), str(res['isRepresentativeMention']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "Coref = dict(ie_function=coref_wrapper,\n",
    "             ie_function_name='Coref',\n",
    "             in_rel=[DataTypes.string],\n",
    "             out_rel=[DataTypes.integer, DataTypes.string, DataTypes.string, DataTypes.string, DataTypes.string,\n",
    "                      DataTypes.string, DataTypes.integer, DataTypes.integer, DataTypes.integer, DataTypes.integer,\n",
    "                      DataTypes.span, DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def openie_wrapper(sentence: str) -> Iterator:\n",
    "    for lst in CoreNLPEngine.openie(sentence):\n",
    "        for res in lst:\n",
    "            yield (res['subject'], tuple(res['subjectSpan']), res['relation'], tuple(res['relationSpan']),\n",
    "                   res['object'], tuple(res['objectSpan']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "OpenIE = dict(ie_function=openie_wrapper,\n",
    "              ie_function_name='OpenIE',\n",
    "              in_rel=[DataTypes.string],\n",
    "              out_rel=[DataTypes.string, DataTypes.span, DataTypes.string, DataTypes.span, DataTypes.string,\n",
    "                       DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def kbp_wrapper(sentence: str) -> Iterator:\n",
    "    for lst in CoreNLPEngine.kbp(sentence):\n",
    "        for res in lst:\n",
    "            yield (res['subject'], tuple(res['subjectSpan']), res['relation'], tuple(res['relationSpan']),\n",
    "                   res['object'], tuple(res['objectSpan']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "KBP = dict(ie_function=kbp_wrapper,\n",
    "           ie_function_name='KBP',\n",
    "           in_rel=[DataTypes.string],\n",
    "           out_rel=[DataTypes.string, DataTypes.span, DataTypes.string, DataTypes.span, DataTypes.string,\n",
    "                    DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def quote_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.quote(sentence):\n",
    "        yield (res['id'], res['text'], res['beginIndex'], res['endIndex'], res['beginToken'], res['endToken'],\n",
    "               res['beginSentence'], res['endSentence'], res['speaker'], res['canonicalSpeaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "Quote = dict(ie_function=quote_wrapper,\n",
    "             ie_function_name='Quote',\n",
    "             in_rel=[DataTypes.string],\n",
    "             out_rel=[DataTypes.integer, DataTypes.string, DataTypes.integer, DataTypes.integer, DataTypes.integer,\n",
    "                      DataTypes.integer, DataTypes.integer, DataTypes.integer, DataTypes.string, DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# currently ignoring sentimentTree\n",
    "def sentiment_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.sentiment(sentence):\n",
    "        yield int(res['sentimentValue']), res['sentiment'], json.dumps(res['sentimentDistribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "Sentiment = dict(ie_function=sentiment_wrapper,\n",
    "                 ie_function_name='Sentiment',\n",
    "                 in_rel=[DataTypes.string],\n",
    "                 out_rel=[DataTypes.integer, DataTypes.string, DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def truecase_wrapper(sentence: str) -> Iterator:\n",
    "    for res in CoreNLPEngine.truecase(sentence):\n",
    "        yield res['token'], res['span'], res['truecase'], res['truecaseText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "TrueCase = dict(ie_function=truecase_wrapper,\n",
    "                ie_function_name='TrueCase',\n",
    "                in_rel=[DataTypes.string],\n",
    "                out_rel=[DataTypes.string, DataTypes.span, DataTypes.string, DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def udfeats_wrapper(sentence: str) -> Iterator:\n",
    "    # for token in CoreNLPEngine.udfeats(sentence):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "UDFeats = dict(ie_function=udfeats_wrapper,\n",
    "               ie_function_name='UDFeats',\n",
    "               in_rel=[DataTypes.string],\n",
    "               out_rel=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
