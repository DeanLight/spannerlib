{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp src.rgxlog_interpreter.src.rgxlog.engine.session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Union, Optional, Callable, Type, Iterable, no_type_check, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lark in /miniconda/lib/python3.8/site-packages (1.1.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "!pip install lark   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /miniconda/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /miniconda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /miniconda/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /miniconda/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /miniconda/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /miniconda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tabulate in /miniconda/lib/python3.8/site-packages (0.9.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "!pip install pandas\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from lark.lark import Lark\n",
    "from pandas import DataFrame\n",
    "from tabulate import tabulate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nbdev_spanner_workbench/spanner_workbench/src/rgxlog_interpreter/src/rgxlog/stdlib/stanford-corenlp-4.1.0\n",
      "/nbdev_spanner_workbench/spanner_workbench/src/rgxlog_interpreter/src/rgxlog/stdlib/stanford-corenlp-4.1.0\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine import *\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.datatypes.ast_node_types import AddFact, RelationDeclaration\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.datatypes.primitive_types import Span, DataTypes, DataTypeMapping\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine import FALSE_VALUE, TRUE_VALUE\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.execution import (Query, FREE_VAR_PREFIX, naive_execution)\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.adding_inference_rules_to_term_graph import AddRulesToTermGraph\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.optimizations_passes import RemoveUselessRelationsFromRule\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.lark_passes import (RemoveTokens, FixStrings, CheckReservedRelationNames,\n",
    "                                              ConvertSpanNodesToSpanInstances, ConvertStatementsToStructuredNodes,\n",
    "                                              CheckDefinedReferencedVariables,\n",
    "                                              CheckReferencedRelationsExistenceAndArity,\n",
    "                                              CheckReferencedIERelationsExistenceAndArity, CheckRuleSafety,\n",
    "                                              TypeCheckAssignments, TypeCheckRelations,\n",
    "                                              SaveDeclaredRelationsSchemas, ResolveVariablesReferences,\n",
    "                                              ExecuteAssignments, AddStatementsToNetxParseGraph, GenericPass)\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.state.graphs import TermGraph, NetxStateGraph, GraphBase, TermGraphBase\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.state.symbol_table import SymbolTable, SymbolTableBase\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.utils.general_utils import rule_to_relation_name, string_to_span, SPAN_PATTERN, QUERY_RESULT_PREFIX\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.utils.passes_utils import LarkNode\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.stdlib.json_path import JsonPath, JsonPathFull\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.stdlib.nlp import (Tokenize, SSplit, POS, Lemma, NER, EntityMentions, CleanXML, Parse, DepParse, Coref,\n",
    "                              OpenIE, KBP, Quote, Sentiment, TrueCase)\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.stdlib.python_regex import PYRGX, PYRGX_STRING\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.stdlib.rust_spanner_regex import RGX, RGX_STRING, RGX_FROM_FILE, RGX_STRING_FROM_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "CSV_DELIMITER = \";\"\n",
    "\n",
    "# ordered by rgx, json, nlp, etc.\n",
    "PREDEFINED_IE_FUNCS = [PYRGX, PYRGX_STRING, RGX, RGX_STRING, RGX_FROM_FILE, RGX_STRING_FROM_FILE,\n",
    "                       JsonPath, JsonPathFull,\n",
    "                       Tokenize, SSplit, POS, Lemma, NER, EntityMentions, CleanXML, Parse, DepParse, Coref, OpenIE, KBP, Quote, Sentiment,\n",
    "                       TrueCase]\n",
    "\n",
    "STRING_PATTERN = re.compile(r\"^[^\\r\\n]+$\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GRAMMAR_FILE_NAME = 'grammar.lark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _infer_relation_type(row: Iterable) -> Sequence[DataTypes]:\n",
    "    \"\"\"\n",
    "    Guess the relation type based on the data.\n",
    "    We support both the actual types (e.g. 'Span'), and their string representation ( e.g. `\"[0,8)\"`).\n",
    "\n",
    "    @param row: an iterable of values, extracted from a csv file or a dataframe.\n",
    "    @raise ValueError: if there is a cell inside `row` of an illegal type.\n",
    "    \"\"\"\n",
    "    relation_types = []\n",
    "    for cell in row:\n",
    "        try:\n",
    "            int(cell)  # check if the cell can be converted to integer\n",
    "            relation_types.append(DataTypes.integer)\n",
    "        except (ValueError, TypeError):\n",
    "            if isinstance(cell, Span) or re.match(SPAN_PATTERN, cell):\n",
    "                relation_types.append(DataTypes.span)\n",
    "            elif re.match(STRING_PATTERN, cell):\n",
    "                relation_types.append(DataTypes.string)\n",
    "            else:\n",
    "                raise ValueError(f\"value doesn't match any datatype: {cell}\")\n",
    "\n",
    "    return relation_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _verify_relation_types(row: Iterable, expected_types: Iterable[DataTypes]) -> None:\n",
    "    if _infer_relation_type(row) != expected_types:\n",
    "        raise Exception(f\"row:\\n{str(row)}\\ndoes not match the relation's types:\\n{str(expected_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _text_to_typed_data(term_list: Sequence[DataTypeMapping.term], relation_types: Sequence[DataTypes]) -> List[DataTypeMapping.term]:\n",
    "    transformed_term_list: List[DataTypeMapping.term] = []\n",
    "    for str_or_object, rel_type in zip(term_list, relation_types):\n",
    "        if rel_type == DataTypes.span:\n",
    "            if isinstance(str_or_object, Span):\n",
    "                transformed_term_list.append(str_or_object)\n",
    "            else:\n",
    "                assert isinstance(str_or_object, str), \"a span can only be a Span object or a string\"\n",
    "                transformed_span = string_to_span(str_or_object)\n",
    "                if transformed_span is None:\n",
    "                    raise TypeError(f\"expected a Span, found this instead: {str_or_object}\")\n",
    "                transformed_term_list.append(transformed_span)\n",
    "\n",
    "        elif rel_type == DataTypes.integer:\n",
    "            if isinstance(str_or_object, Span):\n",
    "                raise TypeError(f\"expected an int, found Span instead: {str_or_object}\")\n",
    "            transformed_term_list.append(int(str_or_object))\n",
    "        else:\n",
    "            assert rel_type == DataTypes.string, f\"illegal type given: {rel_type}\"\n",
    "            transformed_term_list.append(str_or_object)\n",
    "\n",
    "    return transformed_term_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_query_results(query: Query, query_results: List) -> Union[DataFrame, List]:\n",
    "    \"\"\"\n",
    "    Formats a single result from the engine into a usable format.\n",
    "\n",
    "    @param query: the query that was executed, and outputted `query_results`.\n",
    "    @param query_results: the results after executing the aforementioned query.\n",
    "    @return: a false value, a true value, or a dataframe representing the query + its results.\n",
    "    \"\"\"\n",
    "    assert isinstance(query_results, list), \"illegal results format\"\n",
    "\n",
    "    # check for the special conditions for which we can't print a table: no results were returned or a single\n",
    "    # empty tuple was returned\n",
    "\n",
    "    if query_results == FALSE_VALUE:  # empty list := false\n",
    "        return FALSE_VALUE\n",
    "    elif query_results == TRUE_VALUE:  # single tuple := true\n",
    "        return TRUE_VALUE\n",
    "    else:\n",
    "        # convert the resulting tuples to a more organized format\n",
    "        results_matrix = []\n",
    "        for result in query_results:\n",
    "            # span tuples are converted to Span objects\n",
    "            converted_span_result = [Span(term[0], term[1]) if (isinstance(term, tuple) and len(term) == 2)\n",
    "                                     else term\n",
    "                                     for term in result]\n",
    "\n",
    "            results_matrix.append(converted_span_result)\n",
    "\n",
    "        # get the free variables of the query, they will be used as headers\n",
    "        query_free_vars = [term for term, term_type in zip(query.term_list, query.type_list)\n",
    "                           if term_type is DataTypes.free_var_name]\n",
    "\n",
    "        return DataFrame(data=results_matrix, columns=query_free_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tabulate_result(result: Union[DataFrame, List]) -> str:\n",
    "    \"\"\"\n",
    "    Organizes a query result in a table\n",
    "    for example:\n",
    "        {QUERY_RESULT_PREFIX}'lecturer_of(X, \"abigail\")':\n",
    "          X\n",
    "       -------\n",
    "        linus\n",
    "        walter\n",
    "\n",
    "    there are two cases where a table will not be printed:\n",
    "    1. the query returned no results. in this case '[]' will be printed\n",
    "    2. the query returned a single empty tuple, in this case '[()]' will be printed\n",
    "\n",
    "    @param result: the query result (free variable names are the dataframe's column names).\n",
    "    @return: a tabulated string.\n",
    "    \"\"\"\n",
    "    if isinstance(result, DataFrame):\n",
    "        # query results can be printed as a table\n",
    "        result_string = tabulate(result, headers=\"keys\", tablefmt=\"presto\", stralign=\"center\", showindex=False)\n",
    "    else:\n",
    "        assert isinstance(result, list), \"illegal result format\"\n",
    "        if len(result) == 0:\n",
    "            result_string = \"[]\"\n",
    "        else:\n",
    "            assert len(result) == 1, \"illegal result format\"\n",
    "            result_string = \"[()]\"\n",
    "\n",
    "    return result_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def queries_to_string(query_results: List[Tuple[Query, List]]) -> str:\n",
    "    \"\"\"\n",
    "    Takes in a list of results from the engine and converts them into a single string, which contains\n",
    "    either a table, a false value (=`[]`), or a true value (=`[tuple()]`), for each result.\n",
    "\n",
    "    for example:\n",
    "\n",
    "    {QUERY_RESULT_PREFIX}'lecturer_of(X, \"abigail\")':\n",
    "      X\n",
    "    --------\n",
    "    linus\n",
    "    walter\n",
    "\n",
    "\n",
    "    @param query_results: List[the Query object used in execution, the execution's results (from engine)].\n",
    "    \"\"\"\n",
    "\n",
    "    all_result_strings = []\n",
    "    query_results = list(filter(None, query_results))  # remove Nones\n",
    "    for query, results in query_results:\n",
    "        query_result_string = tabulate_result(format_query_results(query, results))\n",
    "        query_title = f\"{QUERY_RESULT_PREFIX}'{query}':\"\n",
    "\n",
    "        # combine the title and table to a single string and save it to the prints buffer\n",
    "        titled_result_string = f'{query_title}\\n{query_result_string}\\n'\n",
    "        all_result_strings.append(titled_result_string)\n",
    "    return \"\\n\".join(all_result_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Session:\n",
    "    def __init__(self, symbol_table: Optional[SymbolTableBase] = None, parse_graph: Optional[GraphBase] = None,\n",
    "                 term_graph: Optional[TermGraphBase] = None):\n",
    "        \"\"\"\n",
    "        parse_graph is the lark graph which contains is the result of parsing a single statement,\n",
    "        term_graph is the combined tree of all statements so far, which describes the connection between relations.\n",
    "        \"\"\"\n",
    "        if symbol_table is None:\n",
    "            self._symbol_table: SymbolTableBase = SymbolTable()\n",
    "            self._symbol_table.register_predefined_ie_functions(PREDEFINED_IE_FUNCS)\n",
    "\n",
    "        else:\n",
    "            self._symbol_table = symbol_table\n",
    "\n",
    "        self._parse_graph = NetxStateGraph() if parse_graph is None else parse_graph\n",
    "        self._term_graph: TermGraphBase = TermGraph() if term_graph is None else term_graph\n",
    "        self._engine = SqliteEngine()\n",
    "        self._execution = naive_execution\n",
    "\n",
    "        self._pass_stack: List[Type[GenericPass]] = [\n",
    "            RemoveTokens,\n",
    "            FixStrings,\n",
    "            CheckReservedRelationNames,\n",
    "            ConvertSpanNodesToSpanInstances,\n",
    "            ConvertStatementsToStructuredNodes,\n",
    "            CheckDefinedReferencedVariables,\n",
    "            CheckReferencedRelationsExistenceAndArity,\n",
    "            CheckReferencedIERelationsExistenceAndArity,\n",
    "            CheckRuleSafety,\n",
    "            TypeCheckAssignments,\n",
    "            TypeCheckRelations,\n",
    "            SaveDeclaredRelationsSchemas,\n",
    "            ResolveVariablesReferences,\n",
    "            ExecuteAssignments,\n",
    "            AddStatementsToNetxParseGraph,\n",
    "            AddRulesToTermGraph\n",
    "        ]\n",
    "\n",
    "        self._grammar = Session._get_grammar_from_file()\n",
    "\n",
    "        self._parser = Lark(self._grammar, parser='lalr')\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_grammar_from_file() -> str:\n",
    "        \"\"\"\n",
    "        @return: Grammar from grammar file in string format.\n",
    "        \"\"\"\n",
    "\n",
    "        grammar_file_path = Path(os.path.join('/nbdev_spanner_workbench','spanner_workbench','src','rgxlog_interpreter','src','rgxlog','grammar'))\n",
    "        with open(grammar_file_path / GRAMMAR_FILE_NAME, 'r') as grammar_file:\n",
    "            return grammar_file.read()\n",
    "\n",
    "    def _run_passes(self, lark_tree: LarkNode, pass_list: list) -> None:\n",
    "        \"\"\"\n",
    "        Runs the passes in pass_list on tree, one after another.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"initial lark tree:\\n{lark_tree.pretty()}\")\n",
    "        logger.debug(f\"initial term graph:\\n{self._term_graph}\")\n",
    "\n",
    "        for curr_pass in pass_list:\n",
    "            curr_pass_object = curr_pass(parse_graph=self._parse_graph,\n",
    "                                         symbol_table=self._symbol_table,\n",
    "                                         term_graph=self._term_graph)\n",
    "            new_tree = curr_pass_object.run_pass(tree=lark_tree)\n",
    "            if new_tree is not None:\n",
    "                lark_tree = new_tree\n",
    "                logger.debug(f\"lark tree after {curr_pass.__name__}:\\n{lark_tree.pretty()}\")\n",
    "    \n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return \"\\n\".join([repr(self._symbol_table), repr(self._parse_graph)])\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f'Symbol Table:\\n{str(self._symbol_table)}\\n\\nTerm Graph:\\n{str(self._parse_graph)}'\n",
    "    \n",
    "    def run_commands(self, query: str, print_results: bool = True, format_results: bool = False) -> (\n",
    "        Union[List[Union[List, List[Tuple], DataFrame]], List[Tuple[Query, List]]]):\n",
    "        \"\"\"\n",
    "        Generates an AST and passes it through the pass stack.\n",
    "\n",
    "        @param format_results: if this is true, return the formatted result instead of the `[Query, List]` pair.\n",
    "        @param query: the user's input.\n",
    "        @param print_results: whether to print the results to stdout or not.\n",
    "        @return: the results of every query, in a list.\n",
    "        \"\"\"\n",
    "        query_results = []\n",
    "        parse_tree = self._parser.parse(query)\n",
    "        for statement in parse_tree.children:\n",
    "            self._run_passes(statement, self._pass_stack)\n",
    "            query_result = self._execution(parse_graph=self._parse_graph,\n",
    "                                           symbol_table=self._symbol_table,\n",
    "                                           rgxlog_engine=self._engine,\n",
    "                                           term_graph=self._term_graph)\n",
    "            if query_result is not None:\n",
    "                query_results.append(query_result)\n",
    "                if print_results:\n",
    "                    print(queries_to_string([query_result]))\n",
    "\n",
    "        if format_results:\n",
    "            return [format_query_results(*query_result) for query_result in query_results]\n",
    "        else:\n",
    "            return query_results\n",
    "        \n",
    "\n",
    "    def register(self, ie_function: Callable, ie_function_name: str, in_rel: List[DataTypes],\n",
    "                out_rel: Union[List[DataTypes], Callable[[int], Sequence[DataTypes]]]) -> None:\n",
    "        \"\"\"\n",
    "        Registers an ie function.\n",
    "\n",
    "        @see params in IEFunction's __init__.\n",
    "        \"\"\"\n",
    "        self._symbol_table.register_ie_function(ie_function, ie_function_name, in_rel, out_rel)\n",
    "\n",
    "    def get_pass_stack(self) -> List[Type[GenericPass]]:\n",
    "        \"\"\"\n",
    "        @return: the current pass stack.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._pass_stack.copy()\n",
    "    \n",
    "\n",
    "    def set_pass_stack(self, user_stack: List[Type[GenericPass]]) -> List[Type[GenericPass]]:\n",
    "        \"\"\"\n",
    "        Sets a new pass stack instead of the current one.\n",
    "\n",
    "        @param user_stack: a user supplied pass stack.\n",
    "        @return: success message with the new pass stack.\n",
    "        \"\"\"\n",
    "\n",
    "        if type(user_stack) is not list:\n",
    "            raise TypeError('user stack should be a list of passes')\n",
    "        for pass_ in user_stack:\n",
    "            if not issubclass(pass_, GenericPass):\n",
    "                raise TypeError('user stack should be a subclass of `GenericPass`')\n",
    "\n",
    "        self._pass_stack = user_stack.copy()\n",
    "        return self.get_pass_stack()\n",
    "\n",
    "    def _remove_rule_relation_from_symbols_and_engine(self, relation_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Removes the relation from the symbol table and the execution tables.\n",
    "\n",
    "        @param relation_name: the name of the relation ot remove.\n",
    "        \"\"\"\n",
    "        self._symbol_table.remove_rule_relation(relation_name)\n",
    "        self._engine.remove_table(relation_name)\n",
    "        \n",
    "    def remove_rule(self, rule: str) -> None:\n",
    "        \"\"\"\n",
    "        Remove a rule from the rgxlog's engine.\n",
    "\n",
    "        @param rule: the rule to be removed.\n",
    "        \"\"\"\n",
    "        is_last = self._term_graph.remove_rule(rule)\n",
    "        if is_last:\n",
    "            relation_name = rule_to_relation_name(rule)\n",
    "            self._remove_rule_relation_from_symbols_and_engine(relation_name)\n",
    "            \n",
    "    def remove_all_rules(self, rule_head: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Removes all rules from the engine.\n",
    "\n",
    "        @param rule_head: if rule head is not none we remove all rules with rule_head.\n",
    "        \"\"\"\n",
    "\n",
    "        if rule_head is None:\n",
    "            self._term_graph = TermGraph()\n",
    "            relations_names = self._symbol_table.remove_all_rule_relations()\n",
    "            self._engine.remove_tables(relations_names)\n",
    "        else:\n",
    "            self._term_graph.remove_rules_with_head(rule_head)\n",
    "            self._remove_rule_relation_from_symbols_and_engine(rule_head)\n",
    "            \n",
    "    def _add_imported_relation_to_engine(self, relation_table: Iterable, relation_name: str, relation_types: Sequence[DataTypes]) -> None:\n",
    "        symbol_table = self._symbol_table\n",
    "        engine = self._engine\n",
    "        # first make sure the types are legal, then we add them to the engine (to make sure\n",
    "        #  we don't add them in case of an error)\n",
    "        facts = []\n",
    "\n",
    "        for row in relation_table:\n",
    "            _verify_relation_types(row, relation_types)\n",
    "            typed_line = _text_to_typed_data(row, relation_types)\n",
    "            facts.append(AddFact(relation_name, typed_line, relation_types))\n",
    "\n",
    "        # declare relation if it does not exist\n",
    "        if not symbol_table.contains_relation(relation_name):\n",
    "            engine.declare_relation_table(RelationDeclaration(relation_name, relation_types))\n",
    "            symbol_table.add_relation_schema(relation_name, relation_types, False)\n",
    "\n",
    "        for fact in facts:\n",
    "            engine.add_fact(fact)\n",
    "            \n",
    "    def import_relation_from_csv(self, csv_file_name: Path, relation_name: str = None, delimiter: str = CSV_DELIMITER) -> None:\n",
    "        if not Path(csv_file_name).is_file():\n",
    "            raise IOError(\"csv file does not exist\")\n",
    "\n",
    "        if os.stat(csv_file_name).st_size == 0:\n",
    "            raise IOError(\"csv file is empty\")\n",
    "\n",
    "        # the relation_name is either an argument or the file's name\n",
    "        if relation_name is None:\n",
    "            relation_name = Path(csv_file_name).stem\n",
    "\n",
    "        with open(csv_file_name) as fh:\n",
    "            reader = csv.reader(fh, delimiter=delimiter)\n",
    "\n",
    "            # read first line and go back to start of file - make sure there is no empty line!\n",
    "            relation_types = _infer_relation_type(next(reader))\n",
    "            fh.seek(0)\n",
    "\n",
    "            self._add_imported_relation_to_engine(reader, relation_name, relation_types)\n",
    "            \n",
    "    def import_relation_from_df(self, relation_df: DataFrame, relation_name: str) -> None:\n",
    "        data = relation_df.values.tolist()\n",
    "\n",
    "        if not isinstance(data, list):\n",
    "            raise Exception(\"dataframe could not be converted to list\")\n",
    "\n",
    "        if len(data) < 1:\n",
    "            raise Exception(\"dataframe is empty\")\n",
    "\n",
    "        relation_types = _infer_relation_type(data[0])\n",
    "\n",
    "        self._add_imported_relation_to_engine(data, relation_name, relation_types)\n",
    "        \n",
    "    def send_commands_result_into_csv(self, commands: str, csv_file_name: Path, delimiter: str = CSV_DELIMITER) -> None:\n",
    "        \"\"\"\n",
    "        run commands as usual and output their formatted results into a csv file (the commands should contain a query)\n",
    "        @param commands: the commands to run\n",
    "        @param csv_file_name: the file into which the output will be written\n",
    "        @param delimiter: a csv separator between values\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        commands_results = self.run_commands(commands, print_results=False)\n",
    "        if len(commands_results) != 1:\n",
    "            raise Exception(\"the commands must have exactly one output\")\n",
    "\n",
    "        formatted_result = format_query_results(*commands_results[0])\n",
    "\n",
    "        if isinstance(formatted_result, DataFrame):\n",
    "            formatted_result.to_csv(csv_file_name, index=False, sep=delimiter)\n",
    "        else:\n",
    "            # true or false\n",
    "            with open(csv_file_name, \"w\", newline=\"\") as f:\n",
    "                writer = csv.writer(f, delimiter=delimiter)\n",
    "                writer.writerows(formatted_result)\n",
    "                \n",
    "    def send_commands_result_into_df(self, commands: str) -> Union[DataFrame, List]:\n",
    "        \"\"\"\n",
    "        run commands as usual and output their formatted results into a dataframe (the commands should contain a query)\n",
    "        @param commands: the commands to run\n",
    "        @return: formatted results (possibly a dataframe)\n",
    "        \"\"\"\n",
    "        commands_results = self.run_commands(commands, print_results=False)\n",
    "        if len(commands_results) != 1:\n",
    "            raise Exception(\"the commands must have exactly one output\")\n",
    "\n",
    "        return format_query_results(*commands_results[0])\n",
    "    \n",
    "    def _relation_name_to_query(self, relation_name: str) -> str:\n",
    "        symbol_table = self._symbol_table\n",
    "        relation_schema = symbol_table.get_relation_schema(relation_name)\n",
    "        relation_arity = len(relation_schema)\n",
    "        query = (f\"?{relation_name}(\" + \", \".join(f\"{FREE_VAR_PREFIX}{i}\" for i in range(relation_arity)) + \")\")\n",
    "        return query\n",
    "    \n",
    "    def export_relation_into_df(self, relation_name: str) -> Union[DataFrame, List]:\n",
    "        query = self._relation_name_to_query(relation_name)\n",
    "        return self.send_commands_result_into_df(query)\n",
    "\n",
    "    def export_relation_into_csv(self, csv_file_name: Path, relation_name: str, delimiter: str = CSV_DELIMITER) -> None:\n",
    "        query = self._relation_name_to_query(relation_name)\n",
    "        self.send_commands_result_into_csv(query, csv_file_name, delimiter)\n",
    "        \n",
    "    def print_registered_ie_functions(self) -> None:\n",
    "        \"\"\"\n",
    "        Prints information about the registered ie functions.\n",
    "        \"\"\"\n",
    "        self._symbol_table.print_registered_ie_functions()\n",
    "        \n",
    "    def remove_ie_function(self, name: str) -> None:\n",
    "        \"\"\"\n",
    "        Removes a function from the symbol table.\n",
    "\n",
    "        @param name: the name of the ie function to remove.\n",
    "        \"\"\"\n",
    "        self._symbol_table.remove_ie_function(name)\n",
    "    def remove_all_ie_functions(self) -> None:\n",
    "        \"\"\"\n",
    "        Removes all the ie functions from the symbol table.\n",
    "        \"\"\"\n",
    "        self._symbol_table.remove_all_ie_functions()\n",
    "        \n",
    "    def print_all_rules(self, head: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Prints all the rules that are registered.\n",
    "\n",
    "        @param head: if specified it will print only rules with the given head relation name.\n",
    "        \"\"\"\n",
    "\n",
    "        self._term_graph.print_all_rules(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:using database file: /tmp/tmpg4a2wu_p_sqlite\n",
      "DEBUG:root:initial lark tree:\n",
      "relation_declaration\n",
      "  relation_name\tA\n",
      "  decl_term_list\n",
      "    decl_string\n",
      "\n",
      "DEBUG:root:initial term graph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n",
      "DEBUG:root:lark tree after RemoveTokens:\n",
      "relation_declaration\n",
      "  relation_name\tA\n",
      "  decl_term_list\n",
      "    decl_string\n",
      "\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.adding_inference_rules_to_term_graph:term graph after AddRulesToTermGraph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command=\"SELECT DISTINCT name FROM sqlite_master WHERE type='table' AND name='A'\"\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command='CREATE TABLE A (col0)'\n",
      "DEBUG:root:initial lark tree:\n",
      "add_fact\n",
      "  relation_name\tA\n",
      "  const_term_list\n",
      "    string\t\"Asmaa\"\n",
      "\n",
      "DEBUG:root:initial term graph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n",
      "DEBUG:root:lark tree after RemoveTokens:\n",
      "add_fact\n",
      "  relation_name\tA\n",
      "  const_term_list\n",
      "    string\tAsmaa\n",
      "\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.adding_inference_rules_to_term_graph:term graph after AddRulesToTermGraph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command='INSERT INTO A (col0)\\nVALUES (\"Asmaa\")'\n",
      "DEBUG:root:initial lark tree:\n",
      "query\n",
      "  relation_name\tA\n",
      "  term_list\n",
      "    free_var_name\tX\n",
      "\n",
      "DEBUG:root:initial term graph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n",
      "DEBUG:root:lark tree after RemoveTokens:\n",
      "query\n",
      "  relation_name\tA\n",
      "  term_list\n",
      "    free_var_name\tX\n",
      "\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.adding_inference_rules_to_term_graph:term graph after AddRulesToTermGraph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command=\"SELECT DISTINCT name FROM sqlite_master WHERE type='table' AND name='__rgxlog__A_select0'\"\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command='CREATE TABLE __rgxlog__A_select0 (col0)'\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command='INSERT INTO __rgxlog__A_select0 SELECT DISTINCT * FROM A'\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command=\"SELECT DISTINCT name FROM sqlite_master WHERE type='table' AND name='__rgxlog__A_select0_project1'\"\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command='CREATE TABLE __rgxlog__A_select0_project1 (col0)'\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command='INSERT INTO __rgxlog__A_select0_project1 SELECT DISTINCT col0 FROM __rgxlog__A_select0'\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command='SELECT DISTINCT * FROM __rgxlog__A_select0_project1'\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command=\"SELECT DISTINCT name FROM sqlite_master WHERE type='table' AND name='__rgxlog__A_select0'\"\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command='DROP TABLE __rgxlog__A_select0'\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command=\"SELECT DISTINCT name FROM sqlite_master WHERE type='table' AND name='__rgxlog__A_select0_project1'\"\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine:sql command='DROP TABLE __rgxlog__A_select0_project1'\n",
      "DEBUG:root:initial lark tree:\n",
      "assignment\n",
      "  var_name\tb\n",
      "  string\t\"hi\"\n",
      "\n",
      "DEBUG:root:initial term graph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n",
      "DEBUG:root:lark tree after RemoveTokens:\n",
      "assignment\n",
      "  var_name\tb\n",
      "  string\thi\n",
      "\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.adding_inference_rules_to_term_graph:term graph after AddRulesToTermGraph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n",
      "DEBUG:root:initial lark tree:\n",
      "assignment\n",
      "  var_name\tb2\n",
      "  var_name\tb\n",
      "\n",
      "DEBUG:root:initial term graph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n",
      "DEBUG:root:lark tree after RemoveTokens:\n",
      "assignment\n",
      "  var_name\tb2\n",
      "  var_name\tb\n",
      "\n",
      "DEBUG:spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.adding_inference_rules_to_term_graph:term graph after AddRulesToTermGraph:\n",
      "(__rgxlog_root) (not_computed) root\n",
      "\n",
      "DependencyGraph is:\n",
      "__rgxlog_root\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'A(X)':\n",
      "   X\n",
      "-------\n",
      " Asmaa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\":\n",
    "    # this is for debugging. don't shadow variables like `query`, that's annoying\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level=logging.DEBUG)\n",
    "    my_session = Session()\n",
    "    my_session.run_commands(\"\"\"\n",
    "    b = \"hi\"\n",
    "    b2 = b\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
