{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewritting of a real code base using spannerlib\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "from IPython.display import display, HTML\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding The Covid-19 NLP Pipeline:\n",
    "\n",
    "The pipline repository [link](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/).\n",
    "\n",
    "The primary objective of the NLP pipeline is to identify individuals who have been positively diagnosed with COVID-19 by extracting pertinent information from unstructured free-text narratives found within the Electronic Health Record (EHR) of the Department of Veterans Affairs (VA). By automating this process, the pipeline streamlines the screening of a substantial volume of clinical text, significantly reducing the time and effort required for identification.\n",
    "The pipeline is built on medSpacy framework, and defines a new UI to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipline Stages:\n",
    "\n",
    "![pipeline_flowchart](img/pipeline_flowchart.png)\n",
    "\n",
    "\n",
    "\n",
    "At each stage, we encountered unique challenges. Each stage involved working with different classes and medSpacy framework attributes. Our approach was to first understand the original implementation, seek opportunities to simplify it, and then rewrite it in spannerlog. What was possible we wrote it declaratively using rules, facts, and queries. Additionally, if any essential functionalities were missing in the library, we built IE functions.\n",
    "As we mentioned the pipline uses medSpacy framework so first we need to import some libraries and install some requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation - step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up The Environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install some requirements to work with [medspacy](https://github.com/medspacy/medspacy) framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (2.7.2)\n",
      "Requirement already satisfied: jinja2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.2)\n",
      "Requirement already satisfied: jinja2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/dean/miniconda3/envs/span/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing and importing primitives from the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "! pip install spannerlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from spannerlib import get_magic_session,Session,Span\n",
    "magic_session = get_magic_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some generic ie functions that will be used in every stage of the pipline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(text_path):\n",
    "    \"\"\"\n",
    "    Reads from file and return it's content.\n",
    "\n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to read from.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file.\n",
    "    \"\"\"\n",
    "    with open(f\"{text_path}\", 'r') as file:\n",
    "        content = file.read()\n",
    "    yield content\n",
    "\n",
    "magic_session.register(\n",
    "    \"read_from_file\",\n",
    "    read_from_file,\n",
    "    [str],[str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive . \n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "for value in read_from_file(\"sample1.txt\"):\n",
    "    print(value)\n",
    "\n",
    "#TODO go to branch b4 merge and find original texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Print the contents of a CSV file in a human-readable format.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', newline='') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            print(','.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Age,Occupation\n",
      "John,25,Engineer\n",
      "Jane,30,Doctor\n",
      "Bob,28,Teacher\n",
      "Alice,22,Student\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "\n",
    "data_to_write = [\n",
    "    ['Name', 'Age', 'Occupation'],\n",
    "    ['John', '25', 'Engineer'],\n",
    "    ['Jane', '30', 'Doctor'],\n",
    "    ['Bob', '28', 'Teacher'],\n",
    "    ['Alice', '22', 'Student']\n",
    "]\n",
    "\n",
    "with open('example.csv', 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerows(data_to_write)\n",
    "\n",
    "print_csv_file('example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_containing_span(spans):\n",
    "    \"\"\"\n",
    "    This function takes a list of spans, where each span is represented\n",
    "    as a list containing a label and a span (interval). It resolves overlaps\n",
    "    by selecting the containing span, favoring the larger span in case of conflicts.\n",
    "\n",
    "    Parameters:\n",
    "    spans (list of lists): A list of spans, where each span is represented\n",
    "        as a list [label, span].\n",
    "\n",
    "    Returns:\n",
    "    list of lists: A list of resolved spans, where each span is a list\n",
    "        [label, span], with conflicts resolved by selecting the containing span.\n",
    "    \"\"\"\n",
    "    # Sort the replacements by the size of the spans in descending order\n",
    "    spans.sort(key=lambda x: x[1].end - x[1].start, reverse=True)\n",
    "\n",
    "    # Initialize a list to keep track of intervals that have been replaced\n",
    "    resolved_spans = []\n",
    "    \n",
    "    for label, span in spans:\n",
    "        conflict = False\n",
    "\n",
    "        for _, existing_span in resolved_spans:\n",
    "            existing_start = existing_span.start\n",
    "            existing_end = existing_span.end\n",
    "\n",
    "            if not (span.end <= existing_start or span.start >= existing_end):\n",
    "                conflict = True\n",
    "                break\n",
    "\n",
    "        if not conflict:\n",
    "            resolved_spans.append([label, span])\n",
    "\n",
    "    return resolved_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Label1, Span: 2-8\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "spans = [ \n",
    "     ['Label1', Span(2, 8)],\n",
    "     ['Label2', Span(5, 8)]\n",
    " ]\n",
    "resolved_spans = select_containing_span(spans)\n",
    "for label, span in resolved_spans:\n",
    "    print(f\"Label: {label}, Span: {span.start}-{span.end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_spans(spans_table, paths_table, session):\n",
    "    \"\"\"\n",
    "    This function takes tables a spans tables and path table for the files paths,\n",
    "    it generate queries for the tables, executes the queries using the given session, processes the results, \n",
    "    and replaces specific spans in a text with the corresponding labels, it first\n",
    "    resolve spans overlapping conflicts for each giving path.\n",
    "\n",
    "    Parameters:\n",
    "    spans_table (str): A string representing the spans table to process, table columns are formated as (Label, Span, Path).\n",
    "    paths_table (str): A string representing the paths table to process, table columns are formated as (Path)\n",
    "    session: the session in the spannerlog to run the queries at.\n",
    "\n",
    "    Returns:\n",
    "    str: The adjusted text string with the new labels.\n",
    "    \"\"\"\n",
    "    # Get a list of all the paths\n",
    "    paths = session.export(f\"?{paths_table}(Path)\")\n",
    "    paths = paths[0].values.tolist()\n",
    "    for path_list in paths:\n",
    "        path = path_list[0]\n",
    "\n",
    "        # Generate a spans query for each path, the query will be formates as (Label, Span, Path)\n",
    "        results = magic_session.export(f'?{spans_table}(Label, Span, \"{path}\")')\n",
    "        if len(results[0]) == 0:\n",
    "            continue\n",
    "        # replacments is list of lists where each list is a [Label, Span]\n",
    "        replacements = results[0].values.tolist()\n",
    "        \n",
    "        with open(f\"{path}\", 'r') as file:\n",
    "            adjusted_string = file.read()\n",
    "    \n",
    "        # Resolve spans conflicts\n",
    "        resolved_replacements = select_containing_span(replacements)\n",
    "    \n",
    "        # Sort the resolved replacements by the starting index of each span in descending order\n",
    "        resolved_replacements.sort(key=lambda x: x[1].span_start, reverse=True)\n",
    "    \n",
    "        # iterate over the resolved query results and replace the space with the corresponding label\n",
    "        for i in range(len(resolved_replacements)):\n",
    "            replace_string, span = resolved_replacements[i]\n",
    "            replace_length = len(replace_string)\n",
    "            adjusted_string = adjusted_string[:span.span_start] + replace_string + adjusted_string[span.span_end:]\n",
    "    \n",
    "        with open(f\"{path}\", 'w') as file:\n",
    "            file.writelines(adjusted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage example\n",
    "with open('example.txt', 'w') as file:\n",
    "    file.write('The boy has novel coronavirus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "new samplePaths(str)\n",
    "samplePaths(\"example.txt\")\n",
    "new sampleMatches(str, span, str)\n",
    "sampleMatches(\"Covid-19\", [12,29), \"example.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/span/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreplace_spans\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msampleMatches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msamplePaths\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmagic_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m read_from_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(value)\n",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m, in \u001b[0;36mreplace_spans\u001b[0;34m(spans_table, paths_table, session)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get a list of all the paths\u001b[39;00m\n\u001b[1;32m     17\u001b[0m paths \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaths_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(Path)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m paths \u001b[38;5;241m=\u001b[39m \u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_list \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[1;32m     20\u001b[0m     path \u001b[38;5;241m=\u001b[39m path_list[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/span/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/span/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "replace_spans('sampleMatches', 'samplePaths', magic_session)\n",
    "for value in read_from_file(\"example.txt\"):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_span_contained(span1, span2):\n",
    "    \"\"\"\n",
    "    Checks if one span is contained within the other span and returns the smaller span if yes.\n",
    "\n",
    "    Parameters:\n",
    "        span1 (span)\n",
    "        span2 (span)\n",
    "\n",
    "    Returns:\n",
    "        span: span1 if contained within span2 or vice versa, or None if not contained.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1.span_start, span1.span_end\n",
    "    start2, end2 = span2.span_start, span2.span_end\n",
    "    \n",
    "    if start2 <= start1 and end1 <= end2:\n",
    "        yield span1\n",
    "        \n",
    "    elif start1 <= start2 and end2 <= end1:\n",
    "        yield span2\n",
    "\n",
    "magic_session.register(is_span_contained, \"is_span_contained\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Span: 8-9\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "span1 = Span(2, 12)\n",
    "span2 = Span(8, 9)\n",
    "for span in is_span_contained(span1, span2):\n",
    "    print(f\" Span: {span.span_start}-{span.span_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_span(span1, span2):\n",
    "    \"\"\"\n",
    "    Computes the relative position of the conatined span within the other span.\n",
    "\n",
    "    Parameters:\n",
    "        span1 (Span): The first span object.\n",
    "        span2 (Span): The second span object.\n",
    "\n",
    "    Returns:\n",
    "        Span: The new relative span of the contained one.\n",
    "        None: If there's no span contained within the other.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1.span_start, span1.span_end\n",
    "    start2, end2 = span2.span_start, span2.span_end\n",
    "    \n",
    "    if start2 <= start1 and end1 <= end2:\n",
    "        yield Span(span1.span_start - span2.span_start, span1.span_end - span2.span_start)\n",
    "        \n",
    "    elif start1 <= start2 and end2 <= end1:\n",
    "        yield Span(span2.span_start - span1.span_start, span2.span_end - span1.span_start)\n",
    "\n",
    "magic_session.register(get_relative_span, \"get_relative_span\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Span: 0-3\n"
     ]
    }
   ],
   "source": [
    "# usage example \n",
    "span1 = Span(2, 12)\n",
    "span2 = Span(2, 5)\n",
    "for span in get_relative_span(span1, span2):\n",
    "    print(f\" Span: {span.span_start}-{span.span_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenization(text_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, processes its content using spaCy's English language model,\n",
    "    tokenizing it into sentences and returns each individual sentence in the processed text using a generator.\n",
    "    \n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be annotated.\n",
    "\n",
    "    Returns:\n",
    "        str: Individual sentences extracted from the input text.\n",
    "    \"\"\"\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        yield sentence.text\n",
    "\n",
    "magic_session.register(ie_function=sent_tokenization, ie_function_name = \"sent_tokenization\", in_rel=[DataTypes.string], out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient presents to be tested for COVID-19 .\n",
      "His family recently tested positive for COVID-19 .\n",
      "COVID-19 results came back positive .\n"
     ]
    }
   ],
   "source": [
    "# usage example \n",
    "for sentence in sent_tokenization(\"sample1.txt\"):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Input:\n",
    "\n",
    "The paths of the text files to be classified should be written in \"files_paths.csv\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1.txt\n",
      "sample2.txt\n",
      "sample3.txt\n",
      "sample4.txt\n",
      "sample5.txt\n",
      "sample6.txt\n",
      "sample7.txt\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('files_paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_rel(\"files_paths.csv\", relation_name=\"FilesPaths\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial files contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "FilesContent(Path, Content) <- FilesPaths(Path), read_from_file(Path) -> (Content)\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue we have to do some pre processing to help ease the next stages, we will certain words in each list to it's lemma forms, here a list of the words that we want to lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "man\n",
      "woman\n",
      "have\n",
      "do\n",
      "emergency\n",
      "epidemic\n",
      "outbreak\n",
      "crisis\n",
      "breakout\n",
      "pandemic\n",
      "spread\n",
      "confirm\n",
      "person\n",
      "patient\n",
      "veteran\n",
      "limit\n",
      "reduce\n",
      "factor\n",
      "contact\n",
      "case\n",
      "lower\n",
      "minimize\n",
      "risk\n",
      "chance\n",
      "possibility\n",
      "care\n",
      "clean\n",
      "desire\n",
      "flight\n",
      "trip\n",
      "plan\n",
      "reschedule\n",
      "postpone\n",
      "barrier\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('lemma_words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a helper method to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text_path, lemma_words_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, lemmatizes its content using spaCy's English language model,\n",
    "    and replaces certain words with their lemmas the rest will remain the same. The updated text is then written back to the same file.\n",
    "\n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be lemmatized.\n",
    "        lemma_words_path(str): The path that contains the list of words to be lemmatized\n",
    "\n",
    "    Returns:\n",
    "        str: The lemmatized text.\n",
    "    \"\"\"\n",
    "    # Define a list of words to be lemmatized\n",
    "    lemma_words = [line.strip() for line in open(f\"{lemma_words_path}\") if line.strip()]\n",
    "\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    lemmatized_text = \"\"\n",
    "    for token in doc:\n",
    "        if token.lemma_ in lemma_words:\n",
    "            lemmatized_text += token.lemma_\n",
    "        elif token.like_num:\n",
    "            lemmatized_text += \"like_num\"\n",
    "        else:\n",
    "            lemmatized_text += token.text\n",
    "        lemmatized_text += \" \"\n",
    "\n",
    "    # Write the lemmatized text back to the same file\n",
    "    with open(text_path, 'w') as file:\n",
    "        file.writelines(lemmatized_text)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy be sick \n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "with open('example.txt', 'w') as file:\n",
    "    file.write('The boy was sick')\n",
    "lemmatized_text = lemmatize_text('example.txt', 'lemma_words.txt')\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the texts to lemmatize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files_paths.csv', 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        path = row[0]\n",
    "        lemmatize_text(path, 'lemma_words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see for example in sample2.txt, was has changed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Concept Tagger](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/concept_tag_rules.py):\n",
    "\n",
    "Concept tag rules, also known as pattern-based rules or custom rules, are a way to specify and define patterns that an NLP (Natural Language Processing) system should recognize within text data. These rules are used to identify specific concepts or entities within text documents. In the context of MedSpaCy and medical NLP, concept tag rules are often used to identify medical entities and concepts accurately.\n",
    "\n",
    "In the orginal project they used the TargetRule class which defines a rule for identifying a specific concept or entity in text.\n",
    "each concept Target Rule looks like this:\n",
    "\n",
    "TargetRule(\n",
    "            literal=\"coronavirus\",\n",
    "            category=\"COVID-19\",\n",
    "            pattern=[{\"LOWER\": {\"REGEX\": \"coronavirus|hcov|ncov$\"}}],\n",
    "          )\n",
    "\n",
    "**Literal** : This specifies the literal text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the category or label associated with the identified entity.\n",
    "\n",
    "**Pattern** : This defines the pattern or conditions under which the entity should be recognized. It's a list of dictionaries specifying conditions for token matching. These rules some times used lemma attribute or POS of each token. A documentation can be found at : https://spacy.io/usage/rule-based-matching.\n",
    "\n",
    "Instead what we did is to define regex patterns, we have added these pattern in concept_target_rules.csv file, there are two types of these patterns lemma and pos, that we will implement each later on.\n",
    "Each rule in the csv file is like this : regexPattern, label, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:hcov|covid(?:(?:-)?(?:\\s)?19|10)?|2019-cov|cov2|ncov-19|covd 19|no-cov|sars cov),COVID-19,lemma\n",
      "(?i)(?:coivid|(?:novel )?corona(?:virus)?(?: (?:20)?19)?|sars(?:\\s)?(?:-)?(?:\\s)?cov(?:id)?(?:-)?(?:2|19)),COVID-19,lemma\n",
      "(?i)(?:\\+(?: ve)?|\\(\\+\\)|positive|\\bpos\\b|active|confirmed),positive,lemma\n",
      "(?i)(?:pneum(?:onia)?|pna|hypoxia|septic shoc|ards\\(?(?:(?:[12])/2)\\)?|(?:hypoxemic|acute|severe)? resp(?:iratory)? failure(?:\\(?(?:[12]/2)\\)?)?)\",associated_diagnosis,lemma\n",
      "(?i)(?:(?:diagnos(?:is|ed)|dx(?:\\.)?)(?:of|with)?),diagnosis,lemma\n",
      "(?i)(?:^screen),screening,lemma\n",
      "(?i)(?:in contact with|any one|co-worker|at work|(?:the|a)(?:wo)?man|(?:another|a) (?:pt|patient|pt\\.)),other_experiencer,lemma\n",
      "(?i)(?:patient|pt(?:\\.)?|vt|veteran),patient,lemma\n",
      "(?i)(?:like_num (?:days|day|weeks|week|months|month) (?:ago|prior)),timesx,lemma\n",
      "(?i)(?:(?:antibody|antibodies|ab) test),antibody test,lemma\n",
      "(?i)(?:(?:coronavirus|hcovs?|ncovs?|covs?)(?:\\s)?(?:-)?(?:\\s)?(?: infection)?(?: strain)?(?:\\s)?(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63) (?:coronavirus|hcovs?|ncovs?|covs?)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:non(?:\\s)?(?:-)?(?:\\s)?(?:novel|covid|ncovid|covid-19)(?: coronavirus)?|other coronavirus),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:wife|husband|spouse|family|member|girlfriend|boyfriend|mother|father|nephew|niece|grandparent|grandparents|granddaughter|relative|relatives|caregiver),family,pos\n",
      "(?i)(?:grandchild|grandson|cousin|grandmother|grandfather|parent|son|daughter|mom|dad|brother|sister|aunt|uncle|child|children|sibling|siblings),family,pos\n",
      "(?i)(?:someone|somebody|person|anyone|anybody|people|individual|individuals|teacher|anybody|employees|employer|customer|client|residents),other_experiencer,pos\n",
      "(?i)(?:resident|pts|patients|coworker|coworkers|workers|colleague|captain|captains|pilot|pilots|sailor|sailors|meeting),other_experiencer,pos\n",
      "(?i)(?:boyfriend|persons|person|church|convention|guest|party|attendee|conference|roommate|friend|friends|coach|player|neighbor|manager|boss),other_experiencer,pos\n",
      "(?i)(?:cashier|landlord|worked|works|^mate|nobody|mates|housemate|housemates|hotel|soldier|airport|tsa|lady|ladies|lobby|staffer|staffers),other_experiencer,pos\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('concept_tags_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_rel(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma Rules:\n",
    "\n",
    "Lemma rules are rules that used the attribute _lemma of each token in the NLP, we already lemmatized the texts, so now we can create a regex patterns for that.\n",
    "\n",
    "Example for a lemma rule from the original NLP:\n",
    "\n",
    "        TargetRule(\n",
    "            \"results positive\",\n",
    "            \"positive\",\n",
    "            pattern=[\n",
    "                {\"LOWER\": \"results\"},\n",
    "                {\"LEMMA\": \"be\", \"OP\": \"?\"},\n",
    "                {\"LOWER\": {\"IN\": [\"pos\", \"positive\"]}},\n",
    "            ],\n",
    "        ),\n",
    "We used the py_rgx_span to capture the patterns, and will use the spans later on in replace_spans that will replace each span with the correct label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO here we need to rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "LemmaMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"lemma\"), py_rgx_span(Content, Pattern) -> (Span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'LemmaMatches(Label, Span, \"sample1.txt\")':\n",
      "  Label   |    Span\n",
      "----------+------------\n",
      " COVID-19 |  [34, 42)\n",
      " COVID-19 |  [85, 93)\n",
      " COVID-19 | [96, 104)\n",
      " positive | [123, 131)\n",
      " positive |  [72, 80)\n",
      " patient  |   [0, 7)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample2.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " COVID-19 | [26, 34)\n",
      " positive | [48, 56)\n",
      " patient  | [4, 11)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'LemmaMatches(Label, Span, \"sample3.txt\")':\n",
      "   Label   |   Span\n",
      "-----------+----------\n",
      " COVID-19  | [58, 66)\n",
      " diagnosis | [37, 46)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample4.txt\")':\n",
      "  Label   |  Span\n",
      "----------+---------\n",
      " COVID-19 | [4, 12)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample5.txt\")':\n",
      "  Label   |  Span\n",
      "----------+---------\n",
      " COVID-19 | [9, 17)\n",
      " positive | [0, 8)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample6.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " COVID-19 | [26, 34)\n",
      " patient  | [4, 11)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace the matches with the correct label\n",
    "replace_spans(\"LemmaMatches\", \"FilesPaths\", magic_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After:\n",
    "As we can see for example in the sample1.txt, every other covid-19 name was changed to COVID-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Rules:\n",
    "As we mentioned above these rules used the POS attribute of each token, there were a small number of rules so we only used this to the tokens we needed.\n",
    "Example of the a rule from the original NLP:\n",
    "\n",
    "        TargetRule(\n",
    "            \"other experiencer\",\n",
    "            category=\"other_experiencer\",\n",
    "            pattern=[\n",
    "                {\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]},\n",
    "                    \"LOWER\": {\n",
    "                        \"IN\": [\n",
    "                            \"someone\",\n",
    "                            \"somebody\",\n",
    "                            \"person\",\n",
    "                            \"anyone\",\n",
    "                            \"anybody\",\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "\n",
    "The patterns we've defined will match words listed under \"IN\", We specifically capture words if their Part-of-Speech (POS) falls into one of the categories: [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]. To accomplish this, two functions are employed: the first function determines the POS of each token, and the second one, py_rgx_span, captures the predefined patterns. After matching words, We confirm the accurate POS tags of the matched words using spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text_with_pos(text_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, processes its content using spaCy's English language model,\n",
    "    and returns a tuple of (POS, Span) for each token if it's one of NOUN|PROPN|PRON|ADJ\n",
    "    otherwise an empty tuple will be returned\n",
    "    \n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be annotated.\n",
    "\n",
    "    Returns:\n",
    "        tuple(str, Span): The POS of the token and it's span\n",
    "    \"\"\"\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]:\n",
    "            yield token.pos_, Span(token.idx, token.idx + len(token.text))\n",
    "        else:\n",
    "            yield tuple()\n",
    "magic_session.register(ie_function=annotate_text_with_pos, ie_function_name = \"annotate_text_with_pos\", in_rel=[DataTypes.string], out_rel=[DataTypes.string, DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ, (0,4)\n",
      "NOUN, (5,8)\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "with open('example.txt', 'w') as file:\n",
    "    file.write('sick boy')\n",
    "\n",
    "for POS, span in annotate_text_with_pos('example.txt'):\n",
    "    print(f\"{POS}, ({span.span_start},{span.span_end})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'POSTable(POS, Span, Path)':\n",
      "  POS  |    Span    |    Path\n",
      "-------+------------+-------------\n",
      "  ADJ  |   [0, 7)   | sample1.txt\n",
      "  ADJ  | [123, 131) | sample1.txt\n",
      "  ADJ  |  [72, 80)  | sample1.txt\n",
      " NOUN  | [105, 112) | sample1.txt\n",
      " NOUN  |  [49, 55)  | sample1.txt\n",
      " NOUN  |  [8, 16)   | sample1.txt\n",
      " PRON  |  [45, 48)  | sample1.txt\n",
      " PROPN |  [34, 42)  | sample1.txt\n",
      " PROPN |  [85, 93)  | sample1.txt\n",
      " PROPN | [96, 104)  | sample1.txt\n",
      "  ADJ  |  [48, 56)  | sample2.txt\n",
      " NOUN  |  [37, 44)  | sample2.txt\n",
      " NOUN  |  [4, 11)   | sample2.txt\n",
      " PROPN |  [26, 34)  | sample2.txt\n",
      " NOUN  |  [0, 12)   | sample3.txt\n",
      " PROPN |  [15, 23)  | sample3.txt\n",
      " PROPN |  [47, 55)  | sample3.txt\n",
      " PROPN |  [58, 66)  | sample3.txt\n",
      " PROPN |  [67, 75)  | sample3.txt\n",
      " NOUN  |  [13, 22)  | sample4.txt\n",
      " PROPN |   [0, 3)   | sample4.txt\n",
      " PROPN |  [4, 12)   | sample4.txt\n",
      "  ADJ  |   [0, 8)   | sample5.txt\n",
      " NOUN  |  [18, 28)  | sample5.txt\n",
      " PROPN |  [9, 17)   | sample5.txt\n",
      " NOUN  |  [4, 11)   | sample6.txt\n",
      " PROPN |  [26, 34)  | sample6.txt\n",
      "  ADJ  |   [0, 8)   | sample7.txt\n",
      "  ADJ  |  [36, 43)  | sample7.txt\n",
      " NOUN  |  [21, 27)  | sample7.txt\n",
      " NOUN  |  [44, 54)  | sample7.txt\n",
      " NOUN  |  [59, 68)  | sample7.txt\n",
      " NOUN  |  [69, 80)  | sample7.txt\n",
      " NOUN  |  [9, 20)   | sample7.txt\n",
      "\n",
      "printing results for query 'POSMatches(Label, Span, Path)':\n",
      "  Label  |   Span   |    Path\n",
      "---------+----------+-------------\n",
      " family  | [49, 55) | sample1.txt\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, Path)':\n",
      "  Label  |   Span   |    Path\n",
      "---------+----------+-------------\n",
      " family  | [49, 55) | sample1.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "POSTable(POS, Span, Path) <- FilesContent(Path, Content), annotate_text_with_pos(Path) -> (POS, Span)\n",
    "?POSTable(POS, Span, Path)\n",
    "\n",
    "POSMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"pos\"), py_rgx_span(Content, Pattern) -> (Span)\n",
    "?POSMatches(Label, Span, Path)\n",
    "\n",
    "POSRuleMatches(Label, Span, Path) <- POSTable(POS, Span, Path), POSMatches(Label, Span, Path)\n",
    "?POSRuleMatches(Label, Span, Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'POSRuleMatches(Label, Span, \"sample1.txt\")':\n",
      "  Label  |   Span\n",
      "---------+----------\n",
      " family  | [49, 55)\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample3.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample6.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace the matches with the correct label\n",
    "replace_spans(\"POSRuleMatches\", \"FilesPaths\", magic_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After:\n",
    "As we can see for example in sample1.txt, wife has changed to family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Target Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/target_rules.py):\n",
    "\n",
    "These rules used the label that was assigned through the concept tagger, to capture some more complex patterns and assign a label for them inorder to decremnt the cases of false positive.\n",
    "Each rule look like this:\n",
    "\n",
    "        TargetRule(\n",
    "            literal=\"coronavirus screening\",\n",
    "            category=\"IGNORE\",\n",
    "            pattern=[\n",
    "                {\"_\": {\"concept_tag\": \"COVID-19\"}},\n",
    "                {\"LOWER\": {\"IN\": [\"screen\", \"screening\", \"screenings\"]}},\n",
    "            ],\n",
    "        ),\n",
    "Since we replaced the spans we found with the corresponding label we didn't need the concept_tag attribute of the token/span.\n",
    "\n",
    "**Literal** : This specifies the literal text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the category or label associated with the identified entity.\n",
    "\n",
    "**Pattern** : This defines the pattern or conditions under which the entity should be recognized. It's a list of dictionaries specifying conditions for token matching. These rules some times used lemma attribute or POS of each token. A documentation can be found at : https://spacy.io/usage/rule-based-matching.\n",
    "\n",
    "Similar to the concept tag apporach we defined regex patterns, we have added these pattern in target_rules.csv file\n",
    "Each rule in the csv file is like this : regexPattern, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:COVID-19 positive (?:unit|floor)|positive COVID-19 (?:unit|floor|exposure)),COVID-19\n",
      "(?i)(?:known(?: positive)? COVID-19(?: positive)? (?:exposure|contact)),COVID-19\n",
      "(?i)(?:COVID-19 positive screening|positive COVID-19 screening|screening COVID-19 positive|screening positive COVID-19),positive coronavirus screening\n",
      "(?i)(?:diagnosis : COVID-19 (?:test|screening)),COVID-19\n",
      "(?i)(?:COVID-19 screening),coronavirus screening\n",
      "(?i)(?:active COVID-19 precaution|droplet isolation precaution|positive for (?:flu|influenza)|(?:the|a) positive case|results are confirm),1 2 3\n",
      "(?i)(?:exposed to positive|[ ] COVID-19|age like_num(?: )?\\+|(?:return|back) to work|COVID-19 infection rate),1 2 3\n",
      "(?i)(?:COVID-19 (?:restriction|emergency|epidemic|outbreak|crisis|breakout|pandemic|spread|screening)|droplet precaution),1 2\n",
      "(?i)(?:contact precautions|positive (?:flu|influenza)|positive (?:patient|person)|confirm (?:with|w/(?:/)?|w)|(?:the|positive) case),1 2\n",
      "(?i)(?:results confirm|(?:neg|pos)\\S+ pressure|positive (?:attitude|feedback|serology)|COVID-19 (guidelines|rate)),1 2\n",
      "(?i)(?:has the patient been diagnosed (?:with|w/(?:/)?|w)),1 2 3 4 5 6\n",
      "(?i)(?:has patient been diagnosed (?:with|w/(?:/)?|w)),1 2 3 4 5\n",
      "(?i)((?:person|patient) with confirm COVID-19),1 2 3 4\n",
      "(?i)(?:COVID-19 positive (?:tested )?other_experiencer),COVID-19\n",
      "(?i)(?:in order to decrease the spread of the COVID-19 infection),1 2 3 4 5 6 7 8 9 10\n",
      "(?i)(?:COVID-19 positive (?:patient|person|people|veteran)),OTHER_PERSON\n",
      "(?i)(?:positive COVID-19 (?:tested )?other_experiencer),COVID-19\n",
      "(?i)(?:(?:(?:contact|exposure) (?:with|to)? )?positive COVID-19 (?:patient|person|veteran)),OTHER_PERSON\n",
      "(?i)(?:(?:patient|person) (?:who|that) test (?:positive|confirm) for COVID-19),OTHER_PERSON\n",
      "(?i)(ref : not detected|history of present illness|does not know|but|therefore|flu|metapneumovirus|;),<IGNORE>\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('target_rules.csv')\n",
    "# TODO so here we are rewriting the tag if we find the regex, and to do ignore, some of the rules rewrite the words into letters to make them ignored in the next part\n",
    "# TODO try to see if i can do it with a span that says ignore, and then dont derive facts that are sub spans of ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_rel(\"target_rules.csv\", relation_name=\"TargetTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "TargetTagMatches(Label, Span, Path) <- FilesContent(Path, Content), TargetTagRules(Pattern, Label), py_rgx_span(Content, Pattern) -> (Span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'TargetTagMatches(Label, Span, \"sample1.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample3.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample6.txt\")':\n",
      "[]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'TargetTagMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_spans(\"TargetTagMatches\", \"FilesPaths\", magic_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After:\n",
    "As we can see in sample6.txt, the covid positive exposure has changed to covid, in order to not give false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Section Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/section_rules.py):\n",
    "\n",
    "Here, we'll add a section detection component that defines rules for detecting sections titles, which usually appear before a semicolon.\n",
    "Section rules are utilized to identify specific section names, enabling the separation of text into different parts. Entities occurring in certain sections are considered positive.\n",
    "\n",
    "In the original project, the SectionRule class was used to define rules for identifying specific section text. Each SectionRule has the following structure\n",
    "\n",
    "      SectionRule(category=\"problem_list\", literal=\"Active Problem List:\"),\n",
    "      SectionRule(category=\"problem_list\", literal=\"Current Problems:\"),\n",
    "    \n",
    "    \n",
    "**Literal** : This specifies the literal section text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the section category associated with the identified section.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Similar to the approach used in the concept tagger stage, regex patterns were derived from these literals, and these patterns are stored in the 'section_target_rules.csv' file and are used to match section texts and replace them with their appropriate category.\n",
    "\n",
    "Each rule in the CSV file follows this format: regexPattern, sectionLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:Lab results :),labs :\n",
      "(?i)(?:Addendum :),addendum :\n",
      "(?i)(?:(?:ALLERGIC REACTIONS|ALLERGIES) :),allergies :\n",
      "(?i)(?:(?:CC|Chief Complaint) :),chief_complaint :\n",
      "(?i)(?:COMMENTS :),comments :\n",
      "(?i)(?:(?:(?:ADMISSION )?DIAGNOSES|Diagnosis|Primary Diagnosis|Primary|Secondary(?: (?:Diagnoses|Diagnosis))) :),diagnoses :\n",
      "(?i)(?:(?:Brief Hospital Course|CONCISE SUMMARY OF HOSPITAL COURSE BY ISSUE/SYSTEM|HOSPITAL COURSE|SUMMARY OF HOSPITAL COURSE) :),hospital_course :\n",
      "(?i)(?:(?:Imaging|MRI|INTERPRETATION|Radiology) :),imaging :\n",
      "(?i)(?:(?:ADMISSION LABS|Discharge Labs|ECHO|Findings|INDICATION|Labs|Micro|Microbiology|Studies|Pertinent Results) :),labs_and_studies :\n",
      "(?i)(?:(?:ACTIVE MEDICATIONS(?: LIST)|ADMISSION MEDICATIONS|CURRENT MEDICATIONS|DISCHARGE MEDICATIONS|HOME MEDICATIONS|MEDICATIONS) :),medications :\n",
      "(?i)(?:(?:MEDICATIONS AT HOME|MEDICATIONS LIST|MEDICATIONS ON ADMISSION|MEDICATIONS ON DISCHARGE|MEDICATIONS ON TRANSFER|MEDICATIONS PRIOR TO ADMISSION) :),medications :\n",
      "(?i)(?:Neuro :),neurological :\n",
      "(?i)(?:(?:A/P|MEDICATIONS LIST|ASSESSMENT/PLAN|ASSESSMENT|Clinical Impression|DISCHARGE DIAGNOSES|DISCHARGE DIAGNOSIS) :),observation_and_plan :\n",
      "(?i)(?:(?:Discharge Condition|Discharge Disposition|FINAL DIAGNOSES|FINAL DIAGNOSIS|IMPRESSION|Impression and Plan|Impression and Recommendation) :),observation_and_plan :\n",
      "(?i)(?:(?:Facility|Service) :),other :\n",
      "(?i)(?:(?:Current Medical Problems|History of Chronic Illness|MHx|PAST HISTORY|PAST MEDICAL Hx|PAST SURGICAL HISTORY|PMH|PMHx|PAST MEDICAL HISTORY|UNDERLYING MEDICAL CONDITION) :),past_medical_history :\n",
      "(?i)(?:(?:Education|Patient Education|DISCHARGE INSTRUCTIONS/FOLLOWUP|DISCHARGE INSTRUCTIONS|Followup Instructions) :),patient_education :\n",
      "(?i)(?:(?:PE|PHYSICAL EXAM|PHYSICAL EXAMINATION) :),physical_exam :\n",
      "(?i)(?:(?:Active Problem List|Current Problems|Medical Problems|PROBLEM LIST) :),problem_list :\n",
      "(?i)(?:REASON FOR THIS EXAMINATION :),reason_for_examination :\n",
      "(?i)(?:(?:Electronic Signature|Signed electronically by) :),signature :\n",
      "(?i)(?:(?:PMHSx|PSH|SH|Sexual History:|Social History) :),social_history :\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('section_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_rel(\"section_rules.csv\", relation_name=\"SectionRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionRulesMatches(Label, Span, Path)':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "SectionRulesMatches(Label, Span, Path) <- FilesContent(Path, Content), SectionRules(Pattern, Label), py_rgx_span(Content, Pattern) -> (Span)\n",
    "?SectionRulesMatches(Label, Span, Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample1.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample3.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample6.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_spans(\"SectionRulesMatches\", \"FilesPaths\", magic_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After:\n",
    "As we can see in sample 3, current problems has changed to problems_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Assertion:\n",
    "\n",
    " Next, we will explore how to assert attributes indicating whether a mention of COVID-19 is positive or not. In our project, we have created a table     named 'CovidAttributes' that contains all attributes for each COVID-19 mention. This table will be used for classifying documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionMatches(Path, Span, CovidAttribute)':\n",
      "    Path     |  Span   |  CovidAttribute\n",
      "-------------+---------+------------------\n",
      " sample3.txt | [0, 76) |     positive\n",
      "\n",
      "printing results for query 'CovidMatches(Path, Span)':\n",
      "    Path     |   Span\n",
      "-------------+-----------\n",
      " sample1.txt | [34, 42)\n",
      " sample1.txt | [85, 93)\n",
      " sample1.txt | [96, 104)\n",
      " sample2.txt | [26, 34)\n",
      " sample3.txt | [58, 66)\n",
      " sample4.txt |  [4, 12)\n",
      " sample5.txt |  [9, 17)\n",
      " sample6.txt | [26, 34)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "#Here, we employ a pattern to identify entities present in specific sections and mark them as positive,\n",
    "#and adding them to the 'CovidAttributes' table.\n",
    "\n",
    "pattern = \"(?i)(?:diagnoses :|observation_and_plan :|past_medical_history :|problem_list :)(?:(?!labs :|addendum :|allergies :|chief_complaint :|comments :|family_history :|hospital_course :|imaging :|labs_and_studies :|medications :|neurological :|other :|patient_education :|physical_exam :|reason_for_examination :|signature :|social_history :).)*\"\n",
    "new SectionRulesAttribute(str, str)\n",
    "SectionRulesAttribute(pattern, \"positive\")\n",
    "\n",
    "SectionMatches(Path, Span, CovidAttribute) <- FilesContent(Path, Content), SectionRulesAttribute(Pattern, CovidAttribute), py_rgx_span(Content, Pattern) -> (Span)\n",
    "?SectionMatches(Path, Span, CovidAttribute)\n",
    "\n",
    "CovidMatches(Path, Span) <- FilesContent(Path, Content), py_rgx_span(Content, \"COVID-19\") -> (Span)\n",
    "?CovidMatches(Path, Span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionCovidAttributes(Path, CovidSpan, CovidAttribute)':\n",
      "    Path     |  CovidSpan  |  CovidAttribute\n",
      "-------------+-------------+------------------\n",
      " sample3.txt |  [58, 66)   |     positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "SectionCovidAttributes(Path, CovidSpan, CovidAttribute) <- SectionMatches(Path, Span1, CovidAttribute), CovidMatches(Path, Span2), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?SectionCovidAttributes(Path, CovidSpan, CovidAttribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text into Sentences:\n",
    "\n",
    "In the subsequent stages, where attributes are assigned to COVID-19 mentions, a departure from the previous stages occurs. Here, patterns are no longer applied to the entire text, instead, they are applied at the sentence level, since the attributes of COVID-19 mentions are typically determined by the context of the sentence in which they appear. This means the text is processed and tokenized into sentences using spaCy's English language model. This process is accomplished through the use of  ie functions and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'Sents(Path, Sent)':\n",
      "    Path     |                                        Sent\n",
      "-------------+------------------------------------------------------------------------------------\n",
      " sample1.txt |                       COVID-19 results came back positive .\n",
      " sample1.txt |                 His family recently tested positive for COVID-19 .\n",
      " sample1.txt |                    patient presents to be tested for COVID-19 .\n",
      " sample2.txt |                               Results be positive .\n",
      " sample2.txt |                        The patient be tested for COVID-19 .\n",
      " sample3.txt |                                 COVID-19 like_num\n",
      " sample3.txt |                                associated_diagnosis\n",
      " sample3.txt |                                     like_num .\n",
      " sample3.txt |                             problem_list : like_num .\n",
      " sample4.txt |                              neg COVID-19 education .\n",
      " sample5.txt |                           positive COVID-19 precaution .\n",
      " sample6.txt |                        The patient have reported COVID-19 .\n",
      " sample7.txt | Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n",
      "printing results for query 'SentSpans(Path, Sent, SentSpan)':\n",
      "    Path     |                                        Sent                                        |  SentSpan\n",
      "-------------+------------------------------------------------------------------------------------+------------\n",
      " sample1.txt |                       COVID-19 results came back positive .                        | [96, 133)\n",
      " sample1.txt |                 His family recently tested positive for COVID-19 .                 |  [45, 95)\n",
      " sample1.txt |                    patient presents to be tested for COVID-19 .                    |  [0, 44)\n",
      " sample2.txt |                               Results be positive .                                |  [37, 58)\n",
      " sample2.txt |                        The patient be tested for COVID-19 .                        |  [0, 36)\n",
      " sample3.txt |                                 COVID-19 like_num                                  |  [58, 75)\n",
      " sample3.txt |                                associated_diagnosis                                |  [26, 46)\n",
      " sample3.txt |                                     like_num .                                     |  [15, 25)\n",
      " sample3.txt |                                     like_num .                                     |  [47, 57)\n",
      " sample3.txt |                             problem_list : like_num .                              |  [0, 25)\n",
      " sample4.txt |                              neg COVID-19 education .                              |  [0, 24)\n",
      " sample5.txt |                           positive COVID-19 precaution .                           |  [0, 30)\n",
      " sample6.txt |                        The patient have reported COVID-19 .                        |  [0, 36)\n",
      " sample7.txt | Elevated cholesterol levels require further assessment and lifestyle adjustments . |  [0, 82)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "#Sentences of the text\n",
    "Sents(Path, Sent) <- FilesPaths(Path), sent_tokenization(Path) -> (Sent)\n",
    "?Sents(Path, Sent)\n",
    "\n",
    "#SentSpan is the span of the sentence in the text\n",
    "SentSpans(Path, Sent, SentSpan) <- FilesContent(Path, Content), Sents(Path, Sent), py_rgx_span(Content, Sent) -> (SentSpan)\n",
    "?SentSpans(Path, Sent, SentSpan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |  CovidAttribute  |       Sent\n",
      "-------------+-------------+------------------+-------------------\n",
      " sample3.txt |   [0, 8)    |     positive     | COVID-19 like_num\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- SectionCovidAttributes(Path, AbsCovidSpan, CovidAttribute),\\\n",
    "SentSpans(Path, Sent, SentSpan) ,get_relative_span(AbsCovidSpan, SentSpan) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Context Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/context_rules.py):\n",
    "\n",
    "These rules assign an attribute for each COVID-19 label based on the context, these attributes will be used later to classify each text.\n",
    "\n",
    "Example for this rule is: \n",
    "\n",
    "    ConTextRule(\n",
    "        literal=\"Not Detected\",\n",
    "        category=\"NEGATED_EXISTENCE\",\n",
    "        direction=\"BACKWARD\",\n",
    "        pattern=[\n",
    "            {\"LOWER\": {\"IN\": [\"not\", \"non\"]}},\n",
    "            {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "            {\"TEXT\": \"-\", \"OP\": \"?\"},\n",
    "            {\"LOWER\": {\"REGEX\": \"detecte?d\"}},\n",
    "        ],\n",
    "        allowed_types={\"COVID-19\"},\n",
    "    ),\n",
    "   **direction** specify if the allowed_types should be before or after the pattern,\n",
    "   **allowed_types** specify on what labels should this rule be applied on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:positive COVID-19|COVID-19 (?:\\([^)]*\\)) (?:positive|detected)|COVID-19(?: positive)? associated_diagnosis)#positive\n",
      "(?i)(?:COVID-19 status : positive)#positive\n",
      "(?i)(?:associated_diagnosis COVID-19|associated_diagnosis (?:with|w|w//|from) (?:associated_diagnosis )?COVID-19)#positive\n",
      "(?i)(?:COVID-19 positive(?: patient| precaution)?|associated_diagnosis (?:due|secondary) to COVID-19)#positive\n",
      "(?i)(?:(?:current|recent) COVID-19 diagnosis)#positive\n",
      "(?i)(?:COVID-19 (?:- )?related (?:admission|associated_diagnosis)|admitted (?:due to|(?:with|w|w/)) COVID-19)#positive\n",
      "(?i)(?:COVID-19 infection|b34(?:\\.)?2|b97.29|u07.1)#positive\n",
      "(?i)(?:COVID-19 eval(?:uation)?|(?:positive )? COVID-19 symptoms|rule out COVID-19)#uncertain\n",
      "(?i)(?:patient (?:do )?have COVID-19)#positive\n",
      "(?i)(?:diagnosis : COVID-19(?: (?:test|screen)(?:ing|ed|s)? positive)?(?: positive)?)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:not|non) (?:- )?detecte?d)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} negative screening|negative screening(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} : negative)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:not be|none) detected)#negated\n",
      "(?i)(?:free from(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? not (?:be )?tested)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} not indicated)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? NEGATIVE NEG)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} negative test)#negated\n",
      "(?i)(?:negative test(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:without any(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:denie(?:s|d)(?: any| travel)?(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#negated\n",
      "(?i)(?:no (?:evidence(?: of)?|(?:hx|-hx|history) of|diagnosis (?:of)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:no(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:no (?:positive|one|residents|confirm case|contact(?: w/?(?:ith)?$))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? no confirm case)#negated\n",
      "(?i)(?:(?:no|n't) (?:be )? confirm(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:(?:no known|not have)(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:answer(?:ed|s|ing)? (?:no|negative|neg)|negative))#negated\n",
      "(?i)(?:(?:answer(?:ed|s|ing)? (?:no|negative|neg)|(?:neg|negative)(?: for)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:not positive(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? not positive)#negated\n",
      "(?i)(?:excluded(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} excluded)#negated\n",
      "(?i)(?:no risk factor for(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#uncertain\n",
      "(?i)(?:negative screening(?: for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? screening (?:negative|neg))#negated\n",
      "(?i)(?:(?:screening (?:negative|neg) for|do (?:not|n't) have (?:any )?(?:signs|symptoms|ss|s/s))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? do not screening positive)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:be negative|not test positive))#negated\n",
      "(?i)(?:(?:be negative|not test positive|not? screening(?: for)|no signs of|no (?:sign|symptom|indication(?:of|for)?)|not? test(?:\\S+)? for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:no exposure|(?:without|w/o) (?:signs|symptoms)(?:or (?:signs|symptoms))|do)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} not have)#negated\n",
      "(?i)(?:(?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria|no concern (?:for|of)|not? (?:at )risk)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria))#negated\n",
      "(?i)(?:(?:no suspicion(?: for)|not suspect|ruled out for|no(?: recent) travel|not be in|clear(?:ed|s|ing) (?:of|for|from))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:not(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:be ruled out|be not likely|not have contact with))#negated\n",
      "(?i)(?:(?:no (?:hx|history) (?:of )travel|not have contact with|no symptoms of|no risk factors|no (?:confirm case|report))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:no (?:exposure|contact) (?:to|with)|not test(?:\\S+)? positive))#negated\n",
      "(?i)(?:(?:no (?:exposure|contact) (?:to|with)|do (?:not|n't) meet(?: screening)(?: criteria)(?: for)|not test(?:\\S+)? positive(?: for)|not tested(?: or diagnosis))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:no|any)(?: known) contact(?: with)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} : no)#negated\n",
      "(?i)(?:(?:(?:not|never) diagnosis with|not been tested (?:for )?or diagnosis with)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} confirm)#positive\n",
      "(?i)(?:(?:confirm|known)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#positive\n",
      "(?i)(?:(?:(?:test(?:\\S+)?)?positive(?: for)?|notif(?:y|ied) of positive (?:results?|test(?:\\S+)?|status))(?: (?!<IGNORE>)\\S+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positiv(?:e|ity)|test(?:\\S+)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positiv(?:e|ity)|test(?:\\S+)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} (?:positive status|results be positive))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} results positive)#positive\n",
      "(?i)(?:results positive(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#positive\n",
      "(?i)(?:notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status)(?: (?!<IGNORE>)\\S+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status))#positive\n",
      "(?i)(?:likely secondary to(?: (?!<IGNORE>)\\S+){0,0} COVID-19)#positive\n",
      "(?i)(?:(?:problem(?: list)? (?:of|:)|(?:active|current|acute) problems :|admi(?:t|ssion) diagnosis(?: :)?)(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#positive\n",
      "(?i)(?:(?:reason for admission :|treatment of|(?:admitting )diagnosis(?: :)?)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} diagnosis like_num)#positive\n",
      "(?i)(?:(?:Reason for admission :|inpatient with|discharged from|in m?icu (?:for|with))(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#admission\n",
      "(?i)(?:(?:admit(?:ted|s|ting) (?:like_num|with|for)|admitted (?:to|on)|Reason for ICU :|admission for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#admission\n",
      "(?i)(?:Reason for ED visit or Hospital Admission :(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#admission\n",
      "(?i)(?:(?:(?:in|to) (?:the )(?:hospital|icu|micu) (?:for|due to)|hospitali(?:zed)?(?: timesx)? (?:for|due to))(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#admission\n",
      "(?i)(?:(?:diagnosis with|found to be positive for)(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,5} found to be positive)#positive\n",
      "(?i)(?:(?:positive test|presum(?:e|ed|es|ing) positive|not(?: yet)? recover(?:s|ing|ed)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positive test|presum(?:e|ed|es|ing) positive))#positive\n",
      "(?i)(?:(?:management of|ards(?: (?:from|with|secondary to))?|acute respiratory distress|post - extubation)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#positive\n",
      "(?i)(?:(?:in(?: the)? setting of|in the s / o|found to have|present(?:s|ed|ing)? with)(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#positive\n",
      "(?i)(?:resp(?:iratory) failure(?:(?: (?:with|due to))?|like_num|\\( like_num \\))(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#positive\n",
      "(?i)(?:(?:active(?: for)|recovering from)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} recovering from)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} (?:detected|value : detected|POSITIVEH))#positive\n",
      "(?i)(?:(?:\\d+(?: )?-|like_num )year(?:(?: )?-(?: )?old| old) (?:(?:aa|white|black|hispanic|caucasian) )?(?:\\b(?!family\\b|other_experiencer\\b)\\S+\\b )?(?:with|w|w/|admitted)(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#patient_experiencer\n",
      "(?i)(?:(?:like_num (?:y[or]|y / o)|[\\d]+yo) (?:\\b(?!family\\b|other_experiencer\\b)\\S+\\b )?(?:patient |veteran )?(?:with|w|w/)(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#patient_experiencer\n",
      "(?i)(?:the (?:veteran|vet|patient) have(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#patient_experiencer\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} precaution)#future\n",
      "(?i)(?:(?:(?:precaution|protection|protect) (?:for|against)|concern about|reports of|vaccine|protect yourself|prevent(?:ed|ion|s|ing)|avoid)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:prevent(?:ed|ion|s|ing)|vaccine|educat\\S*|instruction))#future\n",
      "(?i)(?:(?:questions (?:about|regarding|re|concerning|on|for)|(?:anxiety|ask(?:ing|ed|es|ed)?) about|educat(?:ion|ed|ing|ed)?|instruction)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:(?:information(?: )?(?:on|about|regarding|re)?|protocols?)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} protocols?)#future\n",
      "(?i)(?:(?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?|(?:anxious|worr(?:ied|ies|y|ying)) (?:about|re|regarding))(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?))#future\n",
      "(?i)(?:if(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#future\n",
      "(?i)(?:(?:advisor(?:y|ies)|travel screen(?: :)?|Travel History Questionnaire|prescreen|front gate)(?: (?!<IGNORE>)\\S+)*? COVID-19)#screening\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} (?:questionnaire :|questionn?aire|question\\S*|prescreen|front gate))#screening\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,9} screen\\S*)#screening\n",
      "(?i)(?:screen\\S*(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#screening\n",
      "(?i)(?:have you(?: (?!<IGNORE>)\\S+)*? COVID-19)#not relevant\n",
      "(?i)(?:(?:mers)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:This patient was screened for the following suspected travel related illness(?:es)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? This patient was screened for the following suspected travel related illness(?:es)?)#future\n",
      "(?i)(?:(?:will(?: be) travel|travel plans|if you need|plan to travel)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:(?:(?:limit|reduce|lower|minimize)(?: the)? (?:risk|chance|possibility) of|if you)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:(?:(?:(?:-)?hx|history|) of)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:(?:^(?:check|test|retest|eval)(?: for)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#test\n",
      "(?i)(?:(?:work(?:-|\\s)up)(?: (?!<IGNORE>)\\S+)*? COVID-19)#test\n",
      "(?i)(?:(?:evaluation)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#test\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} (?:evaluation))#test\n",
      "(?i)(?:(?:swab|PCR|specimen sent)(?: (?!<IGNORE>)\\S+)*? COVID-19)#test\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:swab|PCR|specimen sent))#test\n",
      "(?i)(?:(?:awaiting results|at risk for|risk for|currently being ruled out or has tested positive for|to exclude)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:awaiting results|currently being ruled out or has tested positive for|(?:patient|person) of interest))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,0} (?:risk))#uncertain\n",
      "(?i)(?:(?:investigation of)(?: (?!<IGNORE>)\\S+){0,0} COVID-19)#uncertain\n",
      "(?i)(?:(?:question of|differential diagnosis :|ddx :)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:awaiting|questionnaire|r(?:/)?o(?:\\.)?)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} (?:awaiting|questionnaire|r(?:/)?o(?:\\.)?))#uncertain\n",
      "(?i)(?:(?:under investigation|(?:may|might) be positive(?: for)?|flew|tarvel(?:ed)?|travelled)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:under investigation|(?:may|might) be positive))#uncertain\n",
      "(?i)(?:(?:facility (?:with|has)(?: a)?|known to have|(?:same )?room|patients with)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:area|county|community|city) (?:with|of)|in the building|(?:several|multiple|one)(?:of )?(?:the )? other_experiencer)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:in the building))#negated\n",
      "(?i)(?:(?:(?:he|she) thinks (?:he|she) (?:have|had|has)|\\S+ would like)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|unlikely to be)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|possible positive))#uncertain\n",
      "(?i)(?:(?:(?:possible|potential)? exposure|possibly|possible positive)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:(?:risk of|likely|probable|probably)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:suspicion(?: for)?|^suspect|differential diagnosis|ddx(?: :)?|doubt)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:suspicion|^suspect|differential diagnosis|ddx(?: :)?|may have been exposed))#uncertain\n",
      "(?i)(?:(?:(?:positive )?(?:sign|symptom) of)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:sx|s/s|rule (?:- )out|be ruled out(?: for)?|^(?:vs\\.?|versus)$)(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} (?:sx|s/s|rule (?:- )out|^(?:vs\\.?|versus)$))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:(?:possible|potential)? exposure|may have been exposed))#uncertain\n",
      "(?i)(?:(?:concern(:?s)?(?: for| of)?|if (?:negative|positive)|c/f|assess(?:ed)? for|concerning for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:(?:unlikely(?: to be positive)?|low (?:suspicion|probability|risk (?:for|in|of)))(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:unlikely(?: to be positive)?|low (?:suspicion|probability)|is unlikely))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} (?:extremely low))#uncertain\n",
      "(?i)(?:(?:low risk of)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#uncertain\n",
      "(?i)(?:(?:(?:other_experiencer|family) ^test positive(?: for)?|any one|contact with(?: known))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:other_experiencer|family)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:other_experiencer|any one|contact with(?: known)))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,0} (?:(?:a|an|another) \\S+ tested positive))#negated\n",
      "(?i)(?:(?:had contact|same (?:building|floor)|care for|clean)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:had contact|same (?:building|floor)|care for|clean))#negated\n",
      "(?i)(?:(?:concern(?:ed)? about)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#negated\n",
      "(?i)(?:(?:patient concern (?:for|of)|desire|(?:concerned|prepare) (?:for|about))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:seen in|a (?:positive|confirmed) case of|cases|epidemic|pandemic)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} (?:cases|epidemic|pandemic|national emergency|crisis|situation|mandate|\\?))#negated\n",
      "(?i)(?:(?:national emergency|crisis|situation|mandate)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:(?:seen in(?: the)? setting of)(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#negated\n",
      "(?i)(?:(?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off|goals :)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:in the (?:area|community)|outbreak))#negated\n",
      "(?i)(?:(?:in the (?:area|community)|outbreak)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation))#negated\n",
      "(?i)(?:(?:^read about|deploy|(?:come|been) in close contact(?: with)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:^read about|deploy|(?:come|been) in close contact(?: with)?|error))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:have you had close contact|web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases))#negated\n",
      "(?i)(?:(?:have you had close contact|the group|session|(?:nurse(?:s)?|rn) notes)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases|error)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:person|patients) with(?: confirmed)?(?: or)?(?: suspected)?|cases of)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#negated\n",
      "(?i)(?:elective(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} elective)#negated\n",
      "(?i)(?:(?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?)))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:(?:someone|person) who (?:has|have) tested positive|contact with))#negated\n",
      "(?i)(?:(?:(?:someone|person) who (?:has|have) tested positive|contact with)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,0} (?:\\(resolved\\)))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:social worker|initially negative|likely recovered|not aware|positive (?:case|symptom|sign)|client history|emergency contact|several positive|special instructions :))#IGNORE\n",
      "(?i)(?:(?:social worker|initially negative|likely recovered|not aware|positive (?:case|symptom|sign)|client history|emergency contact|several positive|special instructions :)(?: (?!<IGNORE>)\\S+)*? COVID-19)#IGNORE\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('context_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_rel(\"context_rules.csv\", relation_name=\"ContextRules\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'ContextMatches(CovidAttribute, Span, Path, Sent)':\n",
      "   CovidAttribute    |   Span   |    Path     |                        Sent\n",
      "---------------------+----------+-------------+----------------------------------------------------\n",
      "      positive       | [0, 17)  | sample5.txt |           positive COVID-19 precaution .\n",
      "       negated       | [0, 12)  | sample4.txt |              neg COVID-19 education .\n",
      "      positive       | [27, 48) | sample1.txt | His family recently tested positive for COVID-19 .\n",
      "      positive       | [0, 35)  | sample1.txt |       COVID-19 results came back positive .\n",
      " patient_experiencer | [0, 34)  | sample6.txt |        The patient have reported COVID-19 .\n",
      "       future        | [9, 28)  | sample5.txt |           positive COVID-19 precaution .\n",
      "       future        | [4, 22)  | sample4.txt |              neg COVID-19 education .\n",
      "       negated       | [4, 48)  | sample1.txt | His family recently tested positive for COVID-19 .\n",
      "\n",
      "printing results for query 'CovidSpans(Path, Span, Sent)':\n",
      "    Path     |   Span   |                        Sent\n",
      "-------------+----------+----------------------------------------------------\n",
      " sample1.txt |  [0, 8)  |       COVID-19 results came back positive .\n",
      " sample1.txt | [40, 48) | His family recently tested positive for COVID-19 .\n",
      " sample1.txt | [34, 42) |    patient presents to be tested for COVID-19 .\n",
      " sample2.txt | [26, 34) |        The patient be tested for COVID-19 .\n",
      " sample3.txt |  [0, 8)  |                 COVID-19 like_num\n",
      " sample4.txt | [4, 12)  |              neg COVID-19 education .\n",
      " sample5.txt | [9, 17)  |           positive COVID-19 precaution .\n",
      " sample6.txt | [26, 34) |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "#covid_attributes: negated, other_experiencer, is_future, not_relevant, uncertain, positive\n",
    "ContextMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), ContextRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "?ContextMatches(CovidAttribute, Span, Path, Sent)\n",
    "\n",
    "CovidSpans(Path, Span, Sent) <- Sents(Path, Sent), py_rgx_span(Sent, \"COVID-19\") -> (Span)\n",
    "?CovidSpans(Path, Span, Sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- ContextMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Postprocessor](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/postprocess_rules.py):\n",
    "\n",
    "The postprocessor is designed to apply extra adjustments to the processed text using custom logic or specific requirements not addressed by the spaCy pipeline. These rules modify, remove, or change attributes for each mention of COVID-19 based on either their existing attributes, the context of the sentences in which they appear, or a combination of both. This flexibility allows us to address data issues and implement targeted improvements. For instance, it proves useful in identifying and rectifying incorrectly labeled positive cases, thereby enhancing the accuracy of our classification.\n",
    "\n",
    "**How we implemented it:**  \n",
    "As mentioned earlier, postprocess rules are responsible for modifying, removing, or changing attributes for each mention of COVID-19. In the original project, these attributes are represented as boolean variables stored in an object class for each COVID-19 mention. The rules simply switch the corresponding boolean variable to assign or remove the attribute. However, in spannerlog, we don't have the luxury of creating classes. In our project, when we want to remove a specific attribute, we introduce an additional attribute that acts as its negation. For instance, for the attribute 'positive,' we add 'no_positive,' causing the document classifier to behave as if there is no positive attribute.\n",
    "\n",
    "Additionally, in some cases, the entire COVID-19 mention is removed by eliminating its object. In our project, we introduce an 'IGNORE' attribute, which results in the exclusion of the mention from consideration in the document classifier stage.\n",
    "<br>\n",
    "\n",
    "**In the subsequent cells, we will explore three types of postprocess rules:**\n",
    "1) Rules based on patterns\n",
    "2) Rules utilizing existing attributes and patterns\n",
    "3) Rules applied to the next sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Postprocess rules based on patterns:\n",
    "\n",
    "Example rule in the original project:\n",
    "\n",
    "```\n",
    "PostprocessingRule(\n",
    "        patterns=[\n",
    "            PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.sentence_contains,\n",
    "                condition_args=({\"deny\", \"denies\", \"denied\"},),\n",
    "            ),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.sentence_contains,\n",
    "                condition_args=({\"contact\", \"contacts\", \"confirmed\"},),\n",
    "            ),\n",
    "        \\],\n",
    "        action=postprocessing_functions.remove_ent,\n",
    "        description=\"Remove a coronavirus entity if 'denies' and 'contact' are in. This will help get rid of false positives from screening.\",\n",
    "    ),    \n",
    "```\n",
    "This rule iterates through each entity and checks a series of conditions which are the \"PostprocessingPattern\". If all conditions evaluate as True, then some action is taken on the entity, which is 'remove' action in this example.\n",
    "\n",
    "\n",
    "In our case, we assign \"IGNORE\" attribute to the COVID-19 mention causing it to be excluded from consideration during the document classification process.\n",
    "\n",
    "Each rule in the CSV file follows this format: regexPattern, Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*education.*#IGNORE\n",
      ".* \\?#IGNORE\n",
      "(?=.*\\b(?:deny|denies|denied)\\b)(?=.*\\b(?:contact|confirm)\\b).*#IGNORE\n",
      "(?=.*\\b(?:setting of|s/o)\\b)(?!.*\\b(?:COVID-19 infection|COVID-19 ards)\\b).*#no_positive\n",
      "(?i)(.*benign.*)#uncertain\n",
      "admitted to COVID-19 unit#positive\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('postprocess_pattern_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_rel(\"postprocess_pattern_rules.csv\", relation_name=\"PostprocessRules\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PostprocessMatches(CovidAttribute, Span, Path, Sent)':\n",
      "  CovidAttribute  |  Span   |    Path     |           Sent\n",
      "------------------+---------+-------------+--------------------------\n",
      "      IGNORE      | [0, 24) | sample4.txt | neg COVID-19 education .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "PostprocessMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostprocessRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "?PostprocessMatches(CovidAttribute, Span, Path, Sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- PostprocessMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Postprocess rules utilizing existing attributes and patterns:\n",
    "```\n",
    "PostprocessingRule(\n",
    "        patterns=[\n",
    "        \n",
    "            PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.is_modified_by_category,\n",
    "                condition_args=(\"DEFINITE_POSITIVE_EXISTENCE\",),\n",
    "            ),\n",
    "            # PostprocessingPattern(postprocessing_functions.is_modified_by_category, condition_args=(\"TEST\",)),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.sentence_contains,\n",
    "                condition_args=(\n",
    "                    {\n",
    "                        \"should\",\n",
    "                        \"unless\",\n",
    "                        \"either\",\n",
    "                        \"if comes back\",\n",
    "                        \"if returns\",\n",
    "                        \"if s?he tests positive\",\n",
    "                    },\n",
    "                    True,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        action=set_is_uncertain,\n",
    "        action_args=(True,),\n",
    "        description=\"Subjunctive of test returning positive. 'Will contact patient should his covid-19 test return positive.'\",\n",
    "    ),\n",
    "```\n",
    "This rule examines whether a COVID-19 mention possesses a positive attribute and if the sentence containing it includes any of the words specified in 'condition_args' If these conditions are met, the uncertain attribute is set to true.\n",
    "\n",
    "\n",
    "In our case, we check for each COVID-19 mention in the 'CovidAttributes' table if it's labeled as 'positive', also, we check if any of the specified words in 'condition_args' are present in the same sentence using a regex search. If the conditions are met, then we simply assign it an 'uncertain' attribute.\n",
    "\n",
    "Each rule in the CSV file follows this format: regexPattern, ExistingAttribute, NewAttribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*pending.*#negated#no_negated\n",
      ".*(?:should|unless|either|if comes back|if returns|if s?he tests positive).*#positive#uncertain\n",
      ".*precaution.*#positive#no_future\n",
      ".*(?:re[ -]?test|second test|repeat).*#negated#no_negated\n",
      ".*(?:sign|symptom|s/s).*#positive#uncertain\n"
     ]
    }
   ],
   "source": [
    "print_csv_file('postprocess_attributes_rules.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_rel(\"postprocess_attributes_rules.csv\", relation_name=\"PostprocessRulesWithAttributes\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)':\n",
      "  CovidAttribute  |  NewAttribute  |  Span   |    Path     |              Sent\n",
      "------------------+----------------+---------+-------------+--------------------------------\n",
      "     positive     |   no_future    | [0, 30) | sample5.txt | positive COVID-19 precaution .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostprocessRulesWithAttributes(Pattern, CovidAttribute, NewAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "?PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, NewAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |    NewAttribute     |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      no_future      |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "#TODO this adds attributes to spans, but we can add \"overide attributes\" that then remove the old attributes when doing groupby\n",
    "CovidAttributes(Path, CovidSpan, NewAttribute, Sent) <- CovidAttributes(Path, CovidSpan, CovidAttribute, Sent), PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)\n",
    "?CovidAttributes(Path, CovidSpan, NewAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Postprocess rules applied to the next sentence:\n",
    "There's a rule that checks if the following sentence contains positive mentions. If it does, the COVID-19 mentions in the current sentence are also\n",
    "marked as positive. To Implement this rule in our project, we defined a new relation that pairs each sentence with its subsequent sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_sent(text_path):\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = list(doc.sents)\n",
    "    for i in range(len(sentences) - 1):  # Iterate until the second-to-last sentence\n",
    "        yield(sentences[i].text, sentences[i + 1].text)\n",
    "\n",
    "magic_session.register(ie_function=next_sent, ie_function_name = \"next_sent\", in_rel=[DataTypes.string], out_rel=[DataTypes.string,DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: patient presents to be tested for COVID-19 . next sentence: His family recently tested positive for COVID-19 .\n",
      "sentence: His family recently tested positive for COVID-19 . next sentence: COVID-19 results came back positive .\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "for first_sent, second_sent in next_sent(\"sample1.txt\"):\n",
    "    print(f\"sentence: {first_sent}\", f\"next sentence: {second_sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'NextSent(Path, Sent1, Sent2)':\n",
      "    Path     |                       Sent1                        |                       Sent2\n",
      "-------------+----------------------------------------------------+----------------------------------------------------\n",
      " sample1.txt | His family recently tested positive for COVID-19 . |       COVID-19 results came back positive .\n",
      " sample1.txt |    patient presents to be tested for COVID-19 .    | His family recently tested positive for COVID-19 .\n",
      " sample2.txt |        The patient be tested for COVID-19 .        |               Results be positive .\n",
      " sample3.txt |                associated_diagnosis                |                     like_num .\n",
      " sample3.txt |                     like_num .                     |                 COVID-19 like_num\n",
      " sample3.txt |             problem_list : like_num .              |                associated_diagnosis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "NextSent(Path, Sent1, Sent2) <- FilesPaths(Path), next_sent(Path) -> (Sent1, Sent2)\n",
    "?NextSent(Path, Sent1, Sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample2.txt |  [26, 34)   |      positive       |        The patient be tested for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      no_future      |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spannerlog\n",
    "new PostProcessWithNextSentenceRules(str, str)\n",
    "PostProcessWithNextSentenceRules(\"(?i)(?:^(?:positive|detected)|results?(?: be)? positive)\", \"positive\")\n",
    "PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostProcessWithNextSentenceRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent1) <- CovidSpans(Path, CovidSpan, Sent1), NextSent(Path, Sent1, Sent2), PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent2)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Document Classifier](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/document_classifier.py):\n",
    "\n",
    "Now we have the basic pieces in place to make our document classification. Each document is classified as either 'POS', 'UNK', or 'NEG' determined by the attributes of its COVID-19 mentions. The Results are stored in a DataFrame.\n",
    "\n",
    "Document Classifier stage has 2 parts:\n",
    " 1) **Attribute filtering**: Our pipeline assigns various attributes to each COVID-19 mention. However, during this stage, each COVID-19 case is refined to possess only one attribute. This filtering process operates based on specific conditions outlined in the 'attribute_filter' function.\n",
    " 2) **Document classification**: Documents are classified based on distinct conditions, as detailed in the 'classify_doc_helper' function. This step ensures the accurate categorization of each document according to the specified criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_filter(group):\n",
    "    \"\"\"\n",
    "    Filters attributes within each \"CovidSpan\" of a DataFrame table based on specific conditions.\n",
    "\n",
    "    Parameters:\n",
    "        group (pandas.Series): A pandas Series representing attributes for each \"CovidSpan\" within a DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        str: Filtered \"CovidSpan\" attribute determined by the following rules:\n",
    "            - If 'IGNORE' is present, returns 'IGNORE'.\n",
    "            - If 'negated' is present (and 'no_negated' is not present), returns 'negated'.\n",
    "            - If 'future' is present (and 'no_future' is not present), returns 'negated'.\n",
    "            - If 'other experiencer' or 'not relevant' is present, returns 'negated'.\n",
    "            - If 'positive' is present (and 'uncertain' and 'no_positive' are not present), returns 'positive'.\n",
    "            - Otherwise, returns 'uncertain'.\n",
    "    \"\"\"\n",
    "    if 'IGNORE' in group.values:\n",
    "        return 'IGNORE'\n",
    "    elif 'negated' in group.values and not 'no_negated' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'future' in group.values and not 'no_future' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'other experiencer' in group.values or 'not relevant' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'uncertain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "          Path Attribute\n",
      "0  sample1.txt    IGNORE\n",
      "1  sample1.txt   negated\n",
      "2  sample1.txt  positive\n",
      "3  sample2.txt  positive\n",
      "\n",
      "After:\n",
      "          Path Attribute\n",
      "0  sample1.txt    IGNORE\n",
      "1  sample2.txt  positive\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "data = {'Path': [\"sample1.txt\", \"sample1.txt\", \"sample1.txt\", \"sample2.txt\"],\n",
    "        'Attribute': ['IGNORE', 'negated', 'positive', 'positive']}\n",
    "df_example = pd.DataFrame(data)\n",
    "print(\"Before:\")\n",
    "print(df_example)\n",
    "\n",
    "df_example['Attribute'] = df_example.groupby(['Path'])['Attribute'].transform(attribute_filter)\n",
    "df_example = df_example.drop_duplicates().reset_index(drop=True)\n",
    "print(\"\\nAfter:\")\n",
    "print(df_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>CovidSpan</th>\n",
       "      <th>CovidAttribute</th>\n",
       "      <th>Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>[0, 8)</td>\n",
       "      <td>positive</td>\n",
       "      <td>COVID-19 results came back positive .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>[40, 48)</td>\n",
       "      <td>negated</td>\n",
       "      <td>His family recently tested positive for COVID-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>[26, 34)</td>\n",
       "      <td>positive</td>\n",
       "      <td>The patient be tested for COVID-19 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample3.txt</td>\n",
       "      <td>[0, 8)</td>\n",
       "      <td>positive</td>\n",
       "      <td>COVID-19 like_num</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample4.txt</td>\n",
       "      <td>[4, 12)</td>\n",
       "      <td>IGNORE</td>\n",
       "      <td>neg COVID-19 education .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sample5.txt</td>\n",
       "      <td>[9, 17)</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive COVID-19 precaution .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sample6.txt</td>\n",
       "      <td>[26, 34)</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>The patient have reported COVID-19 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path CovidSpan CovidAttribute   \n",
       "0  sample1.txt    [0, 8)       positive  \\\n",
       "1  sample1.txt  [40, 48)        negated   \n",
       "2  sample2.txt  [26, 34)       positive   \n",
       "3  sample3.txt    [0, 8)       positive   \n",
       "4  sample4.txt   [4, 12)         IGNORE   \n",
       "5  sample5.txt   [9, 17)       positive   \n",
       "6  sample6.txt  [26, 34)      uncertain   \n",
       "\n",
       "                                                Sent  \n",
       "0              COVID-19 results came back positive .  \n",
       "1  His family recently tested positive for COVID-...  \n",
       "2               The patient be tested for COVID-19 .  \n",
       "3                                  COVID-19 like_num  \n",
       "4                           neg COVID-19 education .  \n",
       "5                     positive COVID-19 precaution .  \n",
       "6               The patient have reported COVID-19 .  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (magic_session.run_commands(\"?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\", print_results=False, format_results=True))[0]\n",
    "if len(df) == 0:\n",
    "    df = DataFrame(columns=[\"Path\",\"CovidSpan\",\"CovidAttribute\"])\n",
    "df['CovidAttribute'] = df.groupby(['CovidSpan', 'Sent'])['CovidAttribute'].transform(attribute_filter)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_doc_helper(group):\n",
    "    \"\"\"\n",
    "Classifies a document as 'POS', 'UNK', or 'NEG' based on COVID-19 attributes.\n",
    "\n",
    "Parameters:\n",
    "    group (pandas.Series): A pandas Series representing COVID-19 attributes for each document within a DataFrame.\n",
    "    \n",
    "Returns:\n",
    "    str: Document classification determined as follows:\n",
    "         - 'POS': If at least one COVID-19 attribute with \"positive\" is present in the group.\n",
    "         - 'UNK': If at least one COVID-19 attribute with \"uncertain\" is present in the group and no \"positive\" attributes,\n",
    "                  or there's at least one COVID-19 attribute with 'IGNORE' and no other COVID-19 attributes exist.\n",
    "         - 'NEG': Otherwise.\n",
    "\"\"\"\n",
    "    if 'positive' in group.values:\n",
    "        return 'POS'\n",
    "    elif 'uncertain' in group.values:\n",
    "        return 'UNK'\n",
    "    elif 'negated' in group.values:\n",
    "        return 'NEG'\n",
    "    else:\n",
    "        return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "          Path  Attribute\n",
      "0  sample1.txt  uncertain\n",
      "1  sample1.txt    negated\n",
      "2  sample1.txt   positive\n",
      "3  sample2.txt   positive\n",
      "\n",
      "After:\n",
      "          Path DocResult\n",
      "0  sample1.txt       POS\n",
      "1  sample2.txt       POS\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "data = {'Path': [\"sample1.txt\", \"sample1.txt\", \"sample1.txt\", \"sample2.txt\"],\n",
    "        'Attribute': ['uncertain', 'negated', 'positive', 'positive']}\n",
    "df_example = pd.DataFrame(data)\n",
    "print(\"Before:\")\n",
    "print(df_example)\n",
    "\n",
    "df_example['DocResult'] = df_example.groupby(['Path'])['Attribute'].transform(classify_doc_helper)\n",
    "df_example = df_example[['Path', 'DocResult']]\n",
    "df_example = df_example.drop_duplicates().reset_index(drop=True)\n",
    "print(\"\\nAfter:\")\n",
    "print(df_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>DocResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample3.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample4.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample5.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sample6.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path DocResult\n",
       "0  sample1.txt       POS\n",
       "1  sample2.txt       POS\n",
       "2  sample3.txt       POS\n",
       "3  sample4.txt       UNK\n",
       "4  sample5.txt       POS\n",
       "5  sample6.txt       UNK"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DocResult'] = df.groupby('Path')['CovidAttribute'].transform(classify_doc_helper)\n",
    "df = df[['Path', 'DocResult']]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling unmentioned paths:\n",
    "At this step, we assign a classification result 'UNK' to paths not identified in the previous DataFrame result. This occurs when our pipeline doesn't detect any mention of COVID-19 or its synonyms in the text of those paths. As a result, these paths are excluded from all types of relations, consistent with our primary focus on COVID-19 entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>DocResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample3.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample4.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample5.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sample6.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sample7.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path DocResult\n",
       "0  sample1.txt       POS\n",
       "1  sample2.txt       POS\n",
       "2  sample3.txt       POS\n",
       "3  sample4.txt       UNK\n",
       "4  sample5.txt       POS\n",
       "5  sample6.txt       UNK\n",
       "6  sample7.txt       UNK"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = (magic_session.run_commands(\"?FilesPaths(Path)\", print_results=False, format_results=True))[0]\n",
    "df = (pd.merge(df, df_path, on='Path', how='outer'))\n",
    "df['DocResult'] = df['DocResult'].fillna(\"UNK\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing It All Together\n",
    "\n",
    "In this section, we will directly compare the original Python Spacy pipeline project with its spannerlog counterpart. Our emphasis is on showcasing the overall brevity of the spannerlog implementation in contrast to the Python Spacy pipeline.\n",
    "\n",
    "### Code Metrics\n",
    "\n",
    "Let's commence by providing an estimated count of total lines in each implementation:\n",
    "\n",
    "- **Total Number of Lines in the original Python implementation:** **4435**\n",
    "- **Total Number of Lines in our spannerlog implementation:** **596** (7 times smaller!)\n",
    "\n",
    "And here's a detailed comparison:\n",
    "\n",
    "![code line comparison](img/line_counting.png)\n",
    "\n",
    "With the caveat that number of lines do not fully capture code complexity, let us analyze the lines of code a little more in depth.\n",
    "Analyzing our implementation vs the original we note that:\n",
    "\n",
    "- We used the same libraries as the original implementations, so both\n",
    "  - the core computations, that should turn into ie functions\n",
    "  - the wrapping logic which remains in pure python\n",
    "  did not significantly change in size.\n",
    "- even if we assume that our 203 lines of python code are worth over 300 lines of the original implementations core and wrapping logic, we are still left with over 4000 lines of code that were converted into 393 (107+251+35) of either declarative code and data.\n",
    "- This means that over 90% of the original code base, which constitutes control flow and data ingestion logic, underwent a ten-fold decrease in size while providing less surface areas for errors since declarative languages and data can be statically analyzed to a greater extent than imperative code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation - raw lines of code\n",
    "\n",
    "Now, we will present the combined spannerlog and python code (excluding \"generic ie\" functions and excluding queries) to visually illustrate the compactness of the implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept tagger:\n",
    "```python\n",
    "def lemmatize_text(text_path, lemma_words_path):\n",
    "    # Define a list of words to be lemmatized\n",
    "    lemma_words = [line.strip() for line in open(f\"{lemma_words_path}\") if line.strip()]\n",
    "\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    lemmatized_text = \"\"\n",
    "    for token in doc:\n",
    "        if token.lemma_ in lemma_words:\n",
    "            lemmatized_text += token.lemma_\n",
    "        elif token.like_num:\n",
    "            lemmatized_text += \"like_num\"\n",
    "        else:\n",
    "            lemmatized_text += token.text\n",
    "        lemmatized_text += \" \"\n",
    "\n",
    "    # Write the lemmatized text back to the same file\n",
    "    with open(text_path, 'w') as file:\n",
    "        file.writelines(lemmatized_text)\n",
    "\n",
    "    yield lemmatized_text\n",
    "\n",
    "def annotate_text_with_pos(text_path):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]:\n",
    "            yield token.pos_, Span(token.idx, token.idx + len(token.text))\n",
    "        else:\n",
    "            yield tuple()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "session.import_rel(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")\n",
    "\n",
    "%%spannerlog\n",
    "LemmaMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"lemma\"), py_rgx_span(Content, Pattern) -> (Span)\n",
    "%%python\n",
    "replace_spans(\"LemmaMatches\", \"FilesPaths\")\n",
    "%%spannerlog\n",
    "POSTable(POS, Span, Path) <- FilesContent(Path, Content), annotate_text_with_pos(Path) -> (POS, Span)\n",
    "POSMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"pos\"), py_rgx_span(Content, Pattern) -> (Span)\n",
    "POSRuleMatches(Label, Span, Path) <- POSTable(POS, Span, Path), POSMatches(Label, Span, Path)\n",
    "%%python\n",
    "replace_spans(\"POSRuleMatches\", \"FilesPaths\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target matcher:\n",
    "```python\n",
    "magic_session.import_rel(\"target_rules.csv\", relation_name=\"TargetTagRules\", delimiter=\",\")\n",
    "\n",
    "%%spannerlog\n",
    "TargetTagMatches(Label, Span, Path) <- FilesContent(Path, Content), TargetTagRules(Pattern, Label), py_rgx_span(Content,Pattern) -> (Span)\n",
    "replace_spans(\"TargetTagMatches\", \"FilesPaths\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sectionizer:\n",
    "```python\n",
    "\n",
    "magic_session.import_rel(\"section_rules.csv\", relation_name=\"SectionRules\", delimiter=\",\")\n",
    "\n",
    "%%spannerlog\n",
    "SectionRulesMatches(Label, Span, Path) <- FilesContent(Path, Content), SectionRules(Pattern, Label), py_rgx_span(Content,Pattern) -> (Span)\n",
    "replace_spans(\"SectionRulesMatches\", \"FilesPaths\")\n",
    "# TODO here we only needed one match since we only marked the sections that are positive, thats why there is no file here\n",
    "# TODO check this on text with multiple sections\n",
    "pattern = \"(?i)(?:diagnoses :|observation_and_plan :|past_medical_history :|problem_list :)(?:(?!labs :|addendum :|allergies :|chief_complaint :|comments :|family_history :|hospital_course :|imaging :|labs_and_studies :|medications :|neurological :|other :|patient_education :|physical_exam :|reason_for_examination :|signature :|social_history :).)*\"\n",
    "# TODO can we maybe make these delimeter and the have a span delimeter ie function\n",
    "\n",
    "new SectionRulesAttribute(str, str)\n",
    "SectionRulesAttribute(pattern, \"positive\")\n",
    "SectionMatches(Path, Span, CovidAttribute) <- FilesContent(Path, Content), SectionRulesAttribute(Pattern, CovidAttribute), py_rgx_span(Content, Pattern) -> (Span)\n",
    "# TODO here we filter spans by their sections, we only save spans that are in sections marked as positive\n",
    "CovidMatches(Path, Span) <- FilesContent(Path, Content), py_rgx_span(Content, \"COVID-19\") -> (Span)\n",
    "SectionCovidAttributes(Path, CovidSpan, CovidAttribute) <- SectionMatches(Path, Span1, CovidAttribute), CovidMatches(Path, Span2), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "\n",
    "# TODO here we merged the sentence fragmentation with the sectionizer, in the original code this in implicit in the postprocessing rules\n",
    "Sents(Path, Sent) <- FilesPaths(Path), sent_tokenization(Path) -> (Sent)\n",
    "SentSpans(Path, Sent, SentSpan) <- FilesContent(Path, Content), Sents(Path, Sent), py_rgx_span(Content, Sent) -> (SentSpan)\n",
    "\n",
    "# TODO this relation saves the covid attributes left so far, just splits them by sentence\n",
    "# Notive covidattrbutes is first defined here, but we have 4 derivation, by section, by context matcher (that looks on a sentence context), 2 postprocess\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- SectionCovidAttributes(Path, AbsCovidSpan, CovidAttribute),\\\n",
    "SentSpans(Path, Sent, SentSpan) ,get_relative_span(AbsCovidSpan, SentSpan) -> (CovidSpan)\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context matcher:\n",
    "```python\n",
    "magic_session.import_rel(\"context_rules.csv\", relation_name=\"ContextRules\", delimiter=\"#\")\n",
    "\n",
    "%%spannerlog\n",
    "ContextMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), ContextRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "CovidSpans(Path, Span, Sent) <- Sents(Path, Sent), py_rgx_span(Sent, \"COVID-19\") -> (Span)\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- ContextMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessor:\n",
    "```python\n",
    "def next_sent(text_path):\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = list(doc.sents)\n",
    "    for i in range(len(sentences) - 1):  # Iterate until the second-to-last sentence\n",
    "        yield(sentences[i].text, sentences[i + 1].text)\n",
    "\n",
    "magic_session.register(ie_function=next_sent, ie_function_name = \"next_sent\", in_rel=[DataTypes.string], out_rel=[DataTypes.string,DataTypes.string])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "magic_session.import_relation_from_csv(\"postprocess_pattern_rules.csv\", relation_name=\"PostprocessRules\", delimiter=\"#\")\n",
    "\n",
    "%%spannerlog\n",
    "PostprocessMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostprocessRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- PostprocessMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "\n",
    "NextSent(Path, Sent1, Sent2) <- FilesPaths(Path), next_sent(Path) -> (Sent1, Sent2)\n",
    "new PostProcessWithNextSentenceRules(str, str)\n",
    "PostProcessWithNextSentenceRules(\"(?i)(?:^(?:positive|detected)|results?(?: be)? positive)\", \"positive\")\n",
    "PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostProcessWithNextSentenceRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent1) <- CovidSpans(Path, CovidSpan, Sent1), NextSent(Path, Sent1, Sent2), PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlib\n",
    "PostProcessWithNextSentenceMatches()<- Docs(Content,'TargetMatched'),\n",
    "    Covid_matches(covid_span),\n",
    "    is_span_contained(covid_span,s1),\n",
    "    is_span_contained(context_span,s2),\n",
    "    Nextsent(s1,s2),\n",
    "    NextSentencePatterns(pattern,attribute),\n",
    "    py_rgx_span(content,pattern)->(context_span),\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Classifier:\n",
    "```python\n",
    "# TODO here we merge attributes per span\n",
    "def attribute_filter(group):\n",
    "    if 'IGNORE' in group.values:\n",
    "        return 'IGNORE'\n",
    "    elif 'negated' in group.values and not 'no_negated' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'future' in group.values and not 'no_future' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'other experiencer' in group.values or 'not relevant' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'uncertain'\n",
    "\n",
    "df = (magic_session.run_commands(\"?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\", print_results=False, format_results=True))[0]\n",
    "if len(df) == 0:\n",
    "    df = DataFrame(columns=[\"Path\",\"CovidSpan\",\"CovidAttribute\"])\n",
    "df['CovidAttribute'] = df.groupby(['CovidSpan', 'Sent'])['CovidAttribute'].transform(attribute_filter)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# TODO here we merge attributes per doc\n",
    "def classify_doc_helper(group):\n",
    "    if 'positive' in group.values:\n",
    "        return 'POS'\n",
    "    elif 'uncertain' in group.values:\n",
    "        return 'UNK'\n",
    "    elif 'negated' in group.values:\n",
    "        return 'NEG'\n",
    "    else:\n",
    "        return 'UNK'\n",
    "        \n",
    "df['DocResult'] = df.groupby('Path')['CovidAttribute'].transform(classify_doc_helper)\n",
    "df = df[['Path', 'DocResult']]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df_path = (magic_session.run_commands(\"?FilesPaths(Path)\", print_results=False, format_results=True))[0]\n",
    "df = (pd.merge(df, df_path, on='Path', how='outer'))\n",
    "df['DocResult'] = df['DocResult'].fillna(\"UNK\")\n",
    "df\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
