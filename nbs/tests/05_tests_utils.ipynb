{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tests.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installation NLP failed\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#| output: false\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from pandas import DataFrame\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Iterable, Dict, no_type_check, Type\n",
    "import pandas as pd\n",
    "from spannerlib.lark_passes import GenericPass\n",
    "from spannerlib.session import queries_to_string, Session\n",
    "\n",
    "TEMP_FILE_NAME = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_equal_stripped_sorted_tables(result_text: str, expected_text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Compares all lines in between two strings, ignoring the order of the lines.\n",
    "\n",
    "    @param result_text: first string to compare, usually the output of a test.\n",
    "    @param expected_text: second string to compare, usually the expected output of a test.\n",
    "    @return: True if equal, else False.\n",
    "    \"\"\"\n",
    "    sorted_result_text = sorted([line.strip() for line in result_text.splitlines() if line.strip()])\n",
    "    sorted_expected_text = sorted([line.strip() for line in expected_text.splitlines() if line.strip()])\n",
    "    return sorted_result_text == sorted_expected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_equal_dataframes_ignore_order(result_df: DataFrame, expected_df: DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Similarly to `is_equal_stripped_sorted_tables`, compares two dataframes while ignoring the order of the rows.\n",
    "\n",
    "    @param result_df: first dataframe to compare.\n",
    "    @param expected_df: second dataframe to compare.\n",
    "    @return: True if equal, else False.\n",
    "    \"\"\"\n",
    "    result_df_sorted = DataFrame(np.sort(result_df.values, axis=0), index=result_df.index, columns=result_df.columns)\n",
    "    expected_df_sorted = DataFrame(np.sort(expected_df.values, axis=0), index=expected_df.index,\n",
    "                                   columns=expected_df.columns)\n",
    "    return result_df_sorted.equals(expected_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def table_to_query_free_vars_tuples(table: str) -> Iterable:\n",
    "    \"\"\"\n",
    "    Parses the string table into a nicer format.\n",
    "\n",
    "    @param table: the string that represents a table.\n",
    "    @return: the clean format (see comments above return statements).\n",
    "    \"\"\"\n",
    "    # split string into lines and ignore white spaces.\n",
    "    # tuple[0] is always the print statement.\n",
    "    tuples = [line.strip() for line in table.split(\"\\n\") if len(line.strip()) != 0]\n",
    "    if len(tuples) < 2:\n",
    "        raise ValueError(\"illegal output received: \\n\\\"\" + '\\n'.join(tuples) + '\"')\n",
    "    # if table is empty (which means it contains one value of true/false) we return tuple.\n",
    "    # tuple[0] is the print statement, tuple[1] is true/false.\n",
    "    if tuples[1] in [\"[()]\", \"[]\"]:\n",
    "        return tuples\n",
    "    # if table is not empty, then: tuple[0] is the print statement, tuple[1] are the free vars and tuple[3:] contains\n",
    "    # all the tuples inside the table .\n",
    "    else:  # query   |free vars|     tuples\n",
    "        return tuples[0], tuples[1], set(tuples[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_to_tables(result: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    @param result: spannerlog's output.\n",
    "    @return: List of strings, each string represents a table.\n",
    "    \"\"\"\n",
    "\n",
    "    # in spannerlog's output, all tables are separated by two consecutive \\n.\n",
    "    return result.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compare_strings(expected: str, output: str) -> bool:\n",
    "    \"\"\"\n",
    "    @param expected: expected output.\n",
    "    @param output: actual output.\n",
    "    @return: True if output and expected represent the same result, False otherwise.\n",
    "    \"\"\"\n",
    "    expected = \"\\n\".join([line.strip() for line in expected.splitlines()])\n",
    "    output = \"\\n\".join([line.strip() for line in output.splitlines()])\n",
    "\n",
    "    expected_tables, output_tables = split_to_tables(expected), split_to_tables(output)\n",
    "    # if there are different number of tables than false\n",
    "    if len(expected_tables) != len(output_tables):\n",
    "        return False\n",
    "\n",
    "    # check that all the tables are equal\n",
    "    for expected_table, output_table in zip(expected_tables, output_tables):\n",
    "        if table_to_query_free_vars_tuples(expected_table) != table_to_query_free_vars_tuples(output_table):\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@no_type_check\n",
    "def run_test(commands: str, expected_output: Optional[str] = None, functions_to_import: Iterable[Dict] = (),\n",
    "             session: Optional[Session] = None) -> Session:\n",
    "    \"\"\"\n",
    "    A function that executes a test.\n",
    "\n",
    "    @param commands: the commands to run.\n",
    "    @param expected_output: the expected output of the commands. if it has value of None, than we won't check the output.\n",
    "    @param functions_to_import: an iterable of functions we want to import to the session.\n",
    "    @param session: the session in which we run the commands.\n",
    "    @return: the session it created or got as an argument.\n",
    "    \"\"\"\n",
    "    # if session wasn't passed as an arg than we create it\n",
    "    if session is None:\n",
    "        session = Session()\n",
    "\n",
    "    # import all ie functions\n",
    "    for ie_function in functions_to_import:\n",
    "        session.register(**ie_function)\n",
    "    commands_result = session.run_commands(commands, print_results=True)\n",
    "\n",
    "    if expected_output is not None:\n",
    "        commands_result_string = queries_to_string(commands_result)\n",
    "        assert compare_strings(expected_output, commands_result_string), \"expected string != result string\"\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_unordered_dataframes_equal(df1: pd.DataFrame, df2: pd.DataFrame) -> bool:\n",
    "    if set(df1.columns) != set(df2.columns):\n",
    "        print(\"Columns do not match.\")\n",
    "        return False\n",
    "    \n",
    "    df1 = df1.sort_values(by=list(df1.columns)).reset_index(drop=True)\n",
    "    df2 = df2.sort_values(by=list(df2.columns)).reset_index(drop=True)\n",
    "\n",
    "    return df1.equals(df2)\n",
    "\n",
    "@no_type_check\n",
    "def run_test_df_compare(commands: str, expected_output: Optional[pd.DataFrame] = None, functions_to_import: Iterable[Dict] = (),\n",
    "             session: Optional[Session] = None) -> Session:\n",
    "    \"\"\"\n",
    "    A function that executes a test.\n",
    "\n",
    "    @param commands: the commands to run.\n",
    "    @param expected_output: the expected output of the commands (in a format of pd.DataFrame). if it has value of None, than we won't check the output.\n",
    "    @param functions_to_import: an iterable of functions we want to import to the session.\n",
    "    @param session: the session in which we run the commands.\n",
    "    @return: the session it created or got as an argument.\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = Session()\n",
    "\n",
    "    for ie_function in functions_to_import:\n",
    "        session.register(**ie_function)\n",
    "\n",
    "    command_result = session.send_commands_result_into_df(commands)\n",
    "\n",
    "    if expected_output is not None:\n",
    "        assert check_unordered_dataframes_equal(command_result, expected_output)\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_session_with_optimizations(parse_graph_optimization_passes: Iterable[Type[GenericPass]] = (),\n",
    "                                   term_graph_optimization_passes: Iterable[Type[GenericPass]] = ()) -> Session:\n",
    "    \"\"\"\n",
    "    Creates a session and adds optimization passes to the pass stack.\n",
    "    @param parse_graph_optimization_passes: optimization passes that will be added before AddRulesToComputationTermGraph pass.\n",
    "    @param term_graph_optimization_passes: optimization passes that will be added after AddRulesToComputationTermGraph pass\n",
    "    @return: the session.\n",
    "    \"\"\"\n",
    "    session = Session()\n",
    "    pass_stack = session.get_pass_stack()\n",
    "    term_graph_pass = pass_stack.pop()\n",
    "\n",
    "    pass_stack.extend(parse_graph_optimization_passes)\n",
    "    pass_stack.append(term_graph_pass)\n",
    "    pass_stack.extend(term_graph_optimization_passes)\n",
    "\n",
    "    session.set_pass_stack(pass_stack)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_commands_into_csv_test(expected_longrel: str, im_ex_session: Session, commands: str, query_for_csv: str) -> None:\n",
    "    im_ex_session.run_commands(commands, print_results=False)\n",
    "    # query into csv and compare with old file\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        temp_csv = Path(temp_dir) / TEMP_FILE_NAME\n",
    "        im_ex_session.send_commands_result_into_csv(query_for_csv, temp_csv)\n",
    "        assert Path(temp_csv).is_file(), \"file was not created\"\n",
    "\n",
    "        with open(temp_csv) as f_temp:\n",
    "            assert is_equal_stripped_sorted_tables(f_temp.read(), expected_longrel), \"file was not written properly\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
