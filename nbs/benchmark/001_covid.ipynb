{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp banchmark.covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "from IPython.display import display, HTML\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from itables import init_notebook_mode,show\n",
    "init_notebook_mode(all_interactive=False,connected=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "# importing dependencies\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pathlib import Path\n",
    "from spannerlib import get_magic_session,Session\n",
    "from spannerlib.ie_func.basic import rgx, rgx_is_match, rgx_split, span_arity, span_contained\n",
    "\n",
    "VERSION = \"OLD\"\n",
    "# VERSION = \"SPANNERFLOW\"\n",
    "VERSION = \"SPANNERFLOW_PYTHON_IE\"\n",
    "if VERSION in [\"SPANNERFLOW\", \"SPANNERFLOW_PYTHON_IE\"]:\n",
    "    from spannerflow.span import Span\n",
    "else:\n",
    "    from spannerlib import Span\n",
    "sess = get_magic_session()\n",
    "\n",
    "\n",
    "sess.register('py_rgx', rgx, [str, Span], span_arity)\n",
    "sess.register('py_rgx_split', rgx_split, [str, Span], [Span,Span])\n",
    "sess.register('py_rgx_is_match', rgx_is_match, [str, Span], [bool])\n",
    "sess.register('py_span_contained', span_contained, [Span, Span], [bool])    \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "# configurations\n",
    "input_dir = Path('covid_data/sample_inputs')\n",
    "data_dir = Path('covid_data/rules_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def split_sentence(text):\n",
    "    \"\"\"\n",
    "    Splits a text into individual sentences. using spacy's sentence detection.\n",
    "    \n",
    "    Returns:\n",
    "        str: Individual sentences extracted from the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(str(text))\n",
    "    start = 0\n",
    "    for sentence in doc.sents:\n",
    "        end = start+len(sentence.text)\n",
    "        # note that we yield a Span object, so we can keep track of the locations of the sentences\n",
    "        yield Span(text,start,end)\n",
    "        start = end + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class LemmaFromList():\n",
    "    def __init__(self,lemma_list):\n",
    "        self.lemma_list = lemma_list\n",
    "\n",
    "    def __call__(self,text):\n",
    "        doc = nlp(str(text))\n",
    "        for word in doc:\n",
    "            start = word.idx\n",
    "            end = start + len(word.text)\n",
    "            if word.lemma_ in self.lemma_list:\n",
    "                yield (Span(text,start,end),word.lemma_)\n",
    "            elif word.like_num:\n",
    "                yield (Span(text,start,end),'like_num')\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "lemma_list = (data_dir/'lemma_words.txt').read_text().split()\n",
    "lemmatizer = LemmaFromList(lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class PosFromList():\n",
    "    def __init__(self,pos_list):\n",
    "        self.pos_list = pos_list\n",
    "    def __call__(self,text):\n",
    "        doc = nlp(str(text))\n",
    "        for word in doc:\n",
    "            start = word.idx\n",
    "            end = start + len(word.text)\n",
    "            if word.pos_ in self.pos_list:\n",
    "                yield (Span(text,start,end),word.pos_)\n",
    "\n",
    "pos_annotator = PosFromList([\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "if VERSION in [\"SPANNERFLOW\", \"SPANNERFLOW_PYTHON_IE\"]:\n",
    "    sess.register('split_sentence',split_sentence,[Span],[Span])\n",
    "    sess.register('pos',pos_annotator,[Span],[Span,str])\n",
    "    sess.register('lemma',lemmatizer,[Span],[Span,str])\n",
    "else:\n",
    "    sess.register('split_sentence',split_sentence,[str],[Span])\n",
    "    sess.register('pos',pos_annotator,[str],[Span,str])\n",
    "    sess.register('lemma',lemmatizer,[str],[Span,str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def rewrite(text,span_label_pairs):\n",
    "    \"\"\"rewrites a string given a dataframe with spans and the string to rewrite them to\n",
    "    assumes that the spans belong to the text\n",
    "\n",
    "    Args:\n",
    "        text (str like): string to rewrite\n",
    "        span_label_pairs (pd.Dataframe) dataframe with two columns, first is spans in the doc to rewrite\n",
    "            second is what to rewrite to\n",
    "    Returns:\n",
    "        The rewritten string\n",
    "    \"\"\"    \n",
    "    if isinstance(text,Span):\n",
    "        text = text.as_str()\n",
    "    span_label_pairs = sorted(list(span_label_pairs.itertuples(index=False,name=None)), key=lambda x: x[0].start)\n",
    "\n",
    "    rewritten_text = ''\n",
    "    current_pos = 0\n",
    "    for span,label in span_label_pairs:\n",
    "        rewritten_text += text[current_pos:span.start] + label \n",
    "        current_pos = span.end\n",
    "\n",
    "    rewritten_text += text[current_pos:]\n",
    "\n",
    "    return rewritten_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rewrite_docs(docs,span_label,new_version):\n",
    "    \"\"\"Given a dataframe of documents of the form (path,doc,version) and a dataframe of spans to rewrite\n",
    "    of the form (path,word,from_span,to_tag), rewrites the documents and returns a new dataframe of the form\n",
    "    (path,doc,new_version)\n",
    "\n",
    "    \"\"\"\n",
    "    new_tuples =[]\n",
    "    span_label.columns = ['P','D','W','L']\n",
    "    for path,doc,_ in docs.itertuples(index=False,name=None):\n",
    "        span_label_per_doc = span_label[span_label['P'] == path][['W','L']]\n",
    "        new_text = rewrite(doc,span_label_per_doc)\n",
    "        new_tuples.append((path,new_text,new_version))\n",
    "    return pd.DataFrame(new_tuples,columns=['P','D','V'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "sess.import_rel(\"ConceptTagRules\",data_dir/\"concept_tags_rules.csv\" , delim=\",\")\n",
    "sess.import_rel(\"TargetTagRules\",data_dir/\"target_rules.csv\",delim=\",\")\n",
    "sess.import_rel(\"SectionTags\",data_dir/\"section_tags.csv\",delim=\",\")\n",
    "sess.import_rel(\"PositiveSectionTags\",data_dir/\"positive_section_tags.csv\",delim=\",\")\n",
    "sess.import_rel(\"SentenceContextRules\",data_dir/'sentence_context_rules.csv',delim=\"#\")\n",
    "sess.import_rel(\"PostprocessPatternRules\",data_dir/'postprocess_pattern_rules.csv',delim=\"#\")\n",
    "sess.import_rel(\"PostprocessRulesWithAttributes\",data_dir/'postprocess_attributes_rules.csv',delim=\"#\")\n",
    "sess.import_rel(\"NextSentencePostprocessPatternRules\",data_dir/'postprocess_pattern_next_sentence_rules.csv',delim=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from glob import glob\n",
    "file_paths = [Path(p) for p in glob(str(input_dir/'*.txt'))]\n",
    "raw_docs = pd.DataFrame([\n",
    "    [p.name,p.read_text(),'raw_text'] for p in file_paths\n",
    "],columns=['Path','Doc','Version']\n",
    ")\n",
    "if VERSION in [\"SPANNERFLOW\", \"SPANNERFLOW_PYTHON_IE\"]:\n",
    "    sess.import_rel('Docs',raw_docs, scheme=[str, Span, str])\n",
    "else:\n",
    "    sess.import_rel('Docs',raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "Lemmas(P,D,Word,Lem)<-Docs(P,D,\"raw_text\"),lemma(D)->(Word,Lem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "lemma_tags = sess.export('?Lemmas(P,D,W,L)')\n",
    "lemma_docs = rewrite_docs(raw_docs,lemma_tags,'lemma')\n",
    "if VERSION in [\"SPANNERFLOW\", \"SPANNERFLOW_PYTHON_IE\"]:\n",
    "    sess.import_rel('Docs',lemma_docs, scheme=[str, Span, str])\n",
    "else:\n",
    "    sess.import_rel('Docs',lemma_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%spannerlog\n",
    "LemmaConceptMatches(Path,Doc,Span,Label) <- \n",
    "    Docs(Path,Doc,\"lemma\"),\n",
    "    ConceptTagRules(Pattern, Label, \"lemma\"),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_rgx(Pattern,Doc) -> (Span)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "lemma_concept_matches = sess.export('?LemmaConceptMatches(Path,Doc,Span,Label)')\n",
    "lemma_concepts = rewrite_docs(lemma_docs,lemma_concept_matches,'lemma_concept')\n",
    "if VERSION in [\"SPANNERFLOW\", \"SPANNERFLOW_PYTHON_IE\"]:\n",
    "    sess.import_rel('Docs',lemma_concepts, scheme=[str, Span, str])\n",
    "else:\n",
    "    sess.import_rel('Docs',lemma_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "# here we get the spans of all POS\n",
    "Pos(P,D,Word,Lem)<-Docs(P,D,\"lemma_concept\"),pos(D)->(Word,Lem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "# here we look for concept rule matches where the matched word is also tagged via POS\n",
    "PosConceptMatches(Path,Doc,Span,Label) <- \n",
    "    Docs(Path,Doc,\"lemma_concept\"),\n",
    "    ConceptTagRules(Pattern, Label, \"pos\"),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_rgx(Pattern,Doc) -> (Span),\n",
    "    Pos(Path,Doc,Span,POSLabel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "pos_concept_matches = sess.export('?PosConceptMatches(P,D,W,L)')\n",
    "pos_concept_docs = rewrite_docs(lemma_concepts,pos_concept_matches,'pos_concept')\n",
    "if VERSION in [\"SPANNERFLOW\", \"SPANNERFLOW_PYTHON_IE\"]:\n",
    "    sess.import_rel('Docs',pos_concept_docs, scheme=[str, Span, str])\n",
    "else:\n",
    "    sess.import_rel('Docs',pos_concept_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "TargetMatches(Path,Doc, Span, Label) <- \n",
    "    Docs(Path,Doc,\"pos_concept\"),\n",
    "    # TODO CHANGE: on different version\n",
    "    TargetTagRules(Pattern, Label), py_rgx(Pattern,Doc) -> (Span)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "target_matches = sess.export('?TargetMatches(P,D,W,L)')\n",
    "target_rule_docs = rewrite_docs(pos_concept_docs,target_matches,'target_concept')\n",
    "if VERSION in [\"SPANNERFLOW\", \"SPANNERFLOW_PYTHON_IE\"]:\n",
    "    sess.import_rel('Docs',target_rule_docs, scheme=[str, Span, str])\n",
    "else:\n",
    "    sess.import_rel('Docs',target_rule_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "section_tags = pd.read_csv(data_dir/'section_tags.csv',names=['literal','tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# we will programatically build a regex that matches all the section patterns\n",
    "section_delimeter_pattern = section_tags['literal'].str.cat(sep='|')\n",
    "sess.import_var('section_delimeter_pattern',section_delimeter_pattern)\n",
    "section_delimeter_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "# we get section spans and their content using our regex pattern and the py_rgx_split ie function\n",
    "Sections(P,D,Sec,Content)<-Docs(P,D,\"target_concept\"),\n",
    "    py_rgx_split($section_delimeter_pattern,D)->(SecSpan,Content),\n",
    "    as_str(SecSpan)->(Sec).\n",
    "\n",
    "PositiveSections(P,D,Sec,Content)<-Sections(P,D,Sec,Content),SectionTags(Sec,Tag),PositiveSectionTags(Tag).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "Sents(P,S)<-Docs(P,D,\"target_concept\"),split_sentence(D)->(S).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import pairwise\n",
    "\n",
    "def sentence_pairs(text):\n",
    "    yield from pairwise(split_sentence(text))\n",
    "\n",
    "if VERSION in [\"SPANNERFLOW\", \"SPANNERFLOW_PYTHON_IE\"]:\n",
    "    sess.register('sentence_pairs',sentence_pairs,[Span],[Span,Span])\n",
    "else:\n",
    "    sess.register('sentence_pairs',sentence_pairs,[str],[Span,Span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_adjacent(span1,span2):\n",
    "    yield span1.name==span2.name and span1.end +1 == span2.start\n",
    "\n",
    "sess.register('is_adjacent',is_adjacent,[Span,Span],[bool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "SentPairs(P,S1,S2)<-Sents(P,S1),Sents(P,S2),is_adjacent(S1,S2)->(True).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "# first we get the covid mentions and their surrounding sentences, using the py_span_contained ie function\n",
    "# TODO CHANGE: on different version\n",
    "CovidMentions(Path, Span) <- Docs(Path,D,\"target_concept\"), py_rgx(\"COVID-19\",D) -> (Span).\n",
    "\n",
    "# TODO CHANGE: on different version\n",
    "CovidMentionSents(P,Mention,Sent)<-CovidMentions(P,Mention),Sents(P,Sent),py_span_contained(Mention,Sent)->(True).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "\n",
    "# note that for ease of debugging, we extended our head to track which rule a fact was derived from\n",
    "\n",
    "# a tag is positive if it is contained in a positive section\n",
    "CovidTags(Path,Mention,'positive','section')<-\n",
    "    PositiveSections(Path,D,Title,Section),\n",
    "    CovidMentions(Path,Mention),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_span_contained(Mention,Section)->(True).\n",
    "\n",
    "# Context rules tags\n",
    "CovidTags(Path,Mention,Tag,'sentence context')<-\n",
    "    CovidMentionSents(Path,Mention,Sent),\n",
    "    SentenceContextRules(Pattern,Tag,DisambiguationPattern),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_rgx(Pattern,Sent)->(ContextSpan),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_span_contained(Mention,ContextSpan)->(True),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_rgx_is_match(DisambiguationPattern,Sent)->(False).\n",
    "\n",
    "# post processing based on pattern\n",
    "CovidTags(Path,Mention,Tag,'post pattern')<-\n",
    "    CovidMentionSents(Path,Mention,Sent),\n",
    "    PostprocessPatternRules(Pattern,Tag),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_rgx(Pattern,Sent)->(ContextSpan),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_span_contained(Mention,ContextSpan)->(True).\n",
    "\n",
    "# post processing based on pattern and existing attributes\n",
    "# notice the recursive call to CovidTags\n",
    "CovidTags(Path,Mention,Tag,\"post attribute change\")<-\n",
    "    CovidTags(Path,Mention,OldTag,Derivation),\n",
    "    PostprocessRulesWithAttributes(Pattern,OldTag,Tag),\n",
    "    CovidMentionSents(Path,Mention,Sent),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_rgx(Pattern,Sent)->(ContextSpan),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_span_contained(Mention,ContextSpan)->(True).\n",
    "\n",
    "\n",
    "# post processing based on pattern in the next sentence\n",
    "CovidTags(Path,Mention,Tag,\"next sentence\")<-\n",
    "    CovidMentionSents(Path,Mention,Sent),\n",
    "    SentPairs(Path,Sent,NextSent),\n",
    "    PostprocessPatternRules(Pattern,Tag),\n",
    "    # TODO CHANGE: on different version\n",
    "    py_rgx(Pattern,NextSent)->(ContextSpan).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def agg_mention(group):\n",
    "    \"\"\"\n",
    "    aggregates attribute groups of covid spans\n",
    "    \"\"\"\n",
    "    if 'IGNORE' in group:\n",
    "        return 'IGNORE'\n",
    "    elif 'negated' in group and not 'no_negated' in group:\n",
    "        return 'negated'\n",
    "    elif 'future' in group and not 'no_future' in group:\n",
    "        return 'negated'\n",
    "    elif 'other experiencer' in group or 'not relevant' in group:\n",
    "        return 'negated'\n",
    "    elif 'positive' in group and not 'uncertain' in group and not 'no_positive' in group:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'uncertain'\n",
    "\n",
    "#| export\n",
    "def AggDocumentTags(group):\n",
    "    \"\"\"\n",
    "    Classifies a document as 'POS', 'UNK', or 'NEG' based on COVID-19 attributes.\n",
    "    \"\"\"\n",
    "    if 'positive' in group:\n",
    "        return 'POS'\n",
    "    elif 'uncertain' in group:\n",
    "        return 'UNK'\n",
    "    elif 'negated' in group:\n",
    "        return 'NEG'\n",
    "    else:\n",
    "        return 'UNK'\n",
    "\n",
    "\n",
    "sess.register_agg('agg_mention',agg_mention,[str],[str])\n",
    "sess.register_agg('agg_doc_tags',AggDocumentTags,[str],[str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spannerlog\n",
    "AggregatedCovidTags(Path,Mention,agg_mention(Tag))<-\n",
    "    CovidTags(Path,Mention,Tag,Derivation).\n",
    "\n",
    "DocumentTags(Path,agg_doc_tags(Tag))<-\n",
    "    AggregatedCovidTags(Path,Mention,Tag).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "doc_tags = sess.export('?DocumentTags(P,T)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "paths = pd.DataFrame([p.name for p in file_paths],columns=['P'])\n",
    "classification = paths.merge(doc_tags,on='P',how='outer')\n",
    "classification['T']=classification['T'].fillna('UNK')\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Number of Documents: {len(file_paths)}\")\n",
    "print(f\"Time taken: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
