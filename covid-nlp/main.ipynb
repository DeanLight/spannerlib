{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid-19 NLP pipeline\n",
    "\n",
    "### The pipline repository [link](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/)\n",
    "\n",
    "### Inroduction:\n",
    "The primary objective of the NLP pipeline is to identify individuals who have been positively diagnosed with COVID-19 by extracting pertinent information from unstructured free-text narratives found within the Electronic Health Record (EHR) of the Department of Veterans Affairs (VA). By automating this process, the pipeline streamlines the screening of a substantial volume of clinical text, significantly reducing the time and effort required for identification.\n",
    "The pipeline is built on medSpacy framework, and defines a new UI to use.\n",
    "Our goal is to write the pipline in rgxlog language so we show a real world example about the benefits of the rgxlog framework from the NLP world.\n",
    "\n",
    "### pipline stages:\n",
    "- [Concept tagger](#concept-tag-rules)\n",
    "- [Target matcher](#target-rules)\n",
    "- [Context matcher](#context-rules)\n",
    "- Sectionizer\n",
    "- Postprocessor\n",
    "- Document classification\n",
    "\n",
    "We will implement each stage separately later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we need to install some requirements to work with [medspacy](https://github.com/medspacy/medspacy) framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import what we need from the rgxlog framework and define some ie functions that will be used in every stage of the pipline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import rgxlog\n",
    "from rgxlog import magic_session\n",
    "from rgxlog import Session\n",
    "from rgxlog.engine.datatypes.primitive_types import DataTypes\n",
    "from rgxlog.engine.datatypes.primitive_types import Span\n",
    "session = rgxlog.magic_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(text_path):\n",
    "    \"\"\"\n",
    "    Reads from file and return it's content.\n",
    "\n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to read from.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file.\n",
    "    \"\"\"\n",
    "    with open(f\"{text_path}\", 'r') as file:\n",
    "        content = file.read()\n",
    "    yield content\n",
    "magic_session.register(ie_function=read_from_file,\n",
    "                       ie_function_name = \"read_from_file\",\n",
    "                       in_rel=[DataTypes.string],\n",
    "                       out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_interval_conflicts(replacements):\n",
    "    \"\"\"\n",
    "    This function takes a list of replacements, where each replacement is represented\n",
    "    as a list containing a label and a span (interval). It checks for conflicts among\n",
    "    the intervals and returns a list of resolved replacements, ensuring that no two\n",
    "    intervals overlap.\n",
    "\n",
    "    Parameters:\n",
    "    replacements (list of lists): A list of replacements, where each replacement\n",
    "        is represented as a list [label, span].\n",
    "\n",
    "    Returns:\n",
    "    list of lists: A list of resolved replacements, where each replacement is a list\n",
    "        [label, span], ensuring that there are no conflicts among intervals.\n",
    "    \"\"\"\n",
    "    # Sort the replacements by the size of the spans in descending order\n",
    "    replacements.sort(key=lambda x: x[1].span_end - x[1].span_start, reverse=True)\n",
    "\n",
    "    # Initialize a list to keep track of intervals that have been replaced\n",
    "    resolved_replacements = []\n",
    "    \n",
    "    for label, span in replacements:\n",
    "        conflict = False\n",
    "\n",
    "        for _, existing_span in resolved_replacements:\n",
    "            existing_start = existing_span.span_start\n",
    "            existing_end = existing_span.span_end\n",
    "\n",
    "            if not (span.span_end <= existing_start or span.span_start >= existing_end):\n",
    "                conflict = True\n",
    "                break\n",
    "\n",
    "        if not conflict:\n",
    "            resolved_replacements.append([label, span])\n",
    "\n",
    "    return resolved_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_spans(spans_table, paths_table):\n",
    "    \"\"\"\n",
    "    This function takes tables a spans tables and path table for the files paths,\n",
    "    it generate queries for the tables, executes the queries using a session, processes the results, \n",
    "    and replaces specific spans in a text with the corresponding labels, it first\n",
    "    resolve spans overlapping conflicts for each giving path.\n",
    "\n",
    "    Parameters:\n",
    "    spans_table (str): A string representing the spans table to process, table columns are formated as (Label, Span, Path).\n",
    "    paths_table (str): A string representing the paths table to process, table columns are formated as (Path)\n",
    "\n",
    "    Returns:\n",
    "    str: The adjusted text string with the new labels.\n",
    "    \"\"\"\n",
    "    # Get a list of all the paths\n",
    "    paths = session.run_commands(f\"?{paths_table}(Path)\", print_results=False, format_results=True)\n",
    "    paths = paths[0].values.tolist()\n",
    "    for path_list in paths:\n",
    "        path = path_list[0]\n",
    "\n",
    "        # Generate a spans query for each path, the query will be formates as (Label, Span, Path)\n",
    "        results = session.run_commands(f'?{spans_table}(Label, Span, \"{path}\")', print_results=True, format_results=True)\n",
    "        if len(results[0]) == 0:\n",
    "            return \" \"\n",
    "        # replacments is list of lists where each list is a [Label, Span]\n",
    "        replacements = results[0].values.tolist()\n",
    "        \n",
    "        with open(f\"{path}\", 'r') as file:\n",
    "            adjusted_string = file.read()\n",
    "    \n",
    "        # Resolve spans conflicts\n",
    "        resolved_replacements = resolve_interval_conflicts(replacements)\n",
    "    \n",
    "        # Sort the resolved replacements by the starting index of each span in descending order\n",
    "        resolved_replacements.sort(key=lambda x: x[1].span_start, reverse=True)\n",
    "    \n",
    "        # iterate over the resolved query results and replace the space with the corresponding label\n",
    "        for i in range(len(resolved_replacements)):\n",
    "            replace_string, span = resolved_replacements[i]\n",
    "            replace_length = len(replace_string)\n",
    "            adjusted_string = adjusted_string[:span.span_start] + replace_string + adjusted_string[span.span_end:]\n",
    "    \n",
    "        with open(f\"{path}\", 'w') as file:\n",
    "            file.writelines(adjusted_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The paths of the text files to be classified should be written in \"files_paths.csv\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1.txt\n",
      "sample2.txt"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat files_paths.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"files_paths.csv\", relation_name=\"FilesPaths\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                               Content\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                            The patient be tested negative for COVID-19.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "FilesContent(Path, Content) <- FilesPaths(Path), read_from_file(Path) -> (Content)\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='concept-tag-rules'></a>\n",
    "### [Concept Tag Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/concept_tag_rules.py):\n",
    "Concept tag rules, also known as pattern-based rules or custom rules, are a way to specify and define patterns that an NLP (Natural Language Processing) system should recognize within text data. These rules are used to identify specific concepts or entities within text documents. In the context of MedSpaCy and medical NLP, concept tag rules are often used to identify medical entities and concepts accurately.\n",
    "\n",
    "In the orginal project they used the TargetRule class which defines a rule for identifying a specific concept or entity in text.\n",
    "each concept Target Rule looks like this:\n",
    "\n",
    "TargetRule(\n",
    "            literal=\"coronavirus\",\n",
    "            category=\"COVID-19\",\n",
    "            pattern=[{\"LOWER\": {\"REGEX\": \"coronavirus|hcov|ncov$\"}}],\n",
    "          )\n",
    "\n",
    "**Literal** : This specifies the literal text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the category or label associated with the identified entity\n",
    "\n",
    "**Pattern** : This defines the pattern or conditions under which the entity should be recognized. It's a list of dictionaries specifying conditions for token matching. These rules some times used lemma attribute or POS of each token. A documentation can be found at : https://spacy.io/usage/rule-based-matching\n",
    "\n",
    "Instead what we did is to define regex patterns, we have added these pattern in concept_target_rules.csv file, there are two types of these patterns lemma and pos, that we will implement each later on.\n",
    "Each rule in the csv file is like this : regexPattern, label, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:hcov|covid(?:(?:-)?(?:\\s)?19|10)?|2019-cov|cov2|ncov-19|covd 19|no-cov|sars cov),COVID-19,lemma\n",
      "(?i)(?:coivid|(?:novel )?corona(?:virus)?(?: (?:20)?19)?|sars(?:\\s)?(?:-)?(?:\\s)?cov(?:id)?(?:-)?(?:2|19)),COVID-19,lemma\n",
      "(?i)(?:\\+(?: ve)?|\\(\\+\\)|positive|\\bpos\\b|active|confirmed),positive,lemma\n",
      "(?i)(?:pneum(?:onia)?|pna|hypoxia|septic shoc|ards\\(?(?:(?:[12])/2)\\)?|(?:hypoxemic|acute|severe)? resp(?:iratory)? failure(?:\\(?(?:[12]/2)\\)?)?)\",associated_diagnosis,lemma\n",
      "(?i)(?:(?:diagnos(?:is|ed)|dx(\\.)?)(?:of|with)?),diagnosis,lemma\n",
      "(?i)(?:^screen),screening,lemma\n",
      "(?i)(?:in contact with|any one|co-worker|at work|(?:the|a)(?:wo)?man|(?:another|a) (?:pt|patient|pt\\.)),other_experiencer,lemma\n",
      "(?i)(?:patient|pt(?:\\.)?|vt|veteran),patient,lemma\n",
      "(?i)(?:like_num (?:days|day|weeks|week|months|month) (?:ago|prior)),timesx,lemma\n",
      "(?i)(?:(?:antibody|antibodies|ab) test),antibody test,lemma\n",
      "(?i)(?:(?:coronavirus|hcovs?|ncovs?|covs?)(?:\\s)?(?:-)?(?:\\s)?(?: infection)?(?: strain)?(?:\\s)?(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63) (?:coronavirus|hcovs?|ncovs?|covs?)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:non(?:\\s)?(?:-)?(?:\\s)?(?:novel|covid|ncovid|covid-19)(?: coronavirus)?|other coronavirus),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:wife|husband|spouse|family|member|girlfriend|boyfriend|mother|father|nephew|niece|grandparent|grandparents|granddaughter|relative|relatives|caregiver),family,pos\n",
      "(?i)(?:grandchild|grandson|cousin|grandmother|grandfather|parent|son|daughter|mom|dad|brother|sister|aunt|uncle|child|children|sibling|siblings),family,pos\n",
      "(?i)(?:someone|somebody|person|anyone|anybody|people|individual|individuals|teacher|anybody|employees|employer|customer|client|residents),other_experiencer,pos\n",
      "(?i)(?:resident|pts|patients|coworker|coworkers|workers|colleague|captain|captains|pilot|pilots|sailor|sailors|meeting),other_experiencer,pos\n",
      "(?i)(?:boyfriend|persons|person|church|convention|guest|party|attendee|conference|roommate|friend|friends|coach|player|neighbor|manager|boss),other_experiencer,pos\n",
      "(?i)(?:cashier|landlord|worked|works|^mate|nobody|mates|housemate|housemates|hotel|soldier|airport|tsa|lady|ladies|lobby|staffer|staffers),other_experiencer,pos"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat concept_tags_rules.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma Rules:\n",
    "Lemma rules are rules that used the attribute _lemma of each token in the NLP, so what we defined this function to lemmatize the text, most of the rules used only the raw text, thats why we decided that only to lemmatize the tokens we needed.\n",
    "\n",
    "Example for a lemma rule from the original NLP:\n",
    "\n",
    "        TargetRule(\n",
    "            \"results positive\",\n",
    "            \"positive\",\n",
    "            pattern=[\n",
    "                {\"LOWER\": \"results\"},\n",
    "                {\"LEMMA\": \"be\", \"OP\": \"?\"},\n",
    "                {\"LOWER\": {\"IN\": [\"pos\", \"positive\"]}},\n",
    "            ],\n",
    "        ),\n",
    "We used the py_rgx_span to capture the patterns, and will use the spans later on in replace_spans that will replace each span with the correct label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text_path, lemma_words_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, lemmatizes its content using spaCy's English language model,\n",
    "    and replaces certain words with their lemmas the rest will remain the same. The updated text is then written back to the same file.\n",
    "\n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be lemmatized.\n",
    "        lemma_words_path(str): The path that contains the list of words to be lemmatized\n",
    "\n",
    "    Returns:\n",
    "        str: The lemmatized text.\n",
    "    \"\"\"\n",
    "    # Define a list of words to be lemmatized\n",
    "    lemma_words = [line.strip() for line in open(f\"{lemma_words_path}\") if line.strip()]\n",
    "\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    lemmatized_text = \"\"\n",
    "    for token in doc:\n",
    "        if token.lemma_ in lemma_words:\n",
    "            lemmatized_text += token.lemma_\n",
    "        elif token.like_num:\n",
    "            lemmatized_text += \"like_num\"\n",
    "        else:\n",
    "            lemmatized_text += token.text\n",
    "        lemmatized_text += \" \"\n",
    "\n",
    "    # Write the lemmatized text back to the same file\n",
    "    with open(text_path, 'w') as file:\n",
    "        file.writelines(lemmatized_text)\n",
    "\n",
    "    yield lemmatized_text\n",
    "magic_session.register(ie_function=lemmatize_text, ie_function_name = \"lemmatize_text\", in_rel=[DataTypes.string, DataTypes.string], out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'lemma_texts(Path, LemmaText)':\n",
      "    Path     |                                                              LemmaText\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                            The patient be tested negative for COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "lemma_texts(Path, LemmaText) <- FilesPaths(Path), lemmatize_text(Path, \"lemma_words.txt\") -> (LemmaText)\n",
    "?lemma_texts(Path, LemmaText)\n",
    "\n",
    "LemmaMatches(Label, Span, Path) <- lemma_texts(Path, Text), ConceptTagRules(Pattern, Label, \"lemma\"), py_rgx_span(Text, Pattern) -> (Span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'LemmaMatches(Label, Span, \"sample1.txt\")':\n",
      "  Label   |    Span\n",
      "----------+------------\n",
      " positive | [121, 129)\n",
      " positive |  [70, 78)\n",
      " COVID-19 |  [34, 42)\n",
      " COVID-19 |  [83, 91)\n",
      " COVID-19 | [94, 102)\n",
      " patient  |   [0, 7)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample2.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " COVID-19 | [35, 43)\n",
      " patient  | [4, 11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace the matches with the correct label\n",
    "replace_spans(\"LemmaMatches\", \"FilesPaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                               Content\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                            The patient be tested negative for COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Rules:\n",
    "As we mentioned above these rules used the POS attribute of each token, there were a small number of rules so we only used this to the tokens we needed.\n",
    "Example of the a rule from the original NLP:\n",
    "\n",
    "        TargetRule(\n",
    "            \"other experiencer\",\n",
    "            category=\"other_experiencer\",\n",
    "            pattern=[\n",
    "                {\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]},\n",
    "                    \"LOWER\": {\n",
    "                        \"IN\": [\n",
    "                            \"someone\",\n",
    "                            \"somebody\",\n",
    "                            \"person\",\n",
    "                            \"anyone\",\n",
    "                            \"anybody\",\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "The patterns we defined will capture what should be after the POS token, so we have used two ie functions, the first one will be used to know the POS of each token, then we used py_rgx_span to capture the patterns we defined, and we checked if the POS token is right before the captured patterns, if so then thats a match for the rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text_with_pos(text_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, processes its content using spaCy's English language model,\n",
    "    and returns a tuple of (POS, Span) for each token if it's one of NOUN|PROPN|PRON|ADJ\n",
    "    otherwise an empty tuple will be returned\n",
    "    \n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be annotated.\n",
    "\n",
    "    Returns:\n",
    "        tuple(str, Span): The POS of the token and it's span\n",
    "    \"\"\"\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]:\n",
    "            yield token.pos_, Span(token.idx, token.idx + len(token.text))\n",
    "        else:\n",
    "            yield tuple()\n",
    "magic_session.register(ie_function=annotate_text_with_pos, ie_function_name = \"annotate_text_with_pos\", in_rel=[DataTypes.string], out_rel=[DataTypes.string, DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_next_span(first_span, second_span):\n",
    "    \"\"\"\n",
    "    Determine if two spans are adjacent in a text sequence and return a new span\n",
    "    representing their combination if they are.\n",
    "\n",
    "    Parameters:\n",
    "        first_span (Span): The first span to be checked.\n",
    "        second_span (Span): The second span to be checked.\n",
    "\n",
    "    Returns:\n",
    "        a new Span representing their combination if they are adjacent or empty Span otherwise\n",
    "\n",
    "    \"\"\"\n",
    "    if (first_span.span_end + 1) == second_span.span_start:\n",
    "        yield Span(first_span.span_start, second_span.span_end)\n",
    "    else:\n",
    "        yield tuple() \n",
    "magic_session.register(ie_function=is_next_span,\n",
    "                       ie_function_name = \"is_next_span\",\n",
    "                       in_rel=[DataTypes.span, DataTypes.span],\n",
    "                       out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'POSTable(POS, Span, Path)':\n",
      "  POS  |    Span    |    Path\n",
      "-------+------------+-------------\n",
      "  ADJ  |   [0, 7)   | sample1.txt\n",
      "  ADJ  | [121, 129) | sample1.txt\n",
      "  ADJ  |  [70, 78)  | sample1.txt\n",
      " NOUN  | [103, 110) | sample1.txt\n",
      " NOUN  |  [49, 53)  | sample1.txt\n",
      " NOUN  |  [8, 16)   | sample1.txt\n",
      " PROPN |  [83, 91)  | sample1.txt\n",
      "  ADJ  |  [22, 30)  | sample2.txt\n",
      " NOUN  |  [4, 11)   | sample2.txt\n",
      " PROPN |  [35, 43)  | sample2.txt\n",
      "\n",
      "printing results for query 'POSMatches(Label, Span, Path)':\n",
      "  Label  |   Span   |    Path\n",
      "---------+----------+-------------\n",
      " family  | [49, 53) | sample1.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "POSTable(POS, Span, Path) <- lemma_texts(Path, Text), annotate_text_with_pos(Path) -> (POS, Span)\n",
    "?POSTable(POS, Span, Path)\n",
    "\n",
    "POSMatches(Label, Span, Path) <- lemma_texts(Path, Text), ConceptTagRules(Pattern, Label, \"pos\"), py_rgx_span(Text, Pattern) -> (Span)\n",
    "?POSMatches(Label, Span, Path)\n",
    "\n",
    "POSRuleMatches(Label, Span, Path) <- POSTable(POS, FirstSpan, Path), POSMatches(Label, SecondSpan, Path), is_next_span(FirstSpan, SecondSpan) -> (Span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'POSRuleMatches(Label, Span, \"sample1.txt\")':\n",
      "[]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the matches with the correct label\n",
    "replace_spans(\"POSRuleMatches\", \"FilesPaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                               Content\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                            The patient be tested negative for COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='target-rules'></a>\n",
    "### [Target Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/target_rules.py):\n",
    "These rules used the label that was assigned through the concept tagger, to capture some more complex patterns and assign a label for them inorder to decremnt the cases of false positive.\n",
    "Each rule look like this:\n",
    "\n",
    "        TargetRule(\n",
    "            literal=\"coronavirus screening\",\n",
    "            category=\"IGNORE\",\n",
    "            pattern=[\n",
    "                {\"_\": {\"concept_tag\": \"COVID-19\"}},\n",
    "                {\"LOWER\": {\"IN\": [\"screen\", \"screening\", \"screenings\"]}},\n",
    "            ],\n",
    "        ),\n",
    "Since we replaced the spans we found with the corresponding label we didn't need the concept_tag attribute of the token/span.\n",
    "To ease the patterns we have devided them into two groups PreTargetRules and TargetRules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreTargetRules: \n",
    "To ease the process, we have implemented preTarget rules aimed at squash Ø¤onsecutive identical labels assigned through the concept tagger into a single label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:COVID-19(?: COVID-19)*),COVID-19\n",
      "(?i)(?:positive(?: positive)*),positive\n",
      "(?i)(?:patient(?: patient)*),patient\n",
      "(?i)(?:other_experiencer(?: other_experiencer)*),other_experiencer\n",
      "(?i)(?:screening(?: screening)*),screening"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat pre_target_rules.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"pre_target_rules.csv\", relation_name=\"PreTargetTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PreTargetMatches(Label, Span, Path)':\n",
      "  Label   |    Span    |    Path\n",
      "----------+------------+-------------\n",
      " COVID-19 |  [35, 43)  | sample2.txt\n",
      " COVID-19 |  [34, 42)  | sample1.txt\n",
      " COVID-19 |  [83, 91)  | sample1.txt\n",
      " COVID-19 | [94, 102)  | sample1.txt\n",
      " positive | [121, 129) | sample1.txt\n",
      " positive |  [70, 78)  | sample1.txt\n",
      " patient  |  [4, 11)   | sample2.txt\n",
      " patient  |   [0, 7)   | sample1.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "PreTargetMatches(Label, Span, Path) <- lemma_texts(Path, Text), PreTargetTagRules(Pattern, Label), py_rgx_span(Text,Pattern) -> (Span)\n",
    "?PreTargetMatches(Label, Span, Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PreTargetMatches(Label, Span, \"sample1.txt\")':\n",
      "  Label   |    Span\n",
      "----------+------------\n",
      " COVID-19 |  [34, 42)\n",
      " COVID-19 |  [83, 91)\n",
      " COVID-19 | [94, 102)\n",
      " positive | [121, 129)\n",
      " positive |  [70, 78)\n",
      " patient  |   [0, 7)\n",
      "\n",
      "printing results for query 'PreTargetMatches(Label, Span, \"sample2.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " COVID-19 | [35, 43)\n",
      " patient  | [4, 11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_spans(\"PreTargetMatches\", \"FilesPaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                               Content\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                            The patient be tested negative for COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(COVID-19 positive (?:unit|floor)|positive COVID-19 (?:unit|floor|exposure)),COVID-19\n",
      "(?i)(known(?: positive)? COVID-19(?: positive)? (?:exposure|contact)),COVID-19\n",
      "(?i)(COVID-19 positive screening|positive COVID-19 screening|screening COVID-19 positive|screening positive COVID-19),positive coronavirus screening\n",
      "(?i)(diagnosis : COVID-19 (?:test|screening)),COVID-19\n",
      "(?i)(COVID-19 screening),coronavirus screening\n",
      "(?i)(active COVID-19 precaution),IGNORE\n",
      "(?i)(COVID-19 (?:restriction|emergency|epidemic|outbreak|crisis|breakout|pandemic|spread|screening)|droplet (?:isolation )?precaution),IGNORE\n",
      "(?i)(contact precautions|positive (?:for )?(?:flu|influenza)|positive (?:patient|person)|confirm (?:with|w/(?:/)?|w)|the (?:positive )?case),IGNORE\n",
      "(?i)(positive cases|results (?:are )?confirmed|exposed to positive|(?:neg|pos) pressure|a positive case|positive (?:attitude|feedback|serology)|[ ] COVID-19),IGNORE\n",
      "(?i)(has (?:the )?patient been diagnosed (?:with|w/(?:/)?|w)|(?:person|patient) with confirmed covid-19),IGNORE\n",
      "(?i)(COVID-19 positive (?:tested )?other_experiencer),COVID-19\n",
      "(?i)(age 65(?: )?\\+|(?:return|back) to work|in order to decrease the spread of the COVID-19 infection|COVID-19 (?:guidelines|(?:infection )?rate)),IGNORE\n",
      "(?i)(COVID-19 positive (?:patient|person|people|veteran)),OTHER_PERSON\n",
      "(?i)(positive COVID-19 (?:tested )?other_experiencer),COVID-19\n",
      "(?i)((?:(?:contact|exposure) (?:with|to)? )?positive COVID-19 (?:patient|person|veteran)),OTHER_PERSON\n",
      "(?i)((?:patient|person) (?:who|that) test (?:positive|confirm) for COVID-19),OTHER_PERSON\n",
      "(?i)(ref : not detected),IGNORE\n",
      "(?i)(does not know|history of present illness|but|therefore|flu|metapneumovirus|;),IGNORE\n",
      "(?i)(social worker|initially negative|likely recovered|not aware|positive (?:cases|symptom|sign)|client history|emergency contact|several positive|special instructions :),IGNORE"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat target_rules.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"target_rules.csv\", relation_name=\"TargetTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%rgxlog\n",
    "TargetTagMatches(Label, Span, Path) <- lemma_texts(Path, Text), TargetTagRules(Pattern, Label), py_rgx_span(Text,Pattern) -> (Span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'TargetTagMatches(Label, Span, \"sample1.txt\")':\n",
      "[]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_spans(\"TargetTagMatches\", \"FilesPaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                               Content\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                            The patient be tested negative for COVID-19 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='context-rules'></a>\n",
    "### [Context Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/context_rules.py):\n",
    "These rules assign an attribute for each COVID-19 label based on the context, these attributes will be used later to classify each text.\n",
    "\n",
    "Example for this rule is: \n",
    "\n",
    "    ConTextRule(\n",
    "        literal=\"Not Detected\",\n",
    "        category=\"NEGATED_EXISTENCE\",\n",
    "        direction=\"BACKWARD\",\n",
    "        pattern=[\n",
    "            {\"LOWER\": {\"IN\": [\"not\", \"non\"]}},\n",
    "            {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "            {\"TEXT\": \"-\", \"OP\": \"?\"},\n",
    "            {\"LOWER\": {\"REGEX\": \"detecte?d\"}},\n",
    "        ],\n",
    "        allowed_types={\"COVID-19\"},\n",
    "    ),\n",
    "   **direction** specify if the allowed_types should be before or after the pattern,\n",
    "   **allowed_types** specify on what labels should this rule be applied on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:positive COVID-19|COVID-19 (?:\\([^)]*\\)) (?:positive|detected)|COVID-19(?: positive)? associated_diagnosis)#positive\n",
      "(?i)(?:COVID-19 status : positive)#positive\n",
      "(?i)(?:associated_diagnosis COVID-19|associated_diagnosis (?:with|w|w//|from) (?:associated_diagnosis )?COVID-19)#positive\n",
      "(?i)(?:COVID-19 positive(?: patient| precaution)?|associated_diagnosis (?:due|secondary) to COVID-19)#positive\n",
      "(?i)(?:(?:current|recent) COVID-19 diagnosis)#positive\n",
      "(?i)(?:COVID-19 (?:- )?related (?:admission|associated_diagnosis)|admitted (?:due to|(?:with|w|w/)) COVID-19)#positive\n",
      "(?i)(?:COVID-19 infection|b34(?:\\.)?2|b97.29|u07.1)#positive\n",
      "(?i)(?:COVID-19 eval(?:uation)?|(?:positive )? COVID-19 symptoms|rule out COVID-19)#uncertain\n",
      "(?i)(?:patient (?:do )?have COVID-19)#positive\n",
      "(?i)(?:diagnosis : COVID-19(?: (?:test|screen)(?:ing|ed|s)? positive)?(?: positive)?)#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:not|non) (?:- )?detecte?d)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,1} negative screening|negative screening(?: (?!IGNORE)\\w+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,2} : negative)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:not be|none) detected)#negated\n",
      "(?i)(?:free from(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? not (?:be )?tested)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,4} not indicated)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? NEGATIVE NEG)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,3} negative test)#negated\n",
      "(?i)(?:negative test(?: (?!IGNORE)\\w+){0,3} COVID-19)#negated\n",
      "(?i)(?:without any(?: (?!IGNORE)\\w+){0,1} COVID-19)#negated\n",
      "(?i)(?:denie(?:s|d)(?: any| travel)?(?: (?!IGNORE)\\w+){0,9} COVID-19)#negated\n",
      "(?i)(?:no (?:evidence(?: of)?|(?:hx|-hx|history) of|diagnosis (?:of)?)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:no(?: (?!IGNORE)\\w+){0,1} COVID-19)#negated\n",
      "(?i)(?:no (?:positive|one|residents|confirm case|contact(?: w/?(?:ith)?$))(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? no confirm case)#negated\n",
      "(?i)(?:(?:no|n't) (?:be )? confirm(?: (?!IGNORE)\\w+){0,1} COVID-19)#negated\n",
      "(?i)(?:(?:no known|not have)(?: (?!IGNORE)\\w+){0,4} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:answer(?:ed|s|ing)? (?:no|negative|neg)|negative))#negated\n",
      "(?i)(?:(?:answer(?:ed|s|ing)? (?:no|negative|neg)|(?:neg|negative)(?: for)?)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:not positive(?: (?!IGNORE)\\w+)*? COVID-19)#negatedtest\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? not positive)#negated\n",
      "(?i)(?:excluded(?: (?!IGNORE)\\w+){0,3} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,3} excluded)#negated\n",
      "(?i)(?:no risk factor for(?: (?!IGNORE)\\w+){0,4} COVID-19)#uncertain\n",
      "(?i)(?:negative screening(?: for)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? screening (?:negative|neg))#negated\n",
      "(?i)(?:(?:screening (?:negative|neg) for|do (?:not|n't) have (?:any )?(?:signs|symptoms|ss|s/s))(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? do not screening positive)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:be negative|not test positive))#negated\n",
      "(?i)(?:(?:be negative|not test positive|not? screening(?: for)|no signs of|no (?:sign|symptom|indication(?:of|for)?)|not? test(?:ing|ed|s)? for)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:no exposure|(?:without|w/o) (?:signs|symptoms)(?:or (?:signs|symptoms))|do)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,4} not have)#negated\n",
      "(?i)(?:(?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria|no concern (?:for|of)|not? (?:at )risk)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria))#negated\n",
      "(?i)(?:(?:no suspicion(?: for)|not suspect|ruled out for|no(?: recent) travel|not be in|clear(?:ed|s|ing) (?:of|for|from))(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:not(?: (?!IGNORE)\\w+){0,3} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:be ruled out|be not likely|not have contact with))#negated\n",
      "(?i)(?:(?:no (?:hx|history) (?:of )travel|not have contact with|no symptoms of|no risk factors|no (?:confirm case|report))(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:no (?:exposure|contact) (?:to|with)|not test(?:ed|ing|s) positive))#negated\n",
      "(?i)(?:(?:no (?:exposure|contact) (?:to|with)|do (?:not|n't) meet(?: screening)(?: criteria)(?: for)|not test(?:ed|ing|s) positive(?: for)|not tested(?: or diagnosis))(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:no|any)(?: known) contact(?: with)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,3} : no)#negated\n",
      "(?i)(?:(?:(?:not|never) diagnosis with|not been tested (?:for )?or diagnosis with)(?: (?!IGNORE)\\w+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,1} confirm)#positive\n",
      "(?i)(?:(?:confirm|known)(?: (?!IGNORE)\\w+){0,1} COVID-19)#positive\n",
      "(?i)(?:(?:(?:test(?:ed|s|ing)?)?positive(?: for)?|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status))(?: (?!IGNORE)\\w+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:positiv(?:e|ity)|test(?:ed|s|ing)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,2} (?:positive status|results be positive))#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,4} results positive)#positive\n",
      "(?i)(?:results positive(?: (?!IGNORE)\\w+){0,4} COVID-19)#positive\n",
      "(?i)(?:notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status)(?: (?!IGNORE)\\w+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status))#positive\n",
      "(?i)(?:likely secondary to(?: (?!IGNORE)\\w+){0,0} COVID-19)#positive\n",
      "(?i)(?:(?:problem(?: list)? (?:of|:)|(?:active|current|acute) problems :|admi(?:t|ssion) diagnosis(?: :)?)(?: (?!IGNORE)\\w+){0,9} COVID-19)#positive\n",
      "(?i)(?:(?:reason for admission :|treatment of|(?:admitting )diagnosis(?: :)?)(?: (?!IGNORE)\\w+){0,3} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,3} diagnosis like_num)#positive\n",
      "(?i)(?:(?:Reason for admission :|inpatient with|discharged from|in m?icu (?:for|with))(?: (?!IGNORE)\\w+){0,5} COVID-19)#positive\n",
      "(?i)(?:(?:admit(?:ted|s|ting) like_num|admitted (?:to|on|for)|Reason for ICU :|admission for)(?: (?!IGNORE)\\w+)*? COVID-19)#positive\n",
      "(?i)(?:Reason for ED visit or Hospital Admission :(?: (?!IGNORE)\\w+){0,1} COVID-19)#positive\n",
      "(?i)(?:(?:(?:in|to) (?:the )(?:hospital|icu|micu) (?:for|due to)|hospitali(?:zed)?(?: timesx)? (?:for|due to))(?: (?!IGNORE)\\w+){0,4} COVID-19)#positive\n",
      "(?i)(?:(?:diagnosis with|found to be positive for)(?: (?!IGNORE)\\w+){0,5} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,5} found to be positive)#positive\n",
      "(?i)(?:(?:positive test|presum(?:e|ed|es|ing) positive|not(?: yet)? recover(?:s|ing|ed)?)(?: (?!IGNORE)\\w+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:positive test|presum(?:e|ed|es|ing) positive))#positive\n",
      "(?i)(?:(?:management of|ards(?: (?:from|with|secondary to))?|acute respiratory distress|post - extubation)(?: (?!IGNORE)\\w+){0,2} COVID-19)#positive\n",
      "(?i)(?:(?:in(?: the)? setting of|in the s / o|found to have|present(?:s|ed|ing)? with)(?: (?!IGNORE)\\w+){0,5} COVID-19)#positive\n",
      "(?i)(?:resp(?:iratory) failure(?:(?: (?:with|due to))?|like_num|\\( like_num \\))(?: (?!IGNORE)\\w+){0,3} COVID-19)#positive\n",
      "(?i)(?:(?:active(?: for)|recovering from)(?: (?!IGNORE)\\w+){0,1} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,2} recovering from)#positive\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,4} (?:detected|value : detected|POSITIVEH))#positive\n",
      "(?i)(?:(?:\\d+(?: )?-|like_num )year(?:(?: )?-(?: )?old| old) (?:(?:aa|white|black|hispanic|caucasian) )?(?:\\b(?!family\\b|other_experiencer\\b)\\w+\\b )?(?:with|w|w/|admitted)(?: (?!IGNORE)\\w+){0,9} COVID-19)#patient_experiencer\n",
      "(?i)(?:(?:like_num (?:y[or]|y / o)|[\\d]+yo) (?:\\b(?!family\\b|other_experiencer\\b)\\w+\\b )?(?:patient |veteran )?(?:with|w|w/)(?: (?!IGNORE)\\w+){0,9} COVID-19)#patient_experiencer\n",
      "(?i)(?:the (?:veteran|vet|patient) have(?: (?!IGNORE)\\w+){0,2} COVID-19)#patient_experiencer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,1} precaution)#negated\n",
      "(?i)(?:(?:(?:precaution|protection|protect) (?:for|against)|concern about|reports of|vaccine|protect yourself|prevent(?:ed|ion|s|ing)|avoid)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:prevent(?:ed|ion|s|ing)|vaccine|education(?:ion|ed|ing|ed)?|instruction))#negated\n",
      "(?i)(?:(?:questions (?:about|regarding|re|concerning|on|for)|(?:anxiety|ask(?:ing|ed|es|ed)?) about|educat(?:ion|ed|ing|ed)?|instruction)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:information(?: )?(?:on|about|regarding|re)?|protocols?)(?: (?!IGNORE)\\w+){0,2} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,2} protocols?)#negated\n",
      "(?i)(?:(?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?|(?:anxious|worr(?:ied|ies|y|ying)) (?:about|re|regarding))(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?))#negated\n",
      "(?i)(?:if(?: (?!IGNORE)\\w+){0,9} COVID-19)#negated\n",
      "(?i)(?:(?:have you|will(?: be) travel|travel plan|if you need|plan to travel|mers)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:This patient was screened for the following suspected travel related illness(?:es)?)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:limit|reduce|lower|minimize)(?: the)? (?:risk|chance|possibility) of|if you)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:(?:-)?hx|history|) of)(?: (?!IGNORE)\\w+){0,3} COVID-19)#negated\n",
      "(?i)(?:(?:^(?:check|test|retest|eval)(?: for)?)(?: (?!IGNORE)\\w+)*? COVID-19)#test\n",
      "(?i)(?:(?:work(?:-|\\s)up)(?: (?!IGNORE)\\w+)*? COVID-19)#test\n",
      "(?i)(?:(?:evaluation)(?: (?!IGNORE)\\w+){0,1} COVID-19)#test\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,1} (?:evaluation))#test\n",
      "(?i)(?:(?:swab|PCR|specimen sent)(?: (?!IGNORE)\\w+)*? COVID-19)#test\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:swab|PCR|specimen sent))#test\n",
      "(?i)(?:(?:awaiting results|at risk for|risk for|currently being ruled out or has tested positive for|to exclude)(?: (?!IGNORE)\\w+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:awaiting results|currently being ruled out or has tested positive for|(?:patient|person) of interest))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,0} (?:risk))#uncertain\n",
      "(?i)(?:(?:investigation of)(?: (?!IGNORE)\\w+){0,0} COVID-19)#uncertain\n",
      "(?i)(?:(?:question of|differential diagnosis :|ddx :)(?: (?!IGNORE)\\w+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:awaiting|questionnaire|r(?:/)?o(?:\\.)?)(?: (?!IGNORE)\\w+){0,1} COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,1} (?:awaiting|questionnaire|r(?:/)?o(?:\\.)?))#uncertain\n",
      "(?i)(?:(?:under investigation|(?:may|might) be positive(?: for)?|flew|tarvel(?:ed)?|travelled)(?: (?!IGNORE)\\w+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:under investigation|(?:may|might) be positive))#uncertain\n",
      "(?i)(?:(?:facility (?:with|has)(?: a)?|known to have|(?:same )?room|patients with)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:area|county|community|city) (?:with|of)|in the building|(?:several|multiple|one)(?:of )?(?:the )? other_experiencer)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:in the building))#negated\n",
      "(?i)(?:(?:(?:he|she) thinks (?:he|she) (?:have|had|has)|\\S+ would like)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|unlikely to be)(?: (?!IGNORE)\\w+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|possible positive))#uncertain\n",
      "(?i)(?:(?:(?:possible|potential)? exposure|possibly|possible positive)(?: (?!IGNORE)\\w+)*? COVID-19)#uncertain\n",
      "(?i)(?:(?:risk of|likely|probable|probably)(?: (?!IGNORE)\\w+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:suspicion(?: for)?|^suspect|differential diagnosis|ddx(?: :)?|doubt)(?: (?!IGNORE)\\w+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:suspicion|^suspect|differential diagnosis|ddx(?: :)?|may have been exposed))#uncertain\n",
      "(?i)(?:(?:(?:positive )?(?:sign|symptom) of)(?: (?!IGNORE)\\w+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:sx|s/s|rule (?:- )out|be ruled out(?: for)?|^(?:vs\\.?|versus)$)(?: (?!IGNORE)\\w+){0,4} COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,4} (?:sx|s/s|rule (?:- )out|^(?:vs\\.?|versus)$))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:(?:possible|potential)? exposure|may have been exposed))#uncertain\n",
      "(?i)(?:(?:concern(:?s)?(?: for| of)?|if (?:negative|positive)|c/f|assess(?:ed)? for|concerning for)(?: (?!IGNORE)\\w+)*? COVID-19)#uncertain\n",
      "(?i)(?:(?:unlikely(?: to be positive)?|low (?:suspicion|probability|risk (?:for|in|of)))(?: (?!IGNORE)\\w+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:unlikely(?: to be positive)?|low (?:suspicion|probability)|is unlikely))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,2} (?:extremely low))#uncertain\n",
      "(?i)(?:(?:low risk of)(?: (?!IGNORE)\\w+){0,2} COVID-19)#uncertain\n",
      "(?i)(?:(?:(?:other_experiencer|family) ^test positive(?: for)?|any one|contact with(?: known))(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:other_experiencer|any one|contact with(?: known)))#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,0} (?:(?:a|an|another) \\S+ tested positive))#negated\n",
      "(?i)(?:(?:had contact|same (?:building|floor)|care for|clean)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:had contact|same (?:building|floor)|care for|clean))#negated\n",
      "(?i)(?:(?:concern(?:ed)? about)(?: (?!IGNORE)\\w+){0,2} COVID-19)#negated\n",
      "(?i)(?:(?:patient concern (?:for|of)|desire|(?:concerned|prepare) (?:for|about))(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:seen in|a (?:positive|confirmed) case of|cases|epidemic|pandemic)(?: (?!IGNORE)\\w+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,1} (?:cases|epidemic|pandemic|national emergency|crisis|situation|mandate|\\?))#negated\n",
      "(?i)(?:(?:national emergency|crisis|situation|mandate)(?: (?!IGNORE)\\w+){0,1} COVID-19)#negated\n",
      "(?i)(?:(?:seen in(?: the)? setting of)(?: (?!IGNORE)\\w+){0,5} COVID-19)#negated\n",
      "(?i)(?:(?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off|goals :)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off))#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:in the (?:area|community)|outbreak))#negated\n",
      "(?i)(?:(?:in the (?:area|community)|outbreak)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation))#negated\n",
      "(?i)(?:(?:^read about|deploy|(?:come|been) in close contact(?: with)?)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:^read about|deploy|(?:come|been) in close contact(?: with)?|error))#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:have you had close contact|web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases))#negated\n",
      "(?i)(?:(?:have you had close contact|the group|session|(?:nurse(?:s)?|rn) notes)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases|error)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:person|patients) with(?: confirmed)?(?: or)?(?: suspected)?|cases of)(?: (?!IGNORE)\\w+){0,2} COVID-19)#negated\n",
      "(?i)(?:(?:elective)(?: (?!IGNORE)\\w+){0,4} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,4} (?:elective))#negated\n",
      "(?i)(?:(?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?))(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?)))#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:(?:someone|person) who (?:has|have) tested positive|contact with))#negated\n",
      "(?i)(?:(?:(?:someone|person) who (?:has|have) tested positive|contact with)(?: (?!IGNORE)\\w+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!IGNORE)\\w+){0,0} (?:\\(resolved\\)))#positive"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat context_rules.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"context_rules.csv\", relation_name=\"ContextRules\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'ContextMatches(CovidAttribute, Span, Path, Pattern)':\n",
      "  CovidAttribute  |   Span    |    Path     |                                                                                    Pattern\n",
      "------------------+-----------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "     negated      | [22, 43)  | sample2.txt |                                   (?i)(?:(?:answer(?:ed|s|ing)? (?:no|negative|neg)|(?:neg|negative)(?: for)?)(?: (?!IGNORE)\\w+)*? COVID-19)\n",
      "     positive     | [70, 91)  | sample1.txt |                  (?i)(?:(?:(?:test(?:ed|s|ing)?)?positive(?: for)?|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status))(?: (?!IGNORE)\\w+)*? COVID-19)\n",
      "     positive     | [94, 129) | sample1.txt | (?i)(?:COVID-19(?: (?!IGNORE)\\w+)*? (?:positiv(?:e|ity)|test(?:ed|s|ing)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))\n",
      "\n",
      "printing results for query 'CovidSpans(Path, Span)':\n",
      "    Path     |   Span\n",
      "-------------+-----------\n",
      " sample1.txt | [34, 42)\n",
      " sample1.txt | [83, 91)\n",
      " sample1.txt | [94, 102)\n",
      " sample2.txt | [35, 43)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "#covid_attributes: negated, other_experiencer, is_future, not_relevant, uncertain, positive\n",
    "ContextMatches(CovidAttribute, Span, Path, Pattern) <- lemma_texts(Path, Text), ContextRules(Pattern, CovidAttribute), py_rgx_span(Text, Pattern) -> (Span)\n",
    "?ContextMatches(CovidAttribute, Span, Path, Pattern)\n",
    "\n",
    "CovidSpans(Path, Span) <- lemma_texts(Path, Text), py_rgx_span(Text, \"COVID-19\") -> (Span)\n",
    "?CovidSpans(Path, Span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_span_contained(span1, span2):\n",
    "    \"\"\"\n",
    "    Checks if one span is contained within the other span and returns the smaller span if yes.\n",
    "\n",
    "    Parameters:\n",
    "        span1 (span)\n",
    "        span2 (span)\n",
    "\n",
    "    Returns:\n",
    "        span: span1 if contained within span2 or vice versa, or None if not contained.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1.span_start, span1.span_end\n",
    "    start2, end2 = span2.span_start, span2.span_end\n",
    "    \n",
    "    if start2 <= start1 and end1 <= end2:\n",
    "        yield span1\n",
    "        \n",
    "    elif start1 <= start2 and end2 <= end1:\n",
    "        yield span2\n",
    "\n",
    "magic_session.register(is_span_contained, \"is_span_contained\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute)':\n",
      "    Path     |  CovidSpan  |  CovidAttribute\n",
      "-------------+-------------+------------------\n",
      " sample1.txt |  [83, 91)   |     positive\n",
      " sample1.txt |  [94, 102)  |     positive\n",
      " sample2.txt |  [35, 43)   |     negated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute) <- ContextMatches(CovidAttribute, Span1, Path, Pattern), CovidSpans(Path, Span2), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_filter(group):\n",
    "    \"\"\"\n",
    "    Helper function to filter attributes within each \"CovidSpan\" of a DataFrame table based on specific conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        group (pandas.Series):A pandas Series representing a group of attributes for each \"CovidSpan\" within a DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        str: the filtered \"CovidSpan\" attribute, determined as follows:\n",
    "        - 'negated' if 'negated' is present in the group.\n",
    "        - 'uncertain' if 'uncertain' is present in the group and 'negated' is not present.\n",
    "        - 'positive' if neither 'negated' nor 'uncertain' are present in the group.\n",
    "        \n",
    "    \"\"\"\n",
    "    if 'negated' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'uncertain' in group.values:\n",
    "        return 'uncertain'\n",
    "    else:\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>CovidSpan</th>\n",
       "      <th>CovidAttribute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>[83, 91)</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>[94, 102)</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>[35, 43)</td>\n",
       "      <td>negated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path  CovidSpan CovidAttribute\n",
       "0  sample1.txt   [83, 91)       positive\n",
       "1  sample1.txt  [94, 102)       positive\n",
       "2  sample2.txt   [35, 43)        negated"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (session.run_commands(\"?CovidAttributes(Path, CovidSpan, CovidAttribute)\", print_results=False, format_results=True))[0]\n",
    "if len(df) == 0:\n",
    "    df = DataFrame(columns=[\"Path\",\"CovidSpan\",\"CovidAttribute\"])\n",
    "df['CovidAttribute'] = df.groupby('CovidSpan')['CovidAttribute'].transform(attribute_filter)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_doc_helper(group):\n",
    "    \"\"\"\n",
    "     Helper function to classify a document as either 'POS', 'UNK', or 'NEG' based on COVID-19 attributes.\n",
    "    \n",
    "    Parameters:\n",
    "        group (pandas.Series):A pandas Series representing a group of COVID-19 attributes for each document within a DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "       str: the document classification determined as follows:\n",
    "       - 'POS': At least one COVID-19 attribute with \"positive\" in the group.\n",
    "       - 'UNK': At least one COVID-19 attribute with \"uncertain\" in the group and no \"positive\" attributes.\n",
    "       - 'NEG': Otherwise.\n",
    "    \"\"\"\n",
    "    if 'positive' in group.values:\n",
    "        return 'POS'\n",
    "    elif 'uncertain' in group.values:\n",
    "        return 'UNK'\n",
    "    else:\n",
    "        return 'NEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>DocResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path DocResult\n",
       "0  sample1.txt       POS\n",
       "1  sample2.txt       NEG"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DocResult'] = df.groupby('Path')['CovidAttribute'].transform(classify_doc_helper)\n",
    "df = df[['Path', 'DocResult']]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>DocResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path DocResult\n",
       "0  sample1.txt       POS\n",
       "1  sample2.txt       NEG"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = (session.run_commands(\"?FilesPaths(Path)\", print_results=False, format_results=True))[0]\n",
    "df = (pd.merge(df, df_path, on='Path', how='outer'))\n",
    "df['DocResult'] = df['DocResult'].fillna(\"UNK\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
